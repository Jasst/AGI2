# coding: utf-8
"""
AGI_CognitiveReasoning_v7_COMPLETE_FIXED.py
–ü–æ–ª–Ω–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π
"""

import os
import re
import json
import pickle
import random
import traceback
import math
from collections import Counter, defaultdict, deque
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple, Any
from pathlib import Path

import numpy as np
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from sentence_transformers import SentenceTransformer

    _HAS_ST_MODEL = True
except:
    _HAS_ST_MODEL = False


# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    SAVE_DIR = Path("./cognitive_model_data")
    MODEL_PATH = SAVE_DIR / "model_superintel.pt"
    VOCAB_PATH = SAVE_DIR / "vocab_superintel.pkl"
    MEMORY_PATH = SAVE_DIR / "memory_superintel.json"
    LEARNING_PATH = SAVE_DIR / "learning_superintel.json"

    VOCAB_SIZE = 15000
    EMB_DIM = 512
    HIDDEN_SIZE = 1024
    NUM_LAYERS = 4
    NUM_HEADS = 8
    DROPOUT = 0.2
    MAX_SEQ_LEN = 150
    LEARNING_RATE = 2e-4
    MAX_ATTEMPTS = 20
    CONTEXT_SIZE = 30
    CONFIDENCE_THRESHOLD = 0.6
    REFLECTION_DEPTH = 5
    QWEN_API = "http://localhost:1234/v1/chat/completions"


Config.SAVE_DIR.mkdir(exist_ok=True)


# ====================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ======================
def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
    if not isinstance(text, str):
        return "–•–æ—Ä–æ—à–æ."
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    text = re.sub(r'#{1,3}\s*', '', text)
    text = re.sub(r'>\s*', '', text)
    text = re.sub(r'\r\n|\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'^[\*\.\!\?\:\-\‚Äì‚Äî\s]+', '', text)
    text = re.sub(r'[\*\.\!\?\:\-\‚Äì‚Äî\s]+$', '', text)
    words = text.split()
    if len(words) > 150:
        text = ' '.join(words[:150])
    if not text.endswith(('.', '!', '?')):
        text += '.'
    return text or "–•–æ—Ä–æ—à–æ."


def clean_generated_response(text: str) -> str:
    """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏"""
    if not isinstance(text, str) or not text.strip():
        return ""
    words = text.split()
    cleaned = []
    for word in words:
        if not cleaned or cleaned[-1].lower() != word.lower():
            cleaned.append(word)
    if len(cleaned) < 2:
        return ""
    result = ' '.join(cleaned[:25])
    if result and not result.endswith(('.', '!', '?', 'üòä')):
        if len(cleaned) > 2:
            result += '.'
    return result.strip()


def clean_for_similarity(text: str) -> str:
    text = re.sub(r'[^\w\s]', ' ', text, flags=re.UNICODE)
    return re.sub(r'\s+', ' ', text).lower().strip()


def detect_input_type(user_input: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞"""
    s = user_input.lower().strip()
    patterns = {
        "SOCIAL": r'\b(–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π|–¥–æ–±—Ä—ã–π –¥–µ–Ω—å|–∫–∞–∫ –¥–µ–ª–∞|–ø–æ–∫–∞|—Å–ø–∞—Å–∏–±–æ|–±–ª–∞–≥–æ–¥–∞—Ä—é)\b',
        "FACT": r'\b(—á—Ç–æ —Ç–∞–∫–æ–µ|–∫—Ç–æ —Ç–∞–∫–æ–π|–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è|–∫–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞|—Ñ–æ—Ä–º—É–ª–∞|–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|—Ä–∞—Å—Å–∫–∞–∂–∏ –æ)\b',
        "REASON": r'\b(–ø–æ—á–µ–º—É|–∑–∞—á–µ–º|–æ—Ç—á–µ–≥–æ|–ø—Ä–∏—á–∏–Ω–∞|–∫–∞–∫ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç|–æ–±—ä—è—Å–Ω–∏ –º–µ—Ö–∞–Ω–∏–∑–º)\b',
        "PROCESS": r'\b(–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å|–∫–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è|—à–∞–≥|–∞–ª–≥–æ—Ä–∏—Ç–º|–ø–æ—à–∞–≥–æ–≤—ã–π)\b',
        "OPINION": r'\b(–∫–∞–∫ —Ç—ã –¥—É–º–∞–µ—à—å|—Ç–≤–æ—ë –º–Ω–µ–Ω–∏–µ|–ª—É—á—à–µ –ª–∏|–Ω—Ä–∞–≤–∏—Ç—Å—è –ª–∏|—Å–æ–≥–ª–∞—Å–µ–Ω –ª–∏)\b',
        "CREATIVE": r'\b(–ø—Ä–µ–¥—Å—Ç–∞–≤—å|–≤–æ–æ–±—Ä–∞–∑–∏|—Å–æ—á–∏–Ω–∏|–æ–ø–∏—à–∏ –∫–∞–∫|–º–µ—Ç–∞—Ñ–æ—Ä–∞|–∏—Å—Ç–æ—Ä–∏—è|—Å–æ–∑–¥–∞–π)\b',
        "ANALYSIS": r'\b(–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π|—Å—Ä–∞–≤–Ω–∏|—Ä–∞–∑–ª–∏—á–∏–µ|—Å—Ö–æ–¥—Å—Ç–≤–æ|–∞–Ω–∞–ª–∏–∑|—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è)\b',
    }
    for qtype, pattern in patterns.items():
        if re.search(pattern, s):
            return qtype
    return "FACT"


# ====================== –°–ò–°–¢–ï–ú–ê –ú–´–®–õ–ï–ù–ò–Ø ======================
class ThoughtProcess:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è –ê–ò"""

    def __init__(self):
        self.thoughts = []
        self.confidence = 0.0
        self.doubts = []
        self.reasoning_steps = []
        self.final_answer = ""
        self.learning_occurred = False

    def add_thought(self, thought: str, confidence: float = 0.5):
        self.thoughts.append({
            'text': thought,
            'confidence': confidence,
            'timestamp': datetime.now().isoformat()
        })
        self.confidence = np.mean([t['confidence'] for t in self.thoughts])

    def add_doubt(self, doubt: str):
        self.doubts.append({
            'text': doubt,
            'timestamp': datetime.now().isoformat()
        })

    def add_reasoning_step(self, step: str):
        self.reasoning_steps.append(step)

    def is_confident(self, threshold: float = Config.CONFIDENCE_THRESHOLD) -> bool:
        return self.confidence >= threshold

    def __str__(self):
        result = "üß† –ü–†–û–¶–ï–°–° –ú–´–®–õ–ï–ù–ò–Ø:\n"
        if self.thoughts:
            result += "üí≠ –ú—ã—Å–ª–∏:\n"
            for t in self.thoughts[-3:]:
                result += f" ‚Ä¢ {t['text']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {t['confidence']:.1%})\n"
        if self.doubts:
            result += "‚ùì –°–æ–º–Ω–µ–Ω–∏—è:\n"
            for d in self.doubts[-2:]:
                result += f" ‚Ä¢ {d['text']}\n"
        if self.reasoning_steps:
            result += "üìç –õ–æ–≥–∏–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è:\n"
            for i, step in enumerate(self.reasoning_steps[-3:], 1):
                result += f" {i}. {step}\n"
        result += f"üìä –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {self.confidence:.1%}\n"
        return result


# ====================== –ü–ê–ú–Ø–¢–¨ ======================
class ContextMemory:
    """–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤"""

    def __init__(self):
        self.conversations = deque(maxlen=50)
        self.user_profile = {}
        self.topics_discussed = defaultdict(list)
        self.relationships = {}
        self.load()

    def add_interaction(self, user_input: str, ai_response: str, topic: str, similarity: float):
        interaction = {
            'user_input': user_input,
            'ai_response': ai_response,
            'topic': topic,
            'similarity': similarity,
            'timestamp': datetime.now().isoformat()
        }
        self.conversations.append(interaction)
        if topic not in self.topics_discussed:
            self.topics_discussed[topic] = []
        self.topics_discussed[topic].append({
            'question': user_input,
            'answer': ai_response,
            'confidence': similarity
        })
        self.save()

    def get_context(self, topic: str, num_context: int = 5) -> str:
        if topic not in self.topics_discussed:
            return ""
        recent = self.topics_discussed[topic][-num_context:]
        if not recent:
            return ""
        context = f"üìö –ú–æ–∏ –∑–Ω–∞–Ω–∏—è –æ —Ç–µ–º–µ '{topic}':\n"
        for i, item in enumerate(recent, 1):
            context += f"{i}. Q: {item['question'][:50]}...\n   A: {item['answer'][:50]}...\n"
        return context

    def get_recent_context(self, num_last: int = 3) -> str:
        if not self.conversations:
            return ""
        recent = list(self.conversations)[-num_last:]
        context = "üìö –ü–æ—Å–ª–µ–¥–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n"
        for conv in recent:
            context += f"‚Ä¢ {conv['ai_response'][:40]}...\n"
        return context

    def understand_user_intent(self, user_input: str) -> Dict[str, Any]:
        intent = {
            'is_continuation': False,
            'related_topic': None,
            'context': ""
        }
        if self.conversations:
            last_topic = self.conversations[-1]['topic']
            if user_input.lower() in ['–¥–∞', '–µ—â–µ', '–ø—Ä–æ–¥–æ–ª–∂–∏', '–∏?', '—á—Ç–æ –µ—â–µ']:
                intent['is_continuation'] = True
                intent['related_topic'] = last_topic
                intent['context'] = self.get_recent_context(2)
        return intent

    def save(self):
        data = {
            'conversations': list(self.conversations),
            'topics_discussed': dict(self.topics_discussed),
            'user_profile': self.user_profile,
            'relationships': self.relationships
        }
        memory_file = Config.SAVE_DIR / "context_memory.json"
        with open(memory_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        memory_file = Config.SAVE_DIR / "context_memory.json"
        if memory_file.exists():
            try:
                with open(memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.conversations = deque(data.get('conversations', []), maxlen=50)
                    self.topics_discussed = defaultdict(list, data.get('topics_discussed', {}))
                    self.user_profile = data.get('user_profile', {})
                    self.relationships = data.get('relationships', {})
            except:
                pass


# ====================== –ú–ï–ù–ï–î–ñ–ï–† –û–ë–£–ß–ï–ù–ò–Ø ======================
class LearningManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –ê–ò"""

    def __init__(self):
        self.knowledge_base = {}
        self.learning_history = []
        self.skill_level = 0.1
        self.asked_questions_count = 0
        self.correct_answers_count = 0
        self.accuracies = []
        self.load()

    def record_learning(self, topic: str, concept: str, teacher_answer: str,
                        ai_answer: str, similarity: float):
        record = {
            'topic': topic,
            'concept': concept,
            'teacher_answer': teacher_answer,
            'ai_answer': ai_answer,
            'similarity': similarity,
            'timestamp': datetime.now().isoformat(),
            'skill_improvement': similarity
        }
        self.learning_history.append(record)
        if topic not in self.knowledge_base:
            self.knowledge_base[topic] = []
        self.knowledge_base[topic].append({
            'concept': concept,
            'answer': teacher_answer,
            'learned': True,
            'similarity': similarity
        })
        self.update_skill_level(similarity)
        self.save()

    def update_skill_level(self, similarity: float):
        """–ü—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞"""
        self.accuracies.append(similarity)
        recent_accuracies = self.accuracies[-10:]
        avg_accuracy = np.mean(recent_accuracies)

        if avg_accuracy > 0.75:
            improvement = (avg_accuracy - 0.75) * 0.05
            self.skill_level = min(1.0, self.skill_level + improvement)
        elif avg_accuracy > 0.5:
            improvement = (avg_accuracy - 0.5) * 0.02
            self.skill_level = min(1.0, self.skill_level + improvement)

        self.correct_answers_count += int(similarity > 0.7)
        self.asked_questions_count += 1

    def get_known_topics(self) -> List[str]:
        return list(self.knowledge_base.keys())

    def get_learning_progress(self) -> str:
        total = len(self.learning_history)
        if total == 0:
            return "–û–±—É—á–µ–Ω–∏–µ –µ—â–µ –Ω–µ –Ω–∞—á–∞–ª–æ—Å—å"
        recent_acc = np.mean(self.accuracies[-5:]) if self.accuracies else 0
        return (f"üìà –£—Ä–æ–≤–µ–Ω—å –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞: {self.skill_level:.1%} | "
                f"–¢–æ—á–Ω–æ—Å—Ç—å (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5): {recent_acc:.1%} | "
                f"–í—ã—É—á–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π: {len(self.knowledge_base)}")

    def save(self):
        data = {
            'knowledge_base': self.knowledge_base,
            'learning_history': self.learning_history,
            'skill_level': self.skill_level,
            'asked_questions_count': self.asked_questions_count,
            'correct_answers_count': self.correct_answers_count,
            'accuracies': self.accuracies
        }
        with open(Config.LEARNING_PATH, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        if Config.LEARNING_PATH.exists():
            try:
                with open(Config.LEARNING_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.knowledge_base = data.get('knowledge_base', {})
                    self.learning_history = data.get('learning_history', [])
                    self.skill_level = data.get('skill_level', 0.1)
                    self.asked_questions_count = data.get('asked_questions_count', 0)
                    self.correct_answers_count = data.get('correct_answers_count', 0)
                    self.accuracies = data.get('accuracies', [])
            except:
                pass


# ====================== VOCABULARY ======================
class AdvancedVocabManager:
    def __init__(self):
        self.word2idx = {
            '<pad>': 0,
            '<start>': 1,
            '<end>': 2,
            '<unk>': 3,
        }
        self.idx2word = {v: k for k, v in self.word2idx.items()}
        self.word_freq = Counter()
        self.next_id = 4
        self.semantic_tags = defaultdict(list)

    def add_word(self, word: str, semantic_tag: Optional[str] = None) -> int:
        word_lower = word.lower()
        if word_lower not in self.word2idx:
            if self.next_id < Config.VOCAB_SIZE:
                self.word2idx[word_lower] = self.next_id
                self.idx2word[self.next_id] = word_lower
                self.next_id += 1
        self.word_freq[word_lower] += 1
        if semantic_tag:
            self.semantic_tags[word_lower].append(semantic_tag)
        return self.word2idx.get(word_lower, self.word2idx['<unk>'])

    def add_words(self, words: List[str]):
        for w in words:
            if w.strip():
                self.add_word(w)

    def tokenize(self, text: str) -> List[int]:
        words = clean_for_similarity(text).split()
        return [self.word2idx.get(w, self.word2idx['<unk>']) for w in words]

    def decode(self, ids: List[int]) -> str:
        tokens = [self.idx2word.get(i, '') for i in ids if i not in [0, 1, 2]]
        deduped = []
        for token in tokens:
            if not deduped or deduped[-1] != token:
                deduped.append(token)
        deduped = deduped[:20]
        text = ' '.join(deduped)
        text = re.sub(r'<[^>]+>', '', text)
        return text.strip()

    @property
    def size(self):
        return len(self.word2idx)

    def save(self):
        data = {
            'word2idx': self.word2idx,
            'idx2word': self.idx2word,
            'word_freq': dict(self.word_freq),
            'next_id': self.next_id,
            'semantic_tags': dict(self.semantic_tags)
        }
        with open(Config.VOCAB_PATH, 'wb') as f:
            pickle.dump(data, f)

    def load(self):
        if Config.VOCAB_PATH.exists():
            with open(Config.VOCAB_PATH, 'rb') as f:
                data = pickle.load(f)
                self.word2idx = data['word2idx']
                self.idx2word = data['idx2word']
                self.word_freq = Counter(data['word_freq'])
                self.next_id = data['next_id']
                self.semantic_tags = defaultdict(list, data.get('semantic_tags', {}))
                return True
        return False


# ====================== –ü–û–ó–ò–¶–ò–û–ù–ù–û–ï –ö–û–î–ò–†–û–í–ê–ù–ò–ï ======================
class PositionalEncoding(nn.Module):
    def __init__(self, emb_dim: int, max_len: int = 5000):
        super().__init__()
        self.emb_dim = emb_dim
        pe = torch.zeros(max_len, emb_dim)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() *
                             (-math.log(10000.0) / emb_dim))
        pe[:, 0::2] = torch.sin(position * div_term)
        if emb_dim % 2 == 1:
            pe[:, 1::2] = torch.cos(position * div_term[:-1])
        else:
            pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]


# ====================== MULTI-HEAD ATTENTION ======================
class MultiHeadAttention(nn.Module):
    def __init__(self, emb_dim: int, num_heads: int):
        super().__init__()
        assert emb_dim % num_heads == 0
        self.emb_dim = emb_dim
        self.num_heads = num_heads
        self.head_dim = emb_dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.query = nn.Linear(emb_dim, emb_dim)
        self.key = nn.Linear(emb_dim, emb_dim)
        self.value = nn.Linear(emb_dim, emb_dim)
        self.fc_out = nn.Linear(emb_dim, emb_dim)

    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]
        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        weights = F.softmax(scores, dim=-1)
        context = torch.matmul(weights, V)
        context = context.transpose(1, 2).contiguous()
        context = context.view(batch_size, -1, self.emb_dim)
        output = self.fc_out(context)
        return output, weights


# ====================== –¢–†–ê–ù–°–§–û–†–ú–ï–† –ë–õ–û–ö ======================
class TransformerBlock(nn.Module):
    def __init__(self, emb_dim: int, num_heads: int, hidden_size: int, dropout: float = 0.1):
        super().__init__()
        self.attention = MultiHeadAttention(emb_dim, num_heads)
        self.norm1 = nn.LayerNorm(emb_dim)
        self.norm2 = nn.LayerNorm(emb_dim)
        self.ffn = nn.Sequential(
            nn.Linear(emb_dim, hidden_size),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, emb_dim),
            nn.Dropout(dropout)
        )

    def forward(self, x, mask=None):
        attn_out, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + attn_out)
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        return x


# ====================== –°–£–ü–ï–† –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –ú–û–ó–ì ======================
class SuperIntelligentBrain(nn.Module):
    def __init__(self, vocab_size: int, device=None):
        super().__init__()
        self.device = device or (torch.device('cuda') if torch.cuda.is_available()
                                 else torch.device('cpu'))
        self.vocab_size = vocab_size
        self.emb_dim = Config.EMB_DIM
        self.hidden_size = Config.HIDDEN_SIZE

        # Embeddings
        self.embedding = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)
        self.pos_encoding = PositionalEncoding(self.emb_dim, Config.MAX_SEQ_LEN)
        self.embedding_dropout = nn.Dropout(Config.DROPOUT)

        # Encoder –∏ Decoder
        self.encoder_blocks = nn.ModuleList([
            TransformerBlock(self.emb_dim, Config.NUM_HEADS, self.hidden_size, Config.DROPOUT)
            for _ in range(Config.NUM_LAYERS)
        ])

        self.decoder_blocks = nn.ModuleList([
            TransformerBlock(self.emb_dim, Config.NUM_HEADS, self.hidden_size, Config.DROPOUT)
            for _ in range(Config.NUM_LAYERS)
        ])

        # Cross-attention
        self.cross_attentions = nn.ModuleList([
            MultiHeadAttention(self.emb_dim, Config.NUM_HEADS)
            for _ in range(Config.NUM_LAYERS)
        ])

        # Output projection
        self.output_proj = nn.Linear(self.emb_dim, vocab_size)

        # –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–∞—è –±–∞–∑–∞
        self.concept_bank = defaultdict(list)
        self.memory_bank = None

        self.to(self.device)

    def encode(self, input_ids: torch.Tensor) -> torch.Tensor:
        """–≠–Ω–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–∞"""
        x = self.embedding(input_ids)
        x = self.pos_encoding(x)
        x = self.embedding_dropout(x)

        for block in self.encoder_blocks:
            x = block(x)

        self.memory_bank = x
        return x

    def decode_with_attention(self, target_ids: torch.Tensor,
                              encoder_output: torch.Tensor) -> torch.Tensor:
        """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∫—Ä–æ—Å—Å-–∞—Ç–µ–Ω—Ç—à–µ–Ω–æ–º"""
        x = self.embedding(target_ids)
        x = self.pos_encoding(x)
        x = self.embedding_dropout(x)

        for i, block in enumerate(self.decoder_blocks):
            x = block(x)
            cross_out, _ = self.cross_attentions[i](x, encoder_output, encoder_output)
            x = x + cross_out

        return x

    def generate(self, input_ids: torch.Tensor, max_len: int = 80,
                 temperature: float = 1.2) -> List[int]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è"""
        was_training = self.training
        self.eval()

        with torch.no_grad():
            encoder_output = self.encode(input_ids)
            batch_size = input_ids.size(0)
            current_tokens = torch.full((batch_size, 1), 1,
                                        device=self.device, dtype=torch.long)
            generated = []
            last_tokens = deque(maxlen=3)  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è

            for step in range(max_len):
                decoder_output = self.decode_with_attention(current_tokens, encoder_output)
                logits = self.output_proj(decoder_output[:, -1, :])

                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                logits = logits / max(temperature, 0.5)

                # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
                if len(last_tokens) > 0:
                    for token_id in last_tokens:
                        logits[0, token_id] -= 2.0

                probs = F.softmax(logits, dim=-1)

                # Nucleus sampling (top-p) –≤–º–µ—Å—Ç–æ top-k –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                cumsum_probs = torch.cumsum(sorted_probs, dim=-1)
                sorted_indices_to_remove = cumsum_probs > 0.9  # top 90%
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0

                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                probs[0, indices_to_remove] = 0
                probs = probs / probs.sum()

                # –ò–∑–±–µ–≥–∞–µ–º padding —Ç–æ–∫–µ–Ω–∞
                probs[0, 0] = 0
                probs = probs / probs.sum()

                next_token_idx = torch.multinomial(probs[0], 1)
                token_id = next_token_idx.item()

                if token_id == 2 or token_id == 0:  # END –∏–ª–∏ PAD
                    break

                generated.append(token_id)
                last_tokens.append(token_id)
                current_tokens = torch.cat([current_tokens, next_token_idx.view(batch_size, 1)], dim=1)

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                if len(current_tokens[0]) > Config.MAX_SEQ_LEN:
                    break

        if was_training:
            self.train()

        return generated

    def save_model(self):
        torch.save({
            'model_state': self.state_dict(),
            'concept_bank': dict(self.concept_bank),
        }, Config.MODEL_PATH)

    def load_model(self):
        if Config.MODEL_PATH.exists():
            try:
                checkpoint = torch.load(Config.MODEL_PATH, map_location=self.device)
                self.load_state_dict(checkpoint['model_state'])
                self.concept_bank = defaultdict(list, checkpoint.get('concept_bank', {}))
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
                return False
        return False


# ====================== –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –û–¶–ï–ù–ö–ê ======================
class SemanticEvaluator:
    def __init__(self):
        self.model = None
        if _HAS_ST_MODEL:
            try:
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
            except:
                pass

    def similarity(self, a: str, b: str) -> float:
        if not a or not b:
            return 0.0

        if self.model is not None:
            try:
                emb = self.model.encode([a, b], normalize_embeddings=True)
                return float(np.dot(emb[0], emb[1]))
            except:
                pass

        a_clean = set(clean_for_similarity(a).split())
        b_clean = set(clean_for_similarity(b).split())

        if not a_clean or not b_clean:
            return 0.0

        return len(a_clean & b_clean) / len(a_clean | b_clean)


# ====================== –£–ß–ò–¢–ï–õ–¨ (–û–ë–£–ß–ï–ù–ò–ï) ======================
class SupervisedTeacher:
    def __init__(self):
        self.api_url = Config.QWEN_API
        self.evaluator = SemanticEvaluator()
        self.learning_manager = LearningManager()
        self.context_memory = ContextMemory()
        self.step_count = 0

    def ask_teacher(self, prompt: str, context: str = "") -> str:
        """–°–ø—Ä–æ—Å–∏—Ç—å —É —Å—Ç–∞—Ä—à–µ–π –º–æ–¥–µ–ª–∏"""
        full_prompt = prompt
        if context:
            full_prompt = f"{context}\n\n–í–æ–ø—Ä–æ—Å: {prompt}"

        try:
            resp = requests.post(self.api_url, json={
                "messages": [{"role": "user", "content": full_prompt}],
                "max_tokens": 300,
                "temperature": 0.8
            }, timeout=25)

            if resp.status_code == 200:
                return clean_text(resp.json()['choices'][0]['message']['content'])
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ API: {e}")

        return "–Ø –Ω–µ –∑–Ω–∞—é –æ—Ç–≤–µ—Ç–∞."

    def generate_thoughts(self, user_input: str, input_type: str) -> ThoughtProcess:
        thought_process = ThoughtProcess()
        thought_process.add_thought(
            f"–ú–Ω–µ –∑–∞–¥–∞–ª–∏ –≤–æ–ø—Ä–æ—Å —Ç–∏–ø–∞ '{input_type}': '{user_input}'",
            confidence=0.6
        )

        skill_level = self.learning_manager.skill_level
        if skill_level < 0.3:
            thought_process.add_thought(
                "–Ø –µ—â–µ –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è, –Ω—É–∂–Ω–æ –±—ã—Ç—å –æ—Å—Ç–æ—Ä–æ–∂–Ω–µ–µ",
                confidence=0.8
            )
            thought_process.add_doubt("–Ø –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–ø—ã—Ç–µ–Ω")
        elif skill_level > 0.7:
            thought_process.add_thought(
                "–Ø —Ö–æ—Ä–æ—à–æ –æ–±—É—á–∏–ª—Å—è, –º–æ–≥—É –¥–∞–≤–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã",
                confidence=0.85
            )

        intent = self.context_memory.understand_user_intent(user_input)
        if intent['is_continuation']:
            thought_process.add_thought(
                f"–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –æ '{intent['related_topic']}'",
                confidence=0.9
            )

        topic_context = self.context_memory.get_context(input_type, num_context=2)
        if topic_context:
            thought_process.add_reasoning_step("–Ø –ø–æ–º–Ω—é –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –æ—Ç–≤–µ—Ç—ã –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ")

        key_words = [w for w in clean_for_similarity(user_input).split() if len(w) > 3]
        if key_words:
            thought_process.add_reasoning_step(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(key_words[:3])}")

        return thought_process

    def train_step(self, model: SuperIntelligentBrain, vocab: AdvancedVocabManager,
                   user_input: str, input_type: str) -> str:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""

        print(f"\nüë§ –í—ã: {user_input}")
        print(f"üìã –¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞: {input_type}")

        # === 1. –ì–ï–ù–ï–†–ê–¶–ò–Ø –ú–´–°–õ–ï–ô (–†–ï–§–õ–ï–ö–°–ò–Ø) ===
        thought_process = self.generate_thoughts(user_input, input_type)
        print(thought_process)

        # === 2. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===
        vocab.add_words(clean_for_similarity(user_input).split())
        input_tokens = vocab.tokenize(user_input)
        if not input_tokens or len(input_tokens) < 2:
            input_tokens = [1, 3]

        # –û–±—Ä–µ–∑–∞–µ–º –∏ –ø–∞–¥–¥–∏—Ä—É–µ–º encoder input
        input_tokens = input_tokens[:Config.MAX_SEQ_LEN]
        encoder_len = len(input_tokens)
        input_tokens = input_tokens + [0] * (Config.MAX_SEQ_LEN - encoder_len)

        encoder_input = torch.tensor([input_tokens], device=model.device, dtype=torch.long)

        # === 3. –ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –û–¢–í–ï–¢–ê –û–¢ –£–ß–ò–¢–ï–õ–Ø ===
        print("\nüë®‚Äçüè´ –ü–æ–ª—É—á–∞—é –æ—Ç–≤–µ—Ç –æ—Ç —É—á–∏—Ç–µ–ª—è...")
        memory_context = self.context_memory.get_context(input_type, num_context=2)
        teacher_answer = self.ask_teacher(user_input, memory_context)
        print(f"üë®‚Äçüè´ –£—á–∏—Ç–µ–ª—å: {teacher_answer}")

        # === 4. –ü–û–î–ì–û–¢–û–í–ö–ê –¶–ï–õ–ï–í–´–• –¢–û–ö–ï–ù–û–í ===
        vocab.add_words(clean_for_similarity(teacher_answer).split())
        target_tokens = vocab.tokenize(teacher_answer)

        if not target_tokens or len(target_tokens) < 2:
            target_tokens = [1, 3]

        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ MAX_SEQ_LEN - 1 (–º–µ—Å—Ç–æ –¥–ª—è END —Ç–æ–∫–µ–Ω–∞)
        target_tokens = target_tokens[:Config.MAX_SEQ_LEN - 1]
        target_len = len(target_tokens)

        # –ü–∞–¥–¥–∏—Ä—É–µ–º –¥–æ MAX_SEQ_LEN - 1
        target_tokens_padded = target_tokens + [0] * (Config.MAX_SEQ_LEN - 1 - target_len)

        # –í—Ö–æ–¥—ã decoder: [START] + first MAX_SEQ_LEN-1 tokens
        # –í—ã—Ö–æ–¥—ã: last MAX_SEQ_LEN-1 tokens + [END]
        decoder_input = [1] + target_tokens_padded[:-1]  # START + –ø–µ—Ä–≤—ã–µ N-1
        target_output = target_tokens_padded[1:] + [2]  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ N-1 + END

        # –û–±–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–ª–∏–Ω—ã MAX_SEQ_LEN
        decoder_input = decoder_input[:Config.MAX_SEQ_LEN]
        target_output = target_output[:Config.MAX_SEQ_LEN]

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –æ–±–µ –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        while len(decoder_input) < Config.MAX_SEQ_LEN:
            decoder_input.append(0)
        while len(target_output) < Config.MAX_SEQ_LEN:
            target_output.append(0)

        assert len(decoder_input) == Config.MAX_SEQ_LEN, f"decoder_input len: {len(decoder_input)}"
        assert len(target_output) == Config.MAX_SEQ_LEN, f"target_output len: {len(target_output)}"

        decoder_input = torch.tensor([decoder_input], device=model.device, dtype=torch.long)
        target_output = torch.tensor([target_output], device=model.device, dtype=torch.long)

        # === 5. –û–ë–£–ß–ï–ù–ò–ï –° –í–ê–õ–ò–î–ê–¶–ò–ï–ô ===
        print("\nüîÑ –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è:")
        model.train()
        optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE,
                                      weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

        best_loss = float('inf')
        best_response = teacher_answer
        no_improve_count = 0

        for attempt in range(1, Config.MAX_ATTEMPTS + 1):
            model.train()
            optimizer.zero_grad()

            # Forward pass
            encoder_output = model.encode(encoder_input)
            decoder_output = model.decode_with_attention(decoder_input, encoder_output)

            # –ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ —Å–ª–æ–≤–∞—Ä—å
            logits = model.output_proj(decoder_output)

            # Loss calculation —Å —É—á–µ—Ç–æ–º –ø–∞–¥–¥–∏–Ω–≥–∞
            # logits: [batch, seq_len, vocab_size] -> [batch*seq_len, vocab_size]
            # target: [batch, seq_len] -> [batch*seq_len]
            batch_size, seq_len, vocab_size = logits.shape
            loss = F.cross_entropy(
                logits.reshape(batch_size * seq_len, vocab_size),
                target_output.reshape(batch_size * seq_len),
                ignore_index=0,
                reduction='mean'
            )

            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # === 6. –í–ê–õ–ò–î–ê–¶–ò–Ø –ò –ü–†–û–í–ï–†–ö–ê ===
            model.eval()
            with torch.no_grad():
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π
                gen_ids = model.generate(encoder_input, max_len=50, temperature=1.3)
                predicted_answer = vocab.decode(gen_ids)
                predicted_answer = clean_generated_response(predicted_answer)

                if not predicted_answer:
                    predicted_answer = teacher_answer

                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = self.evaluator.similarity(teacher_answer, predicted_answer)

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                improvement = "üìà" if loss.item() < best_loss else "üìâ"
                print(f" üîÅ –ò—Ç–µ—Ä. {attempt:2d}: "
                      f"loss={loss.item():.4f}, "
                      f"—Å—Ö–æ–¥—Å—Ç–≤–æ={similarity:.1%} {improvement}")

                # –û–±–Ω–æ–≤–ª—è–µ–º best –µ—Å–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ
                if loss.item() < best_loss:
                    best_loss = loss.item()
                    best_response = predicted_answer
                    no_improve_count = 0
                else:
                    no_improve_count += 1

            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if no_improve_count >= 3:
                print("‚èπÔ∏è  –û—Å—Ç–∞–Ω–æ–≤–∫–∞: –Ω–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ 3 –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥—Ä—è–¥")
                break

            # –†–∞–Ω–Ω–µ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–∏ —Ö–æ—Ä–æ—à–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
            if loss.item() < 0.5 and similarity > 0.6:
                print("‚úÖ –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ!")
                thought_process.learning_occurred = True
                break

            scheduler.step()

        # === 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ü–ê–ú–Ø–¢–¨ ===
        final_similarity = self.evaluator.similarity(teacher_answer, best_response)

        self.learning_manager.record_learning(
            topic=input_type,
            concept=user_input,
            teacher_answer=teacher_answer,
            ai_answer=best_response,
            similarity=final_similarity
        )

        self.context_memory.add_interaction(
            user_input=user_input,
            ai_response=best_response,
            topic=input_type,
            similarity=final_similarity
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –≤ model.concept_bank
        model.concept_bank[input_type].append({
            'input': user_input,
            'output': best_response,
            'similarity': final_similarity,
            'timestamp': datetime.now().isoformat()
        })

        print(f"\nüìä –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {final_similarity:.1%}")
        print(f"üìö {self.learning_manager.get_learning_progress()}")

        self.step_count += 1
        return best_response


# ====================== –ì–õ–ê–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê ======================
def main():
    print("\n" + "=" * 70)
    print("üß† –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –° –†–ï–§–õ–ï–ö–°–ò–ï–ô –ò –û–ë–£–ß–ï–ù–ò–ï–ú v7.0 (FIXED)")
    print("=" * 70)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø VOCAB ===
    vocab = AdvancedVocabManager()
    if vocab.load():
        print(f"‚úÖ –°–ª–æ–≤–∞—Ä—å –∑–∞–≥—Ä—É–∂–µ–Ω ({vocab.size} —Å–ª–æ–≤)")
    else:
        print("üî® –°–æ–∑–¥–∞—é –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å...")
        base_words = ("–ø—Ä–∏–≤–µ—Ç —Å–ø–∞—Å–∏–±–æ –¥–∞ –Ω–µ—Ç —á—Ç–æ –∫–∞–∫ –ø–æ—á–µ–º—É –≥–¥–µ –∫–æ–≥–¥–∞ –∫—Ç–æ –∫–æ—Ç–æ—Ä—ã–π "
                      "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ–Ω–∏–º–∞—é —É–∑–Ω–∞–ª –Ω–æ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —Ä–µ—Ñ–ª–µ–∫—Å–∏—è —Å–æ–º–Ω–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å "
                      "–æ—Ç–≤–µ—Ç –º–Ω–µ–Ω–∏–µ —Ñ–∞–∫—Ç –ø—Ä–æ—Ü–µ—Å—Å –∞–Ω–∞–ª–∏–∑").split()
        vocab.add_words(base_words)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å —Å {vocab.size} —Å–ª–æ–≤–∞–º–∏")

    # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò ===
    model = SuperIntelligentBrain(vocab_size=max(Config.VOCAB_SIZE, vocab.size),
                                  device=device)
    if model.load_model():
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    else:
        print("üî® –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å...")
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

    # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –£–ß–ò–¢–ï–õ–Ø ===
    teacher = SupervisedTeacher()

    print(f"\nüí° –ö–û–ú–ê–ù–î–´:")
    print(f" '–≤—ã—Ö–æ–¥' - –∑–∞–≤–µ—Ä—à–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É")
    print(f" '–ø–∞–º—è—Ç—å' - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π")
    print(f" '–∫–æ–Ω—Ç–µ–∫—Å—Ç' - –ø–æ–∫–∞–∑–∞—Ç—å –∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç")
    print(f" '—Ç–µ–º—ã' - –ø–æ–∫–∞–∑–∞—Ç—å –∏–∑—É—á–µ–Ω–Ω—ã–µ —Ç–µ–º—ã —Å —Ñ–∞–∫—Ç–∞–º–∏")
    print(f" '–∑–∞–ø–æ–º–Ω–∏–ª' - –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–ø–æ–º–Ω–∏–ª–∞")
    print(f" '–ø—Ä–æ–≥—Ä–µ—Å—Å' - –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è")
    print(f" '–≥—Ä–∞—Ñ–∏–∫' - –ø–æ–∫–∞–∑–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è")
    print(f" '—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å' - —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å")
    print(f" '–æ—á–∏—Å—Ç–∏—Ç—å' - –æ—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å")

    interaction_count = 0

    while True:
        try:
            user_input = input("\nüë§ –í—ã: ").strip()

            if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
                print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å...")
                model.save_model()
                vocab.save()
                teacher.learning_manager.save()
                teacher.context_memory.save()
                print("‚ú® –î–æ –≤—Å—Ç—Ä–µ—á–∏! –°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ!")
                break

            if user_input.lower() in ['–ø–∞–º—è—Ç—å', 'memory']:
                print(f"\nüìö –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è:")
                history = teacher.learning_manager.learning_history
                if history:
                    for item in history[-5:]:
                        print(f" üìç {item['concept'][:40]}...")
                        print(f" –°—Ö–æ–¥—Å—Ç–≤–æ: {item['similarity']:.1%}")
                else:
                    print(" (–∏—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞)")
                continue

            if user_input.lower() in ['–∫–æ–Ω—Ç–µ–∫—Å—Ç', 'context']:
                print(f"\nüß† –ó–ê–ü–û–ú–ù–ï–ù–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢:")
                if teacher.context_memory.conversations:
                    print(f" –í—Å–µ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {len(teacher.context_memory.conversations)}")
                    print(f"\n –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:")
                    for i, conv in enumerate(list(teacher.context_memory.conversations)[-3:], 1):
                        print(f"\n {i}. –¢–µ–º–∞: {conv['topic']}")
                        print(f" –í–æ–ø—Ä–æ—Å: {conv['user_input'][:50]}...")
                        print(f" –û—Ç–≤–µ—Ç: {conv['ai_response'][:50]}...")
                        print(f" –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {conv['similarity']:.1%}")
                else:
                    print(" (–∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—â–µ –Ω–µ –Ω–∞–∫–æ–ø–ª–µ–Ω)")
                continue

            if user_input.lower() in ['—Ç–µ–º—ã', 'topics']:
                print(f"\nüìö –ò–ó–£–ß–ï–ù–ù–´–ï –¢–ï–ú–´ –ò –§–ê–ö–¢–´:")
                topics = teacher.context_memory.topics_discussed
                if topics:
                    for topic, facts in list(topics.items())[-5:]:
                        print(f"\n üìå –¢–µ–º–∞: {topic}")
                        print(f" –í—ã—É—á–µ–Ω–æ —Ñ–∞–∫—Ç–æ–≤: {len(facts)}")
                        if facts:
                            avg_confidence = np.mean([f['confidence'] for f in facts])
                            print(f" –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.1%}")
                            print(f" –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–∞–∫—Ç—ã:")
                            for f in facts[-2:]:
                                print(f" ‚Ä¢ {f['answer'][:50]}...")
                else:
                    print(" (—Ç–µ–º—ã –µ—â–µ –Ω–µ –∏–∑—É—á–µ–Ω—ã)")
                continue

            if user_input.lower() in ['–∑–∞–ø–æ–º–Ω–∏–ª', '–∑–∞–ø–æ–º–Ω–∏', '—á—Ç–æ —Ç—ã –ø–æ–º–Ω–∏—à—å', '–≤—Å–ø–æ–º–Ω–∏']:
                print(f"\nüß† –ß–¢–û –Ø –ó–ê–ü–û–ú–ù–ò–õ:")
                memory = teacher.context_memory
                if memory.conversations:
                    print(f" –í—Å–µ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {len(memory.conversations)}")
                    print(f"\n –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–´ –ò–ó –ü–ê–ú–Ø–¢–ò:")
                    for topic, facts in list(memory.topics_discussed.items())[-3:]:
                        print(f"\n üìå {topic}:")
                        for fact in facts[-2:]:
                            print(f" Q: {fact['question'][:50]}...")
                            print(f" A: {fact['answer'][:50]}...\n")
                else:
                    print(" (–ø–∞–º—è—Ç—å –ø—É—Å—Ç–∞)")
                continue

            if user_input.lower() in ['–ø—Ä–æ–≥—Ä–µ—Å—Å', 'progress', 'stats']:
                print(f"\nüìä –ü–†–û–ì–†–ï–°–° –û–ë–£–ß–ï–ù–ò–Ø:")
                print(f" {teacher.learning_manager.get_learning_progress()}")
                known = teacher.learning_manager.get_known_topics()
                if known:
                    print(f" –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–µ–º—ã: {', '.join(known)}")
                continue

            if user_input.lower() in ['–≥—Ä–∞—Ñ–∏–∫', 'graph', 'chart']:
                print(f"\nüìà –ì–†–ê–§–ò–ö –û–ë–£–ß–ï–ù–ò–Ø (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤):")
                accuracies = teacher.learning_manager.accuracies[-10:]
                if accuracies:
                    for i, acc in enumerate(accuracies, 1):
                        bar_length = int(acc * 30)
                        bar = "‚ñà" * bar_length + "‚ñë" * (30 - bar_length)
                        print(f" {i:2d}. [{bar}] {acc:.1%}")
                    avg = np.mean(accuracies)
                    print(f"\n –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {avg:.1%}")
                else:
                    print(" (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö)")
                continue

            if user_input.lower() in ['—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å', 'save']:
                print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é...")
                model.save_model()
                vocab.save()
                teacher.learning_manager.save()
                teacher.context_memory.save()
                print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ!")
                continue

            if user_input.lower() in ['–æ—á–∏—Å—Ç–∏—Ç—å', 'clear']:
                if input("–í—ã —É–≤–µ—Ä–µ–Ω—ã? (–¥–∞/–Ω–µ—Ç): ").lower() == '–¥–∞':
                    Config.LEARNING_PATH.unlink(missing_ok=True)
                    (Config.SAVE_DIR / "context_memory.json").unlink(missing_ok=True)
                    teacher.learning_manager = LearningManager()
                    teacher.context_memory = ContextMemory()
                    print("‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞!")
                continue

            if not user_input:
                continue

            input_type = detect_input_type(user_input)
            final_answer = teacher.train_step(model, vocab, user_input, input_type)
            print(f"\nüí° –ú–û–ô –û–¢–í–ï–¢: {final_answer}\n")
            print("=" * 70)

            interaction_count += 1
            if interaction_count % 3 == 0:
                print(f"üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...")
                model.save_model()
                vocab.save()
                teacher.learning_manager.save()
                teacher.context_memory.save()

        except KeyboardInterrupt:
            print("\n‚ú® –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ. –î–æ –≤—Å—Ç—Ä–µ—á–∏!")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            traceback.print_exc()


if __name__ == "__main__":
    main()