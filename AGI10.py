# coding: utf-8
"""
AGI_MultiMind_Autonomous_v11.py
–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ–ø–∏–∏ –º–æ–¥–µ–ª–∏ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ
Qwen —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–Ω–∞–Ω–∏–π, –º—ã—à–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ
"""

import os
import re
import json
import pickle
import random
import traceback
import math
from collections import Counter, defaultdict, deque
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple, Any
from pathlib import Path

import numpy as np
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass

try:
    from sentence_transformers import SentenceTransformer

    _HAS_ST_MODEL = True
except:
    _HAS_ST_MODEL = False


# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    SAVE_DIR = Path("./cognitive_multimind_v11")
    MODEL_PATH = SAVE_DIR / "model.pt"
    VOCAB_PATH = SAVE_DIR / "vocab.pkl"
    KNOWLEDGE_PATH = SAVE_DIR / "knowledge.json"
    MEMORY_PATH = SAVE_DIR / "memory.json"

    NUM_MINDS = 5  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö "—É–º–æ–≤"

    VOCAB_SIZE = 20000
    EMB_DIM = 256
    HIDDEN_SIZE = 512
    NUM_LAYERS = 3
    NUM_HEADS = 8
    DROPOUT = 0.1
    MAX_SEQ_LEN = 64
    LEARNING_RATE = 1e-3

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è
    THINKING_ITERATIONS = 3  # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ "–ø–æ–¥—É–º–∞—Ç—å"
    CONSENSUS_THRESHOLD = 0.6  # –ü–æ—Ä–æ–≥ —Å–æ–≥–ª–∞—Å–∏—è –º–µ–∂–¥—É —É–º–∞–º–∏
    CONFIDENCE_TO_ANSWER = 0.65  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞

    QWEN_API = "http://localhost:1234/v1/chat/completions"


Config.SAVE_DIR.mkdir(exist_ok=True)


# ====================== –£–¢–ò–õ–ò–¢–´ ======================
def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return "–•–æ—Ä–æ—à–æ."
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    text = re.sub(r'#{1,3}\s*', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    if text and not text.endswith(('.', '!', '?')):
        text += '.'
    return text or "–•–æ—Ä–æ—à–æ."


def clean_for_tokenize(text: str) -> str:
    text = re.sub(r'[^\w\s]', ' ', text, flags=re.UNICODE)
    return re.sub(r'\s+', ' ', text.lower()).strip()


# ====================== –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô ======================
@dataclass
class KnowledgeEntry:
    """–ó–∞–ø–∏—Å—å –∑–Ω–∞–Ω–∏—è"""
    question: str
    answer: str
    context: str
    confidence: float
    timestamp: str
    source: str  # 'self' –∏–ª–∏ 'qwen'


class KnowledgeBase:
    """–ë–∞–∑–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π"""

    def __init__(self):
        self.entries: List[KnowledgeEntry] = []
        self.index = defaultdict(list)  # —Å–ª–æ–≤–æ -> —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤
        self.load()

    def add(self, question: str, answer: str, context: str = "",
            confidence: float = 1.0, source: str = "qwen"):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–Ω–∞–Ω–∏–µ"""
        entry = KnowledgeEntry(
            question=question,
            answer=answer,
            context=context,
            confidence=confidence,
            timestamp=datetime.now().isoformat(),
            source=source
        )

        self.entries.append(entry)

        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ —Å–ª–æ–≤–∞–º
        words = set(clean_for_tokenize(question).split())
        for word in words:
            self.index[word].append(len(self.entries) - 1)

        self.save()

    def search(self, query: str, top_k: int = 5) -> List[KnowledgeEntry]:
        """–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è"""
        words = set(clean_for_tokenize(query).split())

        # –°—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        scores = Counter()
        for word in words:
            if word in self.index:
                for idx in self.index[word]:
                    scores[idx] += 1

        # –ë–µ—Ä—ë–º —Ç–æ–ø-k
        top_indices = [idx for idx, _ in scores.most_common(top_k)]
        return [self.entries[idx] for idx in top_indices if idx < len(self.entries)]

    def get_all_knowledge(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∫–∞–∫ —Ç–µ–∫—Å—Ç"""
        if not self.entries:
            return ""

        knowledge_text = []
        for entry in self.entries[-20:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20
            knowledge_text.append(f"Q: {entry.question}\nA: {entry.answer}")

        return "\n".join(knowledge_text)

    def save(self):
        data = {
            'entries': [
                {
                    'question': e.question,
                    'answer': e.answer,
                    'context': e.context,
                    'confidence': e.confidence,
                    'timestamp': e.timestamp,
                    'source': e.source
                }
                for e in self.entries
            ]
        }
        with open(Config.KNOWLEDGE_PATH, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        if Config.KNOWLEDGE_PATH.exists():
            try:
                with open(Config.KNOWLEDGE_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for e in data.get('entries', []):
                        entry = KnowledgeEntry(**e)
                        self.entries.append(entry)

                        words = set(clean_for_tokenize(entry.question).split())
                        for word in words:
                            self.index[word].append(len(self.entries) - 1)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∑–Ω–∞–Ω–∏–π: {e}")


# ====================== –£–õ–£–ß–®–ï–ù–ù–´–ô –°–õ–û–í–ê–†–¨ ======================
class ImprovedVocab:
    def __init__(self):
        self.word2idx = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}
        self.idx2word = {0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}
        self.next_id = 4
        self.max_vocab_size = Config.VOCAB_SIZE
        self.load()

    def add_word(self, word: str) -> int:
        word_clean = word.lower().strip()
        if not word_clean or len(word_clean) < 1:
            return 3
        if word_clean in self.word2idx:
            return self.word2idx[word_clean]
        if self.next_id >= self.max_vocab_size:
            return 3

        self.word2idx[word_clean] = self.next_id
        self.idx2word[self.next_id] = word_clean
        self.next_id += 1
        return self.word2idx[word_clean]

    def add_words_from_text(self, text: str):
        words = clean_for_tokenize(text).split()
        for word in words:
            if len(word) > 1:
                self.add_word(word)

    def encode(self, text: str) -> List[int]:
        words = clean_for_tokenize(text).split()
        ids = [1]
        for word in words[:Config.MAX_SEQ_LEN - 2]:
            word_id = self.word2idx.get(word, 3)
            ids.append(min(word_id, self.max_vocab_size - 1))
        ids.append(2)
        while len(ids) < Config.MAX_SEQ_LEN:
            ids.append(0)
        return ids[:Config.MAX_SEQ_LEN]

    def decode(self, ids: List[int]) -> str:
        words = []
        for idx in ids:
            if idx in [0, 2]:
                break
            if idx == 1:
                continue
            if idx in self.idx2word and idx < self.max_vocab_size:
                word = self.idx2word[idx]
                if word not in ['<pad>', '<start>', '<end>', '<unk>']:
                    words.append(word)

        if not words:
            return "–ù–µ –∑–Ω–∞—é."

        unique_words = []
        prev = None
        for w in words[:30]:
            if w != prev:
                unique_words.append(w)
                prev = w

        return ' '.join(unique_words).capitalize() + '.'

    @property
    def size(self):
        return min(len(self.word2idx), self.max_vocab_size)

    def save(self):
        with open(Config.VOCAB_PATH, 'wb') as f:
            pickle.dump({
                'word2idx': self.word2idx,
                'idx2word': self.idx2word,
                'next_id': self.next_id
            }, f)

    def load(self):
        if Config.VOCAB_PATH.exists():
            try:
                with open(Config.VOCAB_PATH, 'rb') as f:
                    data = pickle.load(f)
                    self.word2idx = data['word2idx']
                    self.idx2word = data['idx2word']
                    self.next_id = data.get('next_id', 4)
            except:
                pass


# ====================== –ù–ï–ô–†–û–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê ======================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        if d_model % 2 == 0:
            pe[:, 1::2] = torch.cos(position * div_term)
        else:
            pe[:, 1::2] = torch.cos(position * div_term[:-1])
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]


class MindBrain(nn.Module):
    """–û–¥–Ω–∞ –∫–æ–ø–∏—è '—É–º–∞' """

    def __init__(self, vocab_size: int, device=None):
        super().__init__()
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.vocab_size = vocab_size

        self.embedding = nn.Embedding(vocab_size, Config.EMB_DIM, padding_idx=0)
        self.pos_encoding = PositionalEncoding(Config.EMB_DIM, Config.MAX_SEQ_LEN)

        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=Config.EMB_DIM,
                nhead=Config.NUM_HEADS,
                dim_feedforward=Config.HIDDEN_SIZE,
                dropout=Config.DROPOUT,
                batch_first=True
            ),
            num_layers=Config.NUM_LAYERS
        )

        self.decoder = nn.Linear(Config.EMB_DIM, vocab_size)
        self.to(self.device)

    def forward(self, input_ids):
        input_ids = torch.clamp(input_ids, 0, self.vocab_size - 1)
        x = self.embedding(input_ids)
        x = self.pos_encoding(x)
        padding_mask = (input_ids == 0)
        encoder_out = self.encoder(x, src_key_padding_mask=padding_mask)
        logits = self.decoder(encoder_out)
        return logits

    def generate(self, input_ids, max_len: int = 40, temperature: float = 0.9) -> List[int]:
        self.eval()
        with torch.no_grad():
            input_ids = input_ids.to(self.device)
            output_ids = [1]

            for _ in range(max_len):
                current_seq = torch.tensor([output_ids], device=self.device, dtype=torch.long)
                if current_seq.size(1) < Config.MAX_SEQ_LEN:
                    padding = torch.zeros((1, Config.MAX_SEQ_LEN - current_seq.size(1)),
                                          device=self.device, dtype=torch.long)
                    current_seq = torch.cat([current_seq, padding], dim=1)

                logits = self.forward(input_ids)
                next_logits = logits[0, min(len(output_ids) - 1, Config.MAX_SEQ_LEN - 1), :]
                next_logits = next_logits / temperature
                next_logits[0] = -float('inf')
                next_logits[1] = -float('inf')

                probs = F.softmax(next_logits, dim=0)
                next_token = torch.multinomial(probs, 1).item()

                if next_token == 2:
                    break
                output_ids.append(next_token)

        return output_ids


# ====================== –ö–û–õ–õ–ï–ö–¢–ò–í–ù–´–ô –†–ê–ó–£–ú ======================
class CollectiveMind:
    """–ú–Ω–æ–∂–µ—Å—Ç–≤–æ —É–º–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥—É–º–∞—é—Ç –≤–º–µ—Å—Ç–µ"""

    def __init__(self, vocab: ImprovedVocab, device):
        self.vocab = vocab
        self.device = device

        # –°–æ–∑–¥–∞—ë–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ø–∏–π –º–æ–¥–µ–ª–∏
        self.minds: List[MindBrain] = []
        for i in range(Config.NUM_MINDS):
            mind = MindBrain(vocab.size, device)
            self.minds.append(mind)

        print(f"üß† –°–æ–∑–¥–∞–Ω–æ {Config.NUM_MINDS} —É–º–æ–≤ –¥–ª—è –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è")

    def think_collectively(self, question: str, context: str = "") -> Tuple[str, float]:
        """–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ - –≤—Å–µ —É–º—ã –¥—É–º–∞—é—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""

        print(f"\nüí≠ –ö–û–õ–õ–ï–ö–¢–ò–í–ù–û–ï –ú–´–®–õ–ï–ù–ò–ï ({Config.NUM_MINDS} —É–º–æ–≤):")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        if context:
            full_input = f"{context}\n\n–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"
        else:
            full_input = f"–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"

        input_ids = torch.tensor([self.vocab.encode(full_input)], device=self.device)

        # –ö–∞–∂–¥—ã–π —É–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–π –æ—Ç–≤–µ—Ç
        answers = []
        for i, mind in enumerate(self.minds):
            try:
                generated = mind.generate(input_ids, max_len=35, temperature=0.7 + i * 0.1)
                answer = self.vocab.decode(generated)
                answers.append(answer)
                print(f"  –£–º #{i + 1}: {answer[:60]}...")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –£–º #{i + 1}: –æ—à–∏–±–∫–∞ - {e}")
                answers.append("–ù–µ –∑–Ω–∞—é.")

        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Å–µ–Ω—Å—É—Å
        consensus_answer, confidence = self._find_consensus(answers)

        print(f"\nüéØ –ö–û–ù–°–ï–ù–°–£–°: {consensus_answer[:80]}...")
        print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%}")

        return consensus_answer, confidence

    def _find_consensus(self, answers: List[str]) -> Tuple[str, float]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω—Å–µ–Ω—Å—É—Å –º–µ–∂–¥—É –æ—Ç–≤–µ—Ç–∞–º–∏"""

        if not answers:
            return "–ù–µ –∑–Ω–∞—é.", 0.0

        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã
        valid_answers = [a for a in answers if a and a != "–ù–µ –∑–Ω–∞—é."]

        if not valid_answers:
            return "–ù–µ –∑–Ω–∞—é.", 0.0

        # –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –æ—Ç–≤–µ—Ç–∞–º–∏
        similarities = []
        for i in range(len(valid_answers)):
            for j in range(i + 1, len(valid_answers)):
                sim = self._simple_similarity(valid_answers[i], valid_answers[j])
                similarities.append(sim)

        # –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å = —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = np.mean(similarities) if similarities else 0.0

        # –ï—Å–ª–∏ –≤—ã—Å–æ–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –±–µ—Ä—ë–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        if confidence > Config.CONSENSUS_THRESHOLD:
            best_answer = max(valid_answers, key=len)
        else:
            # –ò–Ω–∞—á–µ –±–µ—Ä—ë–º —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω —Å–ª–æ–≤
            word_counter = Counter()
            for answer in valid_answers:
                words = clean_for_tokenize(answer).split()
                word_counter.update(words)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –∏–∑ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
            common_words = [w for w, c in word_counter.most_common(10)]
            best_answer = ' '.join(common_words).capitalize() + '.'

        return best_answer, confidence

    def _simple_similarity(self, text1: str, text2: str) -> float:
        """–ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        words1 = set(clean_for_tokenize(text1).split())
        words2 = set(clean_for_tokenize(text2).split())

        if not words1 or not words2:
            return 0.0

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0

    def train_all(self, input_ids, target_ids, optimizer):
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ —É–º—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"""
        total_loss = 0.0

        for mind in self.minds:
            mind.train()
            logits = mind(input_ids)
            loss = F.cross_entropy(
                logits.view(-1, self.vocab.size),
                target_ids.view(-1),
                ignore_index=0
            )
            loss.backward()
            total_loss += loss.item()

        return total_loss / len(self.minds)

    def save(self, path: Path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —É–º—ã"""
        for i, mind in enumerate(self.minds):
            mind_path = path.parent / f"{path.stem}_mind{i}.pt"
            torch.save({
                'state_dict': mind.state_dict(),
                'vocab_size': mind.vocab_size
            }, mind_path)

    def load(self, path: Path):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —É–º—ã"""
        loaded = 0
        for i, mind in enumerate(self.minds):
            mind_path = path.parent / f"{path.stem}_mind{i}.pt"
            if mind_path.exists():
                try:
                    checkpoint = torch.load(mind_path, map_location=self.device)
                    mind.load_state_dict(checkpoint['state_dict'])
                    loaded += 1
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É–º–∞ #{i}: {e}")

        return loaded == len(self.minds)


# ====================== –ê–í–¢–û–ù–û–ú–ù–ê–Ø –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê ======================
class AutonomousCognitiveSystem:
    """–°–∏—Å—Ç–µ–º–∞ —Å –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º –º—ã—à–ª–µ–Ω–∏–µ–º"""

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self.vocab = ImprovedVocab()
        self.knowledge = KnowledgeBase()

        # –ë–∞–∑–æ–≤—ã–µ —Å–ª–æ–≤–∞
        base_words = ("–ø—Ä–∏–≤–µ—Ç —Å–ø–∞—Å–∏–±–æ –¥–∞ –Ω–µ—Ç —á—Ç–æ –∫–∞–∫ –ø–æ—á–µ–º—É –≥–¥–µ –∫–æ–≥–¥–∞ –∫—Ç–æ –∫–æ—Ç–æ—Ä—ã–π "
                      "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ–Ω–∏–º–∞—é —É–∑–Ω–∞–ª –Ω–æ–≤–æ–µ —Ö–æ—Ä–æ—à–æ –¥—É–º–∞—é –∑–Ω–∞—é —Å—á–∏—Ç–∞—é –ø–æ–ª–∞–≥–∞—é "
                      "–º–Ω–µ –∫–∞–∂–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ –≤–µ—Ä–æ—è—Ç–Ω–æ").split()
        for word in base_words:
            self.vocab.add_word(word)

        print(f"üìö –°–ª–æ–≤–∞—Ä—å: {self.vocab.size} —Å–ª–æ–≤")

        # –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º
        self.collective_mind = CollectiveMind(self.vocab, self.device)

        if self.collective_mind.load(Config.MODEL_PATH):
            print("‚úÖ –£–º—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        else:
            print("üÜï –ù–æ–≤—ã–µ —É–º—ã —Å–æ–∑–¥–∞–Ω—ã")

        self.vocab.save()

    def autonomous_answer(self, question: str) -> Tuple[str, bool]:
        """–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –æ—Ç–≤–µ—Ç - –¥—É–º–∞–µ—Ç —Å–∞–º, –±–µ–∑ –ø–æ–º–æ—â–∏ Qwen"""

        print("\n" + "=" * 70)
        print(f"üë§ –í–û–ü–†–û–°: {question}")
        print("=" * 70)

        # 1. –ò—â–µ–º –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        print("\nüîç –ò—â—É –≤ –ø–∞–º—è—Ç–∏...")
        relevant_knowledge = self.knowledge.search(question, top_k=3)

        context = ""
        if relevant_knowledge:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(relevant_knowledge)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π")
            for i, entry in enumerate(relevant_knowledge, 1):
                print(f"  {i}. {entry.question[:50]}... (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {entry.confidence:.1%})")
                context += f"{entry.question} -> {entry.answer}\n"
        else:
            print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–º—è—Ç–∏")

        # 2. –î—É–º–∞–µ–º –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
        best_answer = None
        best_confidence = 0.0

        for iteration in range(Config.THINKING_ITERATIONS):
            print(f"\nü§î –ò—Ç–µ—Ä–∞—Ü–∏—è –º—ã—à–ª–µ–Ω–∏—è #{iteration + 1}:")

            answer, confidence = self.collective_mind.think_collectively(question, context)

            if confidence > best_confidence:
                best_answer = answer
                best_confidence = confidence

            # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –º–æ–∂–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
            if confidence >= Config.CONFIDENCE_TO_ANSWER:
                print(f"‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {confidence:.1%} - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è")
                break

        # 3. –†–µ—à–∞–µ–º, –º–æ–∂–µ–º –ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å —Å–∞–º–∏
        can_answer_autonomously = best_confidence >= Config.CONFIDENCE_TO_ANSWER

        if can_answer_autonomously:
            print(f"\n‚úÖ –ú–û–ì–£ –û–¢–í–ï–¢–ò–¢–¨ –°–ê–ú–û–°–¢–û–Ø–¢–ï–õ–¨–ù–û (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_confidence:.1%})")
        else:
            print(f"\n‚ùì –ù–ï –£–í–ï–†–ï–ù (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_confidence:.1%}) - –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å Qwen")

        return best_answer, can_answer_autonomously

    def learn_from_qwen(self, question: str):
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –æ—Ç Qwen –∏ —É—á–∏—Ç—Å—è"""

        print("\nüë®‚Äçüè´ –û–±—Ä–∞—â–∞—é—Å—å –∫ Qwen –∑–∞ –∑–Ω–∞–Ω–∏—è–º–∏...")

        try:
            resp = requests.post(Config.QWEN_API, json={
                "messages": [
                    {"role": "user", "content": f"{question}\n\n–î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π, —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)."}],
                "max_tokens": 150,
                "temperature": 0.7
            }, timeout=20)

            if resp.status_code == 200:
                teacher_answer = clean_text(resp.json()['choices'][0]['message']['content'])
                print(f"üë®‚Äçüè´ QWEN: {teacher_answer}")

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
                self.knowledge.add(question, teacher_answer, source='qwen', confidence=1.0)

                # –û–±—É—á–∞–µ–º –≤—Å–µ —É–º—ã –Ω–∞ —ç—Ç–æ–º –∑–Ω–∞–Ω–∏–∏
                self._train_minds(question, teacher_answer)

                return teacher_answer
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Qwen: {e}")

        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç."

    def _train_minds(self, question: str, answer: str):
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ —É–º—ã –Ω–∞ –Ω–æ–≤–æ–º –∑–Ω–∞–Ω–∏–∏"""

        print("\nüîÑ –û–±—É—á–∞—é –≤—Å–µ —É–º—ã...")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä—å
        self.vocab.add_words_from_text(question)
        self.vocab.add_words_from_text(answer)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –≤ —É–º–∞—Ö –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.collective_mind.minds[0].vocab_size < self.vocab.size:
            print(f"üìö –°–ª–æ–≤–∞—Ä—å –≤—ã—Ä–æ—Å: {self.collective_mind.minds[0].vocab_size} ‚Üí {self.vocab.size}")
            for mind in self.collective_mind.minds:
                old_embedding = mind.embedding.weight.data
                mind.embedding = nn.Embedding(self.vocab.size, Config.EMB_DIM, padding_idx=0)
                mind.embedding.weight.data[:old_embedding.size(0)] = old_embedding
                mind.decoder = nn.Linear(Config.EMB_DIM, self.vocab.size)
                mind.vocab_size = self.vocab.size
                mind.to(self.device)

        # –ö–æ–¥–∏—Ä—É–µ–º
        input_ids = torch.tensor([self.vocab.encode(question)], device=self.device)
        target_ids = torch.tensor([self.vocab.encode(answer)], device=self.device)

        # –°–æ–∑–¥–∞—ë–º –µ–¥–∏–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—Å–µ—Ö —É–º–æ–≤
        all_params = []
        for mind in self.collective_mind.minds:
            all_params.extend(mind.parameters())

        optimizer = torch.optim.AdamW(all_params, lr=Config.LEARNING_RATE)

        # –û–±—É—á–∞–µ–º
        for epoch in range(10):
            optimizer.zero_grad()
            avg_loss = self.collective_mind.train_all(input_ids, target_ids, optimizer)
            torch.nn.utils.clip_grad_norm_(all_params, 1.0)
            optimizer.step()

            if epoch % 3 == 0:
                print(f"  –≠–ø–æ—Ö–∞ {epoch}: loss={avg_loss:.3f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        self.collective_mind.save(Config.MODEL_PATH)
        self.vocab.save()

        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    def process(self, question: str):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞"""

        # 1. –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ
        my_answer, can_answer = self.autonomous_answer(question)

        if can_answer:
            # –û—Ç–≤–µ—á–∞–µ–º —Å–∞–º–∏
            print(f"\nüí° –ú–û–ô –ê–í–¢–û–ù–û–ú–ù–´–ô –û–¢–í–ï–¢: {my_answer}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ
            self.knowledge.add(question, my_answer, source='self', confidence=0.8)

            return my_answer
        else:
            # –ù—É–∂–Ω–∞ –ø–æ–º–æ—â—å Qwen
            print(f"\n‚ö†Ô∏è –ú–æ–π –æ—Ç–≤–µ—Ç: {my_answer}")
            print("‚ùì –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ—Ä–µ–Ω - —É—á—É—Å—å —É Qwen...")

            teacher_answer = self.learn_from_qwen(question)

            print(f"\nüí° –ò–¢–û–ì–û–í–´–ô –û–¢–í–ï–¢: {teacher_answer}")

            return teacher_answer


# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def main():
    print("\n" + "=" * 70)
    print("üß† –ê–í–¢–û–ù–û–ú–ù–ê–Ø –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê v11.0")
    print("–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–º—ã ‚Ä¢ –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ ‚Ä¢ –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ")
    print("=" * 70)

    try:
        system = AutonomousCognitiveSystem()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        traceback.print_exc()
        return

    print(f"\nüí° –ö–û–ú–ê–ù–î–´:")
    print(f" '–≤—ã—Ö–æ–¥' - –∑–∞–≤–µ—Ä—à–∏—Ç—å")
    print(f" '–∑–Ω–∞–Ω–∏—è' - –ø–æ–∫–∞–∑–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
    print(f" '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞' - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è")

    while True:
        try:
            user_input = input("\nüë§ –í–´: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
                print("\n‚ú® –î–æ –≤—Å—Ç—Ä–µ—á–∏!")
                break

            if user_input.lower() == '–∑–Ω–∞–Ω–∏—è':
                entries = system.knowledge.entries[-10:]
                if entries:
                    print("\nüìö –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10):")
                    for i, entry in enumerate(entries, 1):
                        source_icon = "ü§ñ" if entry.source == "self" else "üë®‚Äçüè´"
                        print(f"\n{i}. {source_icon} {entry.question}")
                        print(f"   ‚Üí {entry.answer[:80]}...")
                        print(f"   üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {entry.confidence:.1%} | –ò—Å—Ç–æ—á–Ω–∏–∫: {entry.source}")
                else:
                    print("\nüìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞")
                continue

            if user_input.lower() == '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞':
                total = len(system.knowledge.entries)
                self_learned = sum(1 for e in system.knowledge.entries if e.source == 'self')
                qwen_learned = sum(1 for e in system.knowledge.entries if e.source == 'qwen')

                print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–£–ß–ï–ù–ò–Ø:")
                print(f"  –í—Å–µ–≥–æ –∑–Ω–∞–Ω–∏–π: {total}")
                print(f"  ü§ñ –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ: {self_learned} ({self_learned / total * 100 if total else 0:.1f}%)")
                print(f"  üë®‚Äçüè´ –û—Ç Qwen: {qwen_learned} ({qwen_learned / total * 100 if total else 0:.1f}%)")
                print(f"  üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {system.vocab.size} —Å–ª–æ–≤")
                print(f"  üß† –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–º–æ–≤: {Config.NUM_MINDS}")
                continue

            # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞
            system.process(user_input)

        except KeyboardInterrupt:
            print("\n‚ú® –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            traceback.print_exc()


if __name__ == "__main__":
    main()