# coding: utf-8
"""
AGI_CognitiveReasoning.py
–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏, —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π –∏ –æ–±—É—á–µ–Ω–∏–µ–º
"""
import os
import re
import json
import pickle
import random
import traceback
import math
from collections import Counter, defaultdict, deque
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple, Any
from pathlib import Path
import numpy as np
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from sentence_transformers import SentenceTransformer

    _HAS_ST_MODEL = True
except:
    _HAS_ST_MODEL = False


# ======================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ======================
class Config:
    SAVE_DIR = Path("./cognitive_model_data")
    MODEL_PATH = SAVE_DIR / "model_superintel.pt"
    VOCAB_PATH = SAVE_DIR / "vocab_superintel.pkl"
    MEMORY_PATH = SAVE_DIR / "memory_superintel.json"
    LEARNING_PATH = SAVE_DIR / "learning_superintel.json"

    VOCAB_SIZE = 15000
    EMB_DIM = 512
    HIDDEN_SIZE = 1024
    NUM_LAYERS = 4
    NUM_HEADS = 8
    DROPOUT = 0.2
    MAX_SEQ_LEN = 150

    LEARNING_RATE = 2e-4
    MAX_ATTEMPTS = 20
    CONTEXT_SIZE = 30

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º—ã—à–ª–µ–Ω–∏—è
    CONFIDENCE_THRESHOLD = 0.75  # –ü—Ä–∏ –∫–∞–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è –≥–æ—Ç–æ–≤—ã–º
    REFLECTION_DEPTH = 5  # –ì–ª—É–±–∏–Ω–∞ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏
    QWEN_API = "http://localhost:1234/v1/chat/completions"


Config.SAVE_DIR.mkdir(exist_ok=True)


# ======================
# –°–ò–°–¢–ï–ú–ê –ú–´–®–õ–ï–ù–ò–Ø –ò –†–ï–§–õ–ï–ö–°–ò–ò
# ======================
class ThoughtProcess:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è –ê–ò"""

    def __init__(self):
        self.thoughts = []
        self.confidence = 0.0
        self.doubts = []
        self.reasoning_steps = []
        self.final_answer = ""
        self.learning_occurred = False

    def add_thought(self, thought: str, confidence: float = 0.5):
        """–î–æ–±–∞–≤–∏—Ç—å –º—ã—Å–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å"""
        self.thoughts.append({
            'text': thought,
            'confidence': confidence,
            'timestamp': datetime.now().isoformat()
        })
        self.confidence = np.mean([t['confidence'] for t in self.thoughts])

    def add_doubt(self, doubt: str):
        """–í—ã—Ä–∞–∑–∏—Ç—å —Å–æ–º–Ω–µ–Ω–∏–µ"""
        self.doubts.append({
            'text': doubt,
            'timestamp': datetime.now().isoformat()
        })

    def add_reasoning_step(self, step: str):
        """–î–æ–±–∞–≤–∏—Ç—å —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        self.reasoning_steps.append(step)

    def is_confident(self, threshold: float = Config.CONFIDENCE_THRESHOLD) -> bool:
        """–£–≤–µ—Ä–µ–Ω –ª–∏ –≤ –æ—Ç–≤–µ—Ç–µ"""
        return self.confidence >= threshold

    def __str__(self):
        result = "üß† –ü–†–û–¶–ï–°–° –ú–´–®–õ–ï–ù–ò–Ø:\n"
        if self.thoughts:
            result += "üí≠ –ú—ã—Å–ª–∏:\n"
            for t in self.thoughts[-3:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –º—ã—Å–ª–∏
                result += f"  ‚Ä¢ {t['text']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {t['confidence']:.1%})\n"
        if self.doubts:
            result += "‚ùì –°–æ–º–Ω–µ–Ω–∏—è:\n"
            for d in self.doubts[-2:]:
                result += f"  ‚Ä¢ {d['text']}\n"
        if self.reasoning_steps:
            result += "üìç –õ–æ–≥–∏–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è:\n"
            for i, step in enumerate(self.reasoning_steps[-3:], 1):
                result += f"  {i}. {step}\n"
        result += f"üìä –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {self.confidence:.1%}\n"
        return result


# ======================
# –ú–ï–ù–ï–î–ñ–ï–† –û–ë–£–ß–ï–ù–ò–Ø
# ======================
class LearningManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –ê–ò"""

    def __init__(self):
        self.knowledge_base = {}
        self.learning_history = []
        self.skill_level = 0.1  # –£—Ä–æ–≤–µ–Ω—å –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞ (0-1)
        self.asked_questions_count = 0
        self.correct_answers_count = 0
        self.load()

    def record_learning(self, topic: str, concept: str, teacher_answer: str,
                        ai_answer: str, similarity: float):
        """–ó–∞–ø–∏—Å–∞—Ç—å —Ñ–∞–∫—Ç –æ–±—É—á–µ–Ω–∏—è"""
        record = {
            'topic': topic,
            'concept': concept,
            'teacher_answer': teacher_answer,
            'ai_answer': ai_answer,
            'similarity': similarity,
            'timestamp': datetime.now().isoformat(),
            'skill_improvement': similarity
        }
        self.learning_history.append(record)

        # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        if topic not in self.knowledge_base:
            self.knowledge_base[topic] = []
        self.knowledge_base[topic].append({
            'concept': concept,
            'answer': teacher_answer,
            'learned': True
        })

        # –û–±–Ω–æ–≤–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞
        self.update_skill_level(similarity)
        self.save()

    def update_skill_level(self, similarity: float):
        """–û–±–Ω–æ–≤–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞"""
        new_level = (self.skill_level * len(self.learning_history) + similarity) / (len(self.learning_history) + 1)
        self.skill_level = min(1.0, new_level)
        self.correct_answers_count += int(similarity > 0.7)
        self.asked_questions_count += 1

    def get_known_topics(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–µ–º—ã"""
        return list(self.knowledge_base.keys())

    def get_topic_knowledge(self, topic: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ"""
        return self.knowledge_base.get(topic, [])

    def should_ask_teacher(self, confidence: float) -> bool:
        """–ù—É–∂–Ω–æ –ª–∏ —Å–ø—Ä–æ—Å–∏—Ç—å —É—á–∏—Ç–µ–ª—è"""
        return confidence < Config.CONFIDENCE_THRESHOLD

    def get_learning_progress(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        total = len(self.learning_history)
        if total == 0:
            return "–û–±—É—á–µ–Ω–∏–µ –µ—â–µ –Ω–µ –Ω–∞—á–∞–ª–æ—Å—å"

        accuracy = self.correct_answers_count / self.asked_questions_count if self.asked_questions_count > 0 else 0
        return f"–£—Ä–æ–≤–µ–Ω—å –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞: {self.skill_level:.1%} | –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%} | –í—ã—É—á–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π: {len(self.knowledge_base)}"

    def save(self):
        data = {
            'knowledge_base': self.knowledge_base,
            'learning_history': self.learning_history,
            'skill_level': self.skill_level,
            'asked_questions_count': self.asked_questions_count,
            'correct_answers_count': self.correct_answers_count
        }
        with open(Config.LEARNING_PATH, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        if Config.LEARNING_PATH.exists():
            try:
                with open(Config.LEARNING_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.knowledge_base = data.get('knowledge_base', {})
                    self.learning_history = data.get('learning_history', [])
                    self.skill_level = data.get('skill_level', 0.1)
                    self.asked_questions_count = data.get('asked_questions_count', 0)
                    self.correct_answers_count = data.get('correct_answers_count', 0)
            except:
                pass


# ======================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ======================
def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return "–•–æ—Ä–æ—à–æ."
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    text = re.sub(r'#{1,3}\s*', '', text)
    text = re.sub(r'>\s*', '', text)
    text = re.sub(r'\r\n|\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'^[\*\.\!\?\:\-\‚Äì‚Äî\s]+', '', text)
    text = re.sub(r'[\*\.\!\?\:\-\‚Äì‚Äî\s]+$', '', text)
    words = text.split()
    if len(words) > 150:
        text = ' '.join(words[:150])
        if not text.endswith(('.', '!', '?')):
            text += '.'
    return text or "–•–æ—Ä–æ—à–æ."


def clean_for_similarity(text: str) -> str:
    text = re.sub(r'[^\w\s]', ' ', text, flags=re.UNICODE)
    return re.sub(r'\s+', ' ', text).lower().strip()


def detect_input_type(user_input: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞"""
    s = user_input.lower().strip()
    patterns = {
        "SOCIAL": r'\b(–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π|–¥–æ–±—Ä—ã–π –¥–µ–Ω—å|–∫–∞–∫ –¥–µ–ª–∞|–ø–æ–∫–∞|—Å–ø–∞—Å–∏–±–æ|–±–ª–∞–≥–æ–¥–∞—Ä—é)\b',
        "FACT": r'\b(—á—Ç–æ —Ç–∞–∫–æ–µ|–∫—Ç–æ —Ç–∞–∫–æ–π|–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è|–∫–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞|—Ñ–æ—Ä–º—É–ª–∞|–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|—Ä–∞—Å—Å–∫–∞–∂–∏ –æ)\b',
        "REASON": r'\b(–ø–æ—á–µ–º—É|–∑–∞—á–µ–º|–æ—Ç—á–µ–≥–æ|–ø—Ä–∏—á–∏–Ω–∞|–∫–∞–∫ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç|–æ–±—ä—è—Å–Ω–∏ –º–µ—Ö–∞–Ω–∏–∑–º)\b',
        "PROCESS": r'\b(–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å|–∫–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è|—à–∞–≥|–∞–ª–≥–æ—Ä–∏—Ç–º|–ø–æ—à–∞–≥–æ–≤—ã–π)\b',
        "OPINION": r'\b(–∫–∞–∫ —Ç—ã –¥—É–º–∞–µ—à—å|—Ç–≤–æ—ë –º–Ω–µ–Ω–∏–µ|–ª—É—á—à–µ –ª–∏|–Ω—Ä–∞–≤–∏—Ç—Å—è –ª–∏|—Å–æ–≥–ª–∞—Å–µ–Ω –ª–∏)\b',
        "CREATIVE": r'\b(–ø—Ä–µ–¥—Å—Ç–∞–≤—å|–≤–æ–æ–±—Ä–∞–∑–∏|—Å–æ—á–∏–Ω–∏|–æ–ø–∏—à–∏ –∫–∞–∫|–º–µ—Ç–∞—Ñ–æ—Ä–∞|–∏—Å—Ç–æ—Ä–∏—è|—Å–æ–∑–¥–∞–π)\b',
        "ANALYSIS": r'\b(–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π|—Å—Ä–∞–≤–Ω–∏|—Ä–∞–∑–ª–∏—á–∏–µ|—Å—Ö–æ–¥—Å—Ç–≤–æ|–∞–Ω–∞–ª–∏–∑|—Ç–µ–Ω–¥–µ–Ω—Ü–∏—è)\b',
    }
    for qtype, pattern in patterns.items():
        if re.search(pattern, s):
            return qtype
    return "FACT"


# ======================
# –†–ê–°–®–ò–†–ï–ù–ù–´–ô VOCABULARY
# ======================
class AdvancedVocabManager:
    def __init__(self):
        self.word2idx = {
            '<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3,
            '<START>': 4, '<FACT>': 5, '<REASON>': 6, '<PROC>': 7,
            '<EMOTION>': 8, '<CONCEPT>': 9, '<ENTITY>': 10,
        }
        self.idx2word = {v: k for k, v in self.word2idx.items()}
        self.word_freq = Counter()
        self.next_id = 11
        self.semantic_tags = defaultdict(list)

    def add_word(self, word: str, semantic_tag: Optional[str] = None) -> int:
        word_lower = word.lower()
        if word_lower not in self.word2idx:
            if self.next_id < Config.VOCAB_SIZE:
                self.word2idx[word_lower] = self.next_id
                self.idx2word[self.next_id] = word_lower
                self.next_id += 1

        self.word_freq[word_lower] += 1
        if semantic_tag:
            self.semantic_tags[word_lower].append(semantic_tag)

        return self.word2idx.get(word_lower, self.word2idx['<UNK>'])

    def add_words(self, words: List[str]):
        for w in words:
            if w.strip():
                self.add_word(w)

    def tokenize(self, text: str) -> List[int]:
        words = clean_for_similarity(text).split()
        return [self.word2idx.get(w, self.word2idx['<UNK>']) for w in words]

    def decode(self, ids: List[int]) -> str:
        tokens = [self.idx2word.get(i, '<UNK>') for i in ids if i not in [0, 1, 2]]
        text = ' '.join(tokens)
        text = re.sub(r'<[^>]+>', '', text)
        return text.strip()

    @property
    def size(self):
        return len(self.word2idx)

    def save(self):
        data = {
            'word2idx': self.word2idx,
            'idx2word': self.idx2word,
            'word_freq': dict(self.word_freq),
            'next_id': self.next_id,
            'semantic_tags': dict(self.semantic_tags)
        }
        with open(Config.VOCAB_PATH, 'wb') as f:
            pickle.dump(data, f)

    def load(self):
        if Config.VOCAB_PATH.exists():
            with open(Config.VOCAB_PATH, 'rb') as f:
                data = pickle.load(f)
                self.word2idx = data['word2idx']
                self.idx2word = data['idx2word']
                self.word_freq = Counter(data['word_freq'])
                self.next_id = data['next_id']
                self.semantic_tags = defaultdict(list, data.get('semantic_tags', {}))
            return True
        return False


# ======================
# –ü–û–ó–ò–¶–ò–û–ù–ù–û–ï –ö–û–î–ò–†–û–í–ê–ù–ò–ï
# ======================
class PositionalEncoding(nn.Module):
    def __init__(self, emb_dim: int, max_len: int = 5000):
        super().__init__()
        self.emb_dim = emb_dim

        pe = torch.zeros(max_len, emb_dim)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(10000.0) / emb_dim))

        pe[:, 0::2] = torch.sin(position * div_term)
        if emb_dim % 2 == 1:
            pe[:, 1::2] = torch.cos(position * div_term[:-1])
        else:
            pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]


# ======================
# MULTI-HEAD ATTENTION
# ======================
class MultiHeadAttention(nn.Module):
    def __init__(self, emb_dim: int, num_heads: int):
        super().__init__()
        assert emb_dim % num_heads == 0
        self.emb_dim = emb_dim
        self.num_heads = num_heads
        self.head_dim = emb_dim // num_heads
        self.scale = self.head_dim ** -0.5

        self.query = nn.Linear(emb_dim, emb_dim)
        self.key = nn.Linear(emb_dim, emb_dim)
        self.value = nn.Linear(emb_dim, emb_dim)
        self.fc_out = nn.Linear(emb_dim, emb_dim)

    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]

        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        weights = F.softmax(scores, dim=-1)
        context = torch.matmul(weights, V)

        context = context.transpose(1, 2).contiguous()
        context = context.view(batch_size, -1, self.emb_dim)
        output = self.fc_out(context)

        return output, weights


# ======================
# –¢–†–ê–ù–°–§–û–†–ú–ï–† –ë–õ–û–ö
# ======================
class TransformerBlock(nn.Module):
    def __init__(self, emb_dim: int, num_heads: int, hidden_size: int, dropout: float = 0.1):
        super().__init__()
        self.attention = MultiHeadAttention(emb_dim, num_heads)
        self.norm1 = nn.LayerNorm(emb_dim)
        self.norm2 = nn.LayerNorm(emb_dim)

        self.ffn = nn.Sequential(
            nn.Linear(emb_dim, hidden_size),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, emb_dim),
            nn.Dropout(dropout)
        )

    def forward(self, x, mask=None):
        attn_out, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + attn_out)
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        return x


# ======================
# –°–£–ü–ï–† –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –°–ï–¢–¨
# ======================
class SuperIntelligentBrain(nn.Module):
    def __init__(self, vocab_size: int, device=None):
        super().__init__()
        self.device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))
        self.vocab_size = vocab_size
        self.emb_dim = Config.EMB_DIM
        self.hidden_size = Config.HIDDEN_SIZE

        self.embedding = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)
        self.pos_encoding = PositionalEncoding(self.emb_dim, Config.MAX_SEQ_LEN)
        self.embedding_dropout = nn.Dropout(Config.DROPOUT)

        self.encoder_blocks = nn.ModuleList([
            TransformerBlock(self.emb_dim, Config.NUM_HEADS, self.hidden_size, Config.DROPOUT)
            for _ in range(Config.NUM_LAYERS)
        ])

        self.decoder_blocks = nn.ModuleList([
            TransformerBlock(self.emb_dim, Config.NUM_HEADS, self.hidden_size, Config.DROPOUT)
            for _ in range(Config.NUM_LAYERS)
        ])

        self.cross_attentions = nn.ModuleList([
            MultiHeadAttention(self.emb_dim, Config.NUM_HEADS)
            for _ in range(Config.NUM_LAYERS)
        ])

        self.output_proj = nn.Sequential(
            nn.Linear(self.emb_dim, self.hidden_size),
            nn.GELU(),
            nn.Dropout(Config.DROPOUT),
            nn.Linear(self.hidden_size, vocab_size)
        )

        self.memory_bank = None
        self.concept_bank = defaultdict(list)

        self.to(self.device)

    def encode(self, input_ids: torch.Tensor):
        emb = self.embedding(input_ids)
        emb = self.pos_encoding(emb)
        emb = self.embedding_dropout(emb)

        for block in self.encoder_blocks:
            emb = block(emb)

        self.memory_bank = emb
        return emb

    def decode_with_attention(self, target_ids: torch.Tensor, encoder_output: torch.Tensor):
        emb = self.embedding(target_ids)
        emb = self.pos_encoding(emb)
        emb = self.embedding_dropout(emb)

        for i, block in enumerate(self.decoder_blocks):
            emb = block(emb)
            cross_out, _ = self.cross_attentions[i](emb, encoder_output, encoder_output)
            emb = emb + cross_out

        return emb

    def generate(self, input_ids: torch.Tensor, max_len: int = 80, temperature: float = 0.9) -> List[int]:
        was_training = self.training
        self.eval()

        with torch.no_grad():
            encoder_output = self.encode(input_ids)
            batch_size = input_ids.size(0)
            current_tokens = torch.full((batch_size, 1), 1, device=self.device, dtype=torch.long)
            generated = []

            for step in range(max_len):
                decoder_output = self.decode_with_attention(current_tokens, encoder_output)
                logits = self.output_proj(decoder_output[:, -1, :])

                probs = F.softmax(logits / temperature, dim=-1)

                top_k = min(50, probs.size(-1))
                top_k_probs, top_k_indices = torch.topk(probs, top_k)
                top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)
                next_token = top_k_indices[0, torch.multinomial(top_k_probs[0], 1)]

                token_id = next_token.item()
                if token_id == 2:
                    break

                generated.append(token_id)
                current_tokens = torch.cat([current_tokens, next_token.view(batch_size, 1)], dim=1)

            if was_training:
                self.train()

            return generated

    def save(self):
        torch.save({
            'model_state': self.state_dict(),
            'concept_bank': dict(self.concept_bank),
        }, Config.MODEL_PATH)

    def load(self):
        if Config.MODEL_PATH.exists():
            checkpoint = torch.load(Config.MODEL_PATH, map_location=self.device)
            self.load_state_dict(checkpoint['model_state'])
            self.concept_bank = defaultdict(list, checkpoint.get('concept_bank', {}))
            return True
        return False


# ======================
# –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –û–¶–ï–ù–ö–ê
# ======================
class SemanticEvaluator:
    def __init__(self):
        self.model = None
        if _HAS_ST_MODEL:
            try:
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
            except:
                pass

    def similarity(self, a: str, b: str) -> float:
        if not a or not b:
            return 0.0
        if self.model is not None:
            try:
                emb = self.model.encode([a, b], normalize_embeddings=True)
                return float(np.dot(emb[0], emb[1]))
            except:
                pass

        a_clean = set(clean_for_similarity(a).split())
        b_clean = set(clean_for_similarity(b).split())
        if not a_clean or not b_clean:
            return 0.0
        return len(a_clean & b_clean) / len(a_clean | b_clean)


# ======================
# –ü–†–û–î–í–ò–ù–£–¢–´–ô –£–ß–ò–¢–ï–õ–¨ –° –†–ï–§–õ–ï–ö–°–ò–ï–ô
# ======================
class SupervisedTeacher:
    def __init__(self):
        self.api_url = Config.QWEN_API
        self.evaluator = SemanticEvaluator()
        self.learning_manager = LearningManager()
        self.step_count = 0

    def ask_teacher(self, prompt: str) -> str:
        """–°–ø—Ä–æ—Å–∏—Ç—å —É —Å—Ç–∞—Ä—à–µ–π –º–æ–¥–µ–ª–∏"""
        try:
            resp = requests.post(self.api_url, json={
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 300,
                "temperature": 0.8
            }, timeout=25)
            if resp.status_code == 200:
                return clean_text(resp.json()['choices'][0]['message']['content'])
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ API: {e}")
        return "–Ø –Ω–µ –∑–Ω–∞—é –æ—Ç–≤–µ—Ç–∞."

    def generate_thoughts(self, user_input: str, input_type: str) -> ThoughtProcess:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º—ã—Å–ª–∏ –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º"""
        thought_process = ThoughtProcess()

        # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è –º—ã—Å–ª—å
        thought_process.add_thought(
            f"–ú–Ω–µ –∑–∞–¥–∞–ª–∏ –≤–æ–ø—Ä–æ—Å —Ç–∏–ø–∞ '{input_type}': '{user_input}'",
            confidence=0.6
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –∑–Ω–∞–Ω–∏–π
        known_topics = self.learning_manager.get_known_topics()
        skill_level = self.learning_manager.skill_level

        if skill_level < 0.3:
            thought_process.add_thought(
                "–Ø –µ—â—ë –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è, –Ω—É–∂–Ω–æ –±—ã—Ç—å –æ—Å—Ç–æ—Ä–æ–∂–Ω–µ–µ —Å –æ—Ç–≤–µ—Ç–∞–º–∏",
                confidence=0.8
            )
            thought_process.add_doubt("–Ø –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–ø—ã—Ç–µ–Ω, –≤–æ–∑–º–æ–∂–Ω–æ —è –æ—à–∏–±–∞—é—Å—å")
        elif skill_level > 0.7:
            thought_process.add_thought(
                "–Ø —Ö–æ—Ä–æ—à–æ –æ–±—É—á–∏–ª—Å—è, –º–æ–≥—É –¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã",
                confidence=0.85
            )

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        key_words = [w for w in clean_for_similarity(user_input).split() if len(w) > 3]
        if key_words:
            thought_process.add_reasoning_step(
                f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(key_words[:3])}"
            )

        return thought_process

    def train_step(self, model: SuperIntelligentBrain, vocab: AdvancedVocabManager,
                   user_input: str, input_type: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π"""

        print(f"\nüë§ –í—ã: {user_input}")
        print(f"üìã –¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞: {input_type}")

        # ========== –≠–¢–ê–ü 1: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ú–´–°–õ–ï–ô ==========
        print("\nüß† –ú—ã—à–ª–µ–Ω–∏–µ...")
        thought_process = self.generate_thoughts(user_input, input_type)
        print(thought_process)

        # ========== –≠–¢–ê–ü 2: –ü–û–ü–´–¢–ö–ê –û–¢–í–ï–¢–ò–¢–¨ –°–ê–ú–û–°–¢–û–Ø–¢–ï–õ–¨–ù–û ==========
        print("\nüîÑ –ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∏—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ...")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ª–æ–≤–∞—Ä—å
        vocab.add_words(clean_for_similarity(user_input).split())

        input_tokens = vocab.tokenize(user_input)
        if not input_tokens:
            input_tokens = [1, 3]

        encoder_input = torch.tensor([input_tokens[:Config.MAX_SEQ_LEN]], device=model.device)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        gen_ids = model.generate(encoder_input, max_len=80, temperature=0.85)
        ai_answer = vocab.decode(gen_ids)

        print(f"üí≠ –ú–æ–π –æ—Ç–≤–µ—Ç: {ai_answer}")

        # ========== –≠–¢–ê–ü 3: –ü–†–û–í–ï–†–ö–ê –£–í–ï–†–ï–ù–ù–û–°–¢–ò ==========
        if thought_process.is_confident():
            print("\n‚úÖ –Ø —É–≤–µ—Ä–µ–Ω –≤ —Å–≤–æ–µ–º –æ—Ç–≤–µ—Ç–µ!")
            vocab.add_words(clean_for_similarity(ai_answer).split())
            return ai_answer
        else:
            thought_process.add_doubt("–Ø –Ω–µ —Å–æ–≤—Å–µ–º —É–≤–µ—Ä–µ–Ω –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞")
            print("\n‚ùì –Ø –Ω–µ —É–≤–µ—Ä–µ–Ω. –°–ø—Ä–∞—à–∏–≤–∞—é —É —É—á–∏—Ç–µ–ª—è...")

        # ========== –≠–¢–ê–ü 4: –û–ë–£–ß–ï–ù–ò–ï –£ –£–ß–ò–¢–ï–õ–Ø ==========
        print("\nüë®‚Äçüè´ –°–ø—Ä–∞—à–∏–≤–∞—é —É —Å—Ç–∞—Ä—à–µ–π –º–æ–¥–µ–ª–∏...")
        teacher_answer = self.ask_teacher(user_input)
        print(f"üë®‚Äçüè´ –£—á–∏—Ç–µ–ª—å: {teacher_answer}")

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity = self.evaluator.similarity(ai_answer, teacher_answer)
        print(f"üìä –°—Ö–æ–¥—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤: {similarity:.1%}")

        # ========== –≠–¢–ê–ü 5: –ê–ù–ê–õ–ò–ó –ò –û–ë–£–ß–ï–ù–ò–ï ==========
        print("\nüìö –û–±—É—á–µ–Ω–∏–µ...")

        target_tokens = vocab.tokenize(teacher_answer)
        if not target_tokens:
            target_tokens = [3]

        vocab.add_words(clean_for_similarity(teacher_answer).split())

        # –ü–µ—Ä–µ—Å—á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        target_tokens = vocab.tokenize(teacher_answer)
        if not target_tokens:
            target_tokens = [3]

        target_ids = torch.tensor([[1] + target_tokens[:Config.MAX_SEQ_LEN - 1]], device=model.device)
        target_out = torch.tensor([target_tokens[:Config.MAX_SEQ_LEN - 1] + [2]], device=model.device)

        # –¢—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        model.train()
        optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)

        best_sim = similarity
        best_response = ai_answer
        improvements = []

        for attempt in range(1, Config.MAX_ATTEMPTS + 1):
            model.train()
            optimizer.zero_grad()

            encoder_output = model.encode(encoder_input)
            decoder_output = model.decode_with_attention(target_ids, encoder_output)
            logits = model.output_proj(decoder_output)

            loss = F.cross_entropy(logits.view(-1, model.vocab_size), target_out.view(-1), ignore_index=0)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            gen_ids = model.generate(encoder_input, max_len=80, temperature=0.8)
            brain_answer = vocab.decode(gen_ids)

            new_sim = self.evaluator.similarity(teacher_answer, brain_answer)

            improvement = "üìà" if new_sim > best_sim else "üìâ" if new_sim < best_sim else "‚û°Ô∏è"
            print(f"  üîÅ –ò—Ç–µ—Ä–∞—Ü–∏—è {attempt}: loss={loss.item():.4f}, —Å—Ö–æ–¥—Å—Ç–≤–æ={new_sim:.1%} {improvement}")

            if new_sim > best_sim:
                best_sim = new_sim
                best_response = brain_answer
                improvements.append(new_sim)

            if new_sim >= Config.CONFIDENCE_THRESHOLD:
                print("‚úÖ –ö–û–ù–¶–ï–ü–¶–ò–Ø –£–°–í–û–ï–ù–ê!\n")
                thought_process.learning_occurred = True
                break

            self.step_count += 1

        # ========== –≠–¢–ê–ü 6: –°–û–•–†–ê–ù–ï–ù–ò–ï –û–ë–£–ß–ï–ù–ò–Ø ==========
        self.learning_manager.record_learning(
            topic=input_type,
            concept=user_input,
            teacher_answer=teacher_answer,
            ai_answer=best_response,
            similarity=best_sim
        )

        print(f"\nüìö {self.learning_manager.get_learning_progress()}")

        return best_response


# ======================
# –ì–õ–ê–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê
# ======================
def main():
    print("\n" + "=" * 70)
    print("üß† –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –° –†–ï–§–õ–ï–ö–°–ò–ï–ô –ò –û–ë–£–ß–ï–ù–ò–ï–ú v4.0")
    print("=" * 70)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å
    vocab = AdvancedVocabManager()
    if vocab.load():
        print(f"‚úÖ –°–ª–æ–≤–∞—Ä—å –∑–∞–≥—Ä—É–∂–µ–Ω ({vocab.size} —Å–ª–æ–≤)")
    else:
        print("üî® –°–æ–∑–¥–∞—é –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å...")
        base_words = "–ø—Ä–∏–≤–µ—Ç —Å–ø–∞—Å–∏–±–æ –¥–∞ –Ω–µ—Ç —á—Ç–æ –∫–∞–∫ –ø–æ—á–µ–º—É –≥–¥–µ –∫–æ–≥–¥–∞ –∫—Ç–æ –∫–æ—Ç–æ—Ä—ã–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ–Ω–∏–º–∞—é —É–∑–Ω–∞–ª –Ω–æ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —Ä–µ—Ñ–ª–µ–∫—Å–∏—è —Å–æ–º–Ω–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—Ç –º–Ω–µ–Ω–∏–µ —Ñ–∞–∫—Ç –ø—Ä–æ—Ü–µ—Å—Å –∞–Ω–∞–ª–∏–∑".split()
        vocab.add_words(base_words)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Å–ª–æ–≤–∞—Ä—å —Å {vocab.size} —Å–ª–æ–≤–∞–º–∏")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    model = SuperIntelligentBrain(vocab_size=max(Config.VOCAB_SIZE, vocab.size), device=device)
    if model.load():
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    else:
        print("üî® –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å...")
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É—á–∏—Ç–µ–ª—è
    teacher = SupervisedTeacher()

    print(f"\nüí° –ö–û–ú–ê–ù–î–´:")
    print(f"   '–≤—ã—Ö–æ–¥' - –∑–∞–≤–µ—Ä—à–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É")
    print(f"   '–ø–∞–º—è—Ç—å' - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π")
    print(f"   '–ø—Ä–æ–≥—Ä–µ—Å—Å' - –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è")
    print(f"   '—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å' - —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å")
    print(f"   '–æ—á–∏—Å—Ç–∏—Ç—å' - –æ—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å")

    interaction_count = 0

    while True:
        try:
            user_input = input("\nüë§ –í—ã: ").strip()

            if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
                print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å...")
                model.save()
                vocab.save()
                teacher.learning_manager.save()
                print("‚ú® –î–æ –≤—Å—Ç—Ä–µ—á–∏! –°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ!")
                break

            if user_input.lower() in ['–ø–∞–º—è—Ç—å', 'memory']:
                print(f"\nüìö –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è:")
                history = teacher.learning_manager.learning_history
                if history:
                    for item in history[-5:]:
                        print(f"  üìç {item['concept'][:40]}...")
                        print(f"     –°—Ö–æ–¥—Å—Ç–≤–æ: {item['similarity']:.1%}")
                else:
                    print("  (–∏—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞)")
                continue

            if user_input.lower() in ['–ø—Ä–æ–≥—Ä–µ—Å—Å', 'progress', 'stats']:
                print(f"\nüìä –ü–†–û–ì–†–ï–°–° –û–ë–£–ß–ï–ù–ò–Ø:")
                print(f"  {teacher.learning_manager.get_learning_progress()}")
                known = teacher.learning_manager.get_known_topics()
                if known:
                    print(f"  –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–µ–º—ã: {', '.join(known)}")
                continue

            if user_input.lower() in ['—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å', 'save']:
                print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é...")
                model.save()
                vocab.save()
                teacher.learning_manager.save()
                print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ!")
                continue

            if user_input.lower() in ['–æ—á–∏—Å—Ç–∏—Ç—å', 'clear']:
                if input("–í—ã —É–≤–µ—Ä–µ–Ω—ã? (–¥–∞/–Ω–µ—Ç): ").lower() == '–¥–∞':
                    Config.LEARNING_PATH.unlink(missing_ok=True)
                    teacher.learning_manager = LearningManager()
                    print("‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞!")
                continue

            if not user_input:
                continue

            input_type = detect_input_type(user_input)
            final_answer = teacher.train_step(model, vocab, user_input, input_type)

            print(f"\nüí° –ú–û–ô –û–¢–í–ï–¢: {final_answer}\n")
            print("=" * 70)

            interaction_count += 1
            if interaction_count % 3 == 0:
                print(f"üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...")
                model.save()
                vocab.save()
                teacher.learning_manager.save()

        except KeyboardInterrupt:
            print("\n‚ú® –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ. –î–æ –≤—Å—Ç—Ä–µ—á–∏!")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            traceback.print_exc()


if __name__ == "__main__":
    main()