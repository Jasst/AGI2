# coding: utf-8
"""
AGI_Enhanced_v23.py ‚Äî ADVANCED COGNITIVE ARCHITECTURE
–£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å:
- –í–µ–∫—Ç–æ—Ä–Ω–æ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç—å—é
- –ú–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è
- –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç—å—é (—Ä–∞–±–æ—á–∞—è ‚Üí —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è ‚Üí —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è)
- –°–∏—Å—Ç–µ–º–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º
- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–æ–π –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
"""

import re
import json
import requests
import time
import os
import sys
import hashlib
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Set
from datetime import datetime, timezone
from collections import defaultdict, Counter
import math


# ================= –ó–ê–ì–†–£–ó–ö–ê –ö–õ–Æ–ß–ê =================
def load_api_key():
    key = os.getenv("OPENROUTER_API_KEY", "")
    if not key:
        env_path = Path(".env")
        if env_path.exists():
            with open(env_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        if "=" in line:
                            k, v = line.split("=", 1)
                            if k.strip() == "OPENROUTER_API_KEY":
                                key = v.strip().strip('"').strip("'")
    return key


# ================= –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =================
class Config:
    ROOT = Path("./cognitive_v23")
    ROOT.mkdir(exist_ok=True)

    # –§–∞–π–ª—ã –ø–∞–º—è—Ç–∏
    SEMANTIC_DB = ROOT / "semantic_memory.json"
    EPISODIC_DB = ROOT / "episodic_memory.json"
    CAUSAL_DB = ROOT / "causal_graph.json"
    WORKING_DB = ROOT / "working_memory.json"
    META_DB = ROOT / "meta_state.json"
    VECTORS_DB = ROOT / "concept_vectors.json"
    LOG = ROOT / "system.log"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–º—è—Ç–∏
    WORKING_MEMORY_SIZE = 15  # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å
    EPISODIC_MEMORY_SIZE = 200  # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è
    SEMANTIC_MEMORY_SIZE = 1000  # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    LEARNING_RATE = 0.15
    DECAY_RATE = 0.005
    MIN_CONFIDENCE = 0.1
    CONSOLIDATION_THRESHOLD = 3  # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–Ω–∏–º–∞–Ω–∏—è
    ATTENTION_TOP_K = 7
    CONTEXT_WINDOW = 5

    # API –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
    OPENROUTER_API_KEY = load_api_key()
    MODEL = "qwen/qwen-2.5-7b-instruct"
    TIMEOUT = 30
    MAX_TOKENS = 400


# ================= –£–¢–ò–õ–ò–¢–´ =================
def clean_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    text = text.lower()
    text = re.sub(r'[^\w\s\-–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def extract_keywords(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    stop_words = {
        '—á—Ç–æ', '–∫–∞–∫', '–ø–æ—á–µ–º—É', '–µ—Å–ª–∏', '—Ç–æ', '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏',
        '—è', '—Ç—ã', '–º—ã', '–≤—ã', '–æ–Ω–∏', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–º–æ–π', '—Ç–≤–æ–π',
        '–≤', '–Ω–∞', '–∏', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–∫', '–æ', '–∏–∑', '—É',
        '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–Ω–∏', '–∂–µ', '–±—ã', '–ª–∏', '—É–∂–µ', '–µ—â–µ',
        '–±—ã—Ç—å', '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏'
    }

    words = clean_text(text).split()
    keywords = [w for w in words if len(w) > 2 and w not in stop_words]
    return keywords


def text_hash(text: str) -> str:
    """–•–µ—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()[:12]


def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
    if not vec1 or not vec2:
        return 0.0

    keys = set(vec1.keys()) & set(vec2.keys())
    if not keys:
        return 0.0

    dot = sum(vec1[k] * vec2[k] for k in keys)
    mag1 = math.sqrt(sum(v ** 2 for v in vec1.values()))
    mag2 = math.sqrt(sum(v ** 2 for v in vec2.values()))

    if mag1 == 0 or mag2 == 0:
        return 0.0

    return dot / (mag1 * mag2)


def print_typing(text: str, delay=0.01):
    """–≠—Ñ—Ñ–µ–∫—Ç –ø–µ—á–∞—Ç–∞–Ω–∏—è"""
    for c in text:
        print(c, end="", flush=True)
        time.sleep(delay)
    print(flush=True)


# ================= –ö–û–ù–¶–ï–ü–¢ =================
@dataclass
class Concept:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ü–µ–ø—Ç —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º"""
    name: str
    confidence: float = 0.2
    frequency: int = 0
    last_accessed: float = field(default_factory=time.time)

    # –°–≤—è–∑–∏
    relations: Dict[str, float] = field(default_factory=dict)  # —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
    causes: Dict[str, float] = field(default_factory=dict)  # —á—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç
    effects: Dict[str, float] = field(default_factory=dict)  # —á—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç —ç—Ç–æ

    # –ö–æ–Ω—Ç–µ–∫—Å—Ç
    contexts: List[str] = field(default_factory=list)  # –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    emotional_valence: float = 0.0  # —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞ (-1 –¥–æ 1)

    # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (TF-IDF –ø–æ–¥–æ–±–Ω–æ–µ)
    vector: Dict[str, float] = field(default_factory=dict)

    def reinforce(self, amount: float = None):
        """–£—Å–∏–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–∞"""
        if amount is None:
            amount = Config.LEARNING_RATE
        self.confidence = min(1.0, self.confidence + amount)
        self.frequency += 1
        self.last_accessed = time.time()

    def decay(self):
        """–ó–∞—Ç—É—Ö–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–∞"""
        self.confidence *= (1 - Config.DECAY_RATE)
        if self.frequency > 0:
            self.frequency -= 1

    def add_relation(self, other: str, strength: float = 0.3):
        """–î–æ–±–∞–≤–∏—Ç—å —Å–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º –∫–æ–Ω—Ü–µ–ø—Ç–æ–º"""
        current = self.relations.get(other, 0.0)
        self.relations[other] = min(1.0, current + strength)

    def add_context(self, context: str):
        """–î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        if context not in self.contexts:
            self.contexts.append(context)
            if len(self.contexts) > 10:
                self.contexts.pop(0)

    def update_vector(self, keywords: List[str]):
        """–û–±–Ω–æ–≤–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"""
        for word in keywords:
            self.vector[word] = self.vector.get(word, 0.0) + 1.0

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        total = sum(self.vector.values())
        if total > 0:
            self.vector = {k: v / total for k, v in self.vector.items()}

    def to_dict(self) -> dict:
        return {
            'name': self.name,
            'confidence': self.confidence,
            'frequency': self.frequency,
            'last_accessed': self.last_accessed,
            'relations': self.relations,
            'causes': self.causes,
            'effects': self.effects,
            'contexts': self.contexts,
            'emotional_valence': self.emotional_valence,
            'vector': self.vector
        }

    @staticmethod
    def from_dict(data: dict) -> 'Concept':
        return Concept(
            name=data['name'],
            confidence=data.get('confidence', 0.2),
            frequency=data.get('frequency', 0),
            last_accessed=data.get('last_accessed', time.time()),
            relations=data.get('relations', {}),
            causes=data.get('causes', {}),
            effects=data.get('effects', {}),
            contexts=data.get('contexts', []),
            emotional_valence=data.get('emotional_valence', 0.0),
            vector=data.get('vector', {})
        )


# ================= –≠–ü–ò–ó–û–î =================
@dataclass
class Episode:
    """–≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å - –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ"""
    id: str
    timestamp: float
    input_text: str
    response: str
    concepts: List[str]
    importance: float = 0.5
    emotional_tone: float = 0.0

    def to_dict(self) -> dict:
        return {
            'id': self.id,
            'timestamp': self.timestamp,
            'input_text': self.input_text,
            'response': self.response,
            'concepts': self.concepts,
            'importance': self.importance,
            'emotional_tone': self.emotional_tone
        }

    @staticmethod
    def from_dict(data: dict) -> 'Episode':
        return Episode(
            id=data['id'],
            timestamp=data['timestamp'],
            input_text=data['input_text'],
            response=data['response'],
            concepts=data['concepts'],
            importance=data.get('importance', 0.5),
            emotional_tone=data.get('emotional_tone', 0.0)
        )


# ================= –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ =================
class SemanticMemory:
    """–î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""

    def __init__(self):
        self.concepts: Dict[str, Concept] = {}
        self.load()

    def get_or_create(self, name: str) -> Concept:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ü–µ–ø—Ç"""
        if name not in self.concepts:
            self.concepts[name] = Concept(name=name)
        return self.concepts[name]

    def learn_from_text(self, text: str, importance: float = 0.5):
        """–û–±—É—á–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        keywords = extract_keywords(text)

        # –°–æ–∑–¥–∞–Ω–∏–µ/—É—Å–∏–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        for word in keywords:
            concept = self.get_or_create(word)
            concept.reinforce(Config.LEARNING_RATE * importance)
            concept.update_vector(keywords)
            concept.add_context(text[:100])

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
        for i in range(len(keywords) - 1):
            c1 = self.get_or_create(keywords[i])
            c2 = self.get_or_create(keywords[i + 1])
            c1.add_relation(c2.name, 0.2)
            c2.add_relation(c1.name, 0.15)

    def find_similar(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        query_keywords = extract_keywords(query)
        query_vector = {}
        for word in query_keywords:
            query_vector[word] = query_vector.get(word, 0.0) + 1.0

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        total = sum(query_vector.values())
        if total > 0:
            query_vector = {k: v / total for k, v in query_vector.items()}

        similarities = []
        for name, concept in self.concepts.items():
            if concept.confidence < Config.MIN_CONFIDENCE:
                continue

            sim = cosine_similarity(query_vector, concept.vector)
            if sim > 0:
                # –£—á–∏—Ç—ã–≤–∞–µ–º confidence –∏ frequency
                score = sim * concept.confidence * (1 + math.log1p(concept.frequency))
                similarities.append((name, score))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

    def get_related_concepts(self, concept_name: str, depth: int = 2) -> Set[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã —Å –∑–∞–¥–∞–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω–æ–π"""
        if concept_name not in self.concepts:
            return set()

        result = {concept_name}
        current_level = {concept_name}

        for _ in range(depth):
            next_level = set()
            for name in current_level:
                if name in self.concepts:
                    concept = self.concepts[name]
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã —Å –≤—ã—Å–æ–∫–æ–π —Å–∏–ª–æ–π —Å–≤—è–∑–∏
                    for rel_name, strength in concept.relations.items():
                        if strength > 0.3:
                            next_level.add(rel_name)

            result.update(next_level)
            current_level = next_level

            if not current_level:
                break

        return result

    def decay_all(self):
        """–ó–∞—Ç—É—Ö–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""
        for concept in self.concepts.values():
            concept.decay()

        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª–∞–±—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        to_remove = [
            name for name, c in self.concepts.items()
            if c.confidence < Config.MIN_CONFIDENCE and c.frequency == 0
        ]
        for name in to_remove:
            del self.concepts[name]

    def get_statistics(self) -> dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏"""
        return {
            'total_concepts': len(self.concepts),
            'strong_concepts': sum(1 for c in self.concepts.values() if c.confidence > 0.5),
            'total_relations': sum(len(c.relations) for c in self.concepts.values()),
            'avg_confidence': sum(c.confidence for c in self.concepts.values()) / max(len(self.concepts), 1)
        }

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–º—è—Ç—å"""
        data = {name: concept.to_dict() for name, concept in self.concepts.items()}
        with open(Config.SEMANTIC_DB, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å"""
        if Config.SEMANTIC_DB.exists():
            try:
                with open(Config.SEMANTIC_DB, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.concepts = {
                        name: Concept.from_dict(cdata)
                        for name, cdata in data.items()
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏: {e}")


# ================= –≠–ü–ò–ó–û–î–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ =================
class EpisodicMemory:
    """–≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è"""

    def __init__(self):
        self.episodes: List[Episode] = []
        self.load()

    def add(self, input_text: str, response: str, concepts: List[str], importance: float = 0.5):
        """–î–æ–±–∞–≤–∏—Ç—å —ç–ø–∏–∑–æ–¥"""
        episode = Episode(
            id=text_hash(f"{input_text}{time.time()}"),
            timestamp=time.time(),
            input_text=input_text,
            response=response,
            concepts=concepts,
            importance=importance
        )

        self.episodes.append(episode)

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
        if len(self.episodes) > Config.EPISODIC_MEMORY_SIZE:
            # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–µ —Å—Ç–∞—Ä—ã–µ —ç–ø–∏–∑–æ–¥—ã
            self.episodes.sort(key=lambda e: e.importance * (1 / (time.time() - e.timestamp + 1)))
            self.episodes = self.episodes[-Config.EPISODIC_MEMORY_SIZE:]

    def recall_similar(self, query: str, top_k: int = 3) -> List[Episode]:
        """–í—Å–ø–æ–º–Ω–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ —ç–ø–∏–∑–æ–¥—ã"""
        query_keywords = set(extract_keywords(query))

        scored_episodes = []
        for episode in self.episodes:
            episode_keywords = set(extract_keywords(episode.input_text))

            # Jaccard similarity
            intersection = len(query_keywords & episode_keywords)
            union = len(query_keywords | episode_keywords)

            if union > 0:
                similarity = intersection / union
                # –£—á–∏—Ç—ã–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∏ —Å–≤–µ–∂–µ—Å—Ç—å
                recency = 1 / (1 + (time.time() - episode.timestamp) / 86400)  # –¥–Ω–∏
                score = similarity * episode.importance * (0.5 + 0.5 * recency)
                scored_episodes.append((episode, score))

        scored_episodes.sort(key=lambda x: x[1], reverse=True)
        return [ep for ep, _ in scored_episodes[:top_k]]

    def get_recent(self, n: int = 5) -> List[Episode]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ø–∏–∑–æ–¥—ã"""
        return sorted(self.episodes, key=lambda e: e.timestamp, reverse=True)[:n]

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–º—è—Ç—å"""
        data = [ep.to_dict() for ep in self.episodes]
        with open(Config.EPISODIC_DB, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å"""
        if Config.EPISODIC_DB.exists():
            try:
                with open(Config.EPISODIC_DB, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.episodes = [Episode.from_dict(ep) for ep in data]
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏: {e}")


# ================= –ü–†–ò–ß–ò–ù–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ =================
class CausalMemory:
    """–ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏"""

    def __init__(self):
        self.graph: Dict[str, Dict[str, float]] = {}
        self.load()

    def add_link(self, cause: str, effect: str, strength: float = 0.3):
        """–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏—á–∏–Ω–Ω—É—é —Å–≤—è–∑—å"""
        if cause not in self.graph:
            self.graph[cause] = {}

        current = self.graph[cause].get(effect, 0.0)
        self.graph[cause][effect] = min(1.0, current + strength)

    def learn_from_conditional(self, text: str):
        """–û–±—É—á–µ–Ω–∏–µ –∏–∑ —É—Å–ª–æ–≤–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"""
        text = clean_text(text)

        # –ï—Å–ª–∏-—Ç–æ –ø–∞—Ç—Ç–µ—Ä–Ω
        if '–µ—Å–ª–∏' in text and '—Ç–æ' in text:
            parts = text.split('—Ç–æ', 1)
            condition = parts[0].replace('–µ—Å–ª–∏', '').strip()
            consequence = parts[1].strip()

            cond_keywords = extract_keywords(condition)
            cons_keywords = extract_keywords(consequence)

            if cond_keywords and cons_keywords:
                # –°–≤—è–∑—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
                for c in cond_keywords[-2:]:  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —Å–ª–æ–≤–∞ –∏–∑ —É—Å–ª–æ–≤–∏—è
                    for e in cons_keywords[:2]:  # –ø–µ—Ä–≤—ã–µ 2 —Å–ª–æ–≤–∞ –∏–∑ —Å–ª–µ–¥—Å—Ç–≤–∏—è
                        self.add_link(c, e, 0.4)

        # –ü–æ—Ç–æ–º—É —á—Ç–æ –ø–∞—Ç—Ç–µ—Ä–Ω
        elif '–ø–æ—Ç–æ–º—É —á—Ç–æ' in text or '—Ç–∞–∫ –∫–∞–∫' in text:
            if '–ø–æ—Ç–æ–º—É —á—Ç–æ' in text:
                parts = text.split('–ø–æ—Ç–æ–º—É —á—Ç–æ', 1)
            else:
                parts = text.split('—Ç–∞–∫ –∫–∞–∫', 1)

            effect_part = parts[0].strip()
            cause_part = parts[1].strip()

            cause_keywords = extract_keywords(cause_part)
            effect_keywords = extract_keywords(effect_part)

            if cause_keywords and effect_keywords:
                for c in cause_keywords[-2:]:
                    for e in effect_keywords[:2]:
                        self.add_link(c, e, 0.4)

    def predict_chain(self, start: str, max_steps: int = 5) -> List[str]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø—Ä–∏—á–∏–Ω–Ω—É—é —Ü–µ–ø–æ—á–∫—É"""
        chain = [start]
        current = start

        for _ in range(max_steps):
            if current not in self.graph or not self.graph[current]:
                break

            # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–µ —Å–ª–µ–¥—Å—Ç–≤–∏–µ
            next_concept = max(self.graph[current].items(), key=lambda x: x[1])
            if next_concept[1] < 0.2:  # —Å–ª–∏—à–∫–æ–º —Å–ª–∞–±–∞—è —Å–≤—è–∑—å
                break

            if next_concept[0] in chain:  # —Ü–∏–∫–ª
                break

            chain.append(next_concept[0])
            current = next_concept[0]

        return chain

    def get_all_chains(self, min_length: int = 2, max_count: int = 5) -> List[List[str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∑–Ω–∞—á–∏–º—ã–µ —Ü–µ–ø–æ—á–∫–∏"""
        chains = []
        for start in self.graph:
            chain = self.predict_chain(start, max_steps=4)
            if len(chain) >= min_length:
                chains.append(chain)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ –∏ —Å–∏–ª–µ —Å–≤—è–∑–µ–π
        chains.sort(key=lambda c: len(c), reverse=True)
        return chains[:max_count]

    def decay_all(self):
        """–ó–∞—Ç—É—Ö–∞–Ω–∏–µ —Å–≤—è–∑–µ–π"""
        for cause in list(self.graph.keys()):
            for effect in list(self.graph[cause].keys()):
                self.graph[cause][effect] *= (1 - Config.DECAY_RATE)

                if self.graph[cause][effect] < Config.MIN_CONFIDENCE:
                    del self.graph[cause][effect]

            if not self.graph[cause]:
                del self.graph[cause]

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–º—è—Ç—å"""
        with open(Config.CAUSAL_DB, 'w', encoding='utf-8') as f:
            json.dump(self.graph, f, ensure_ascii=False, indent=2)

    def load(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å"""
        if Config.CAUSAL_DB.exists():
            try:
                with open(Config.CAUSAL_DB, 'r', encoding='utf-8') as f:
                    self.graph = json.load(f)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏—á–∏–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏: {e}")


# ================= –†–ê–ë–û–ß–ê–Ø –ü–ê–ú–Ø–¢–¨ =================
@dataclass
class WorkingMemoryItem:
    """–≠–ª–µ–º–µ–Ω—Ç —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏"""
    content: str
    timestamp: float
    importance: float
    concepts: List[str]


class WorkingMemory:
    """–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è —Ä–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å"""

    def __init__(self):
        self.items: List[WorkingMemoryItem] = []
        self.attention_focus: Optional[str] = None

    def add(self, content: str, importance: float = 0.5):
        """–î–æ–±–∞–≤–∏—Ç—å –≤ —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å"""
        concepts = extract_keywords(content)

        item = WorkingMemoryItem(
            content=content,
            timestamp=time.time(),
            importance=importance,
            concepts=concepts
        )

        self.items.append(item)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–æ–∫—É—Å –≤–Ω–∏–º–∞–Ω–∏—è
        if concepts:
            self.attention_focus = concepts[0]

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
        if len(self.items) > Config.WORKING_MEMORY_SIZE:
            # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–µ —Å—Ç–∞—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            self.items.sort(key=lambda x: x.importance * (1 / (time.time() - x.timestamp + 1)))
            self.items = self.items[-Config.WORKING_MEMORY_SIZE:]

    def get_recent_context(self, n: int = 5) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        recent = sorted(self.items, key=lambda x: x.timestamp, reverse=True)[:n]
        return [item.content for item in recent]

    def get_relevant(self, query: str, top_k: int = 3) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã"""
        query_concepts = set(extract_keywords(query))

        scored = []
        for item in self.items:
            item_concepts = set(item.concepts)
            overlap = len(query_concepts & item_concepts)

            if overlap > 0:
                score = overlap * item.importance
                scored.append((item.content, score))

        scored.sort(key=lambda x: x[1], reverse=True)
        return [content for content, _ in scored[:top_k]]


# ================= –ú–ï–•–ê–ù–ò–ó–ú –í–ù–ò–ú–ê–ù–ò–Ø =================
class AttentionMechanism:
    """–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""

    @staticmethod
    def compute_relevance(query: str, items: List[str]) -> List[Tuple[str, float]]:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫ –∑–∞–ø—Ä–æ—Å—É"""
        query_concepts = set(extract_keywords(query))

        scored = []
        for item in items:
            item_concepts = set(extract_keywords(item))

            # Jaccard similarity
            intersection = len(query_concepts & item_concepts)
            union = len(query_concepts | item_concepts)

            if union > 0:
                relevance = intersection / union
                scored.append((item, relevance))

        return sorted(scored, key=lambda x: x[1], reverse=True)

    @staticmethod
    def select_top_k(items: List[Tuple[str, float]], k: int) -> List[str]:
        """–í—ã–±—Ä–∞—Ç—å —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö"""
        return [item for item, _ in items[:k]]


# ================= –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê =================
class CognitiveSystemV23:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç—å—é"""

    def __init__(self):
        print("üß† Cognitive System v23 ‚Äî Advanced Memory Architecture\n")

        if not Config.OPENROUTER_API_KEY:
            print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ –Ω–∞–π–¥–µ–Ω OPENROUTER_API_KEY!")
            sys.exit(1)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º –ø–∞–º—è—Ç–∏
        self.semantic = SemanticMemory()
        self.episodic = EpisodicMemory()
        self.causal = CausalMemory()
        self.working = WorkingMemory()
        self.attention = AttentionMechanism()

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.meta = self.load_meta()

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.log_file = open(Config.LOG, 'a', encoding='utf-8')
        self.log("System initialized")

        print("‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        self._print_statistics()

    def log(self, message: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        timestamp = datetime.now(timezone.utc).isoformat()
        self.log_file.write(f"[{timestamp}] {message}\n")
        self.log_file.flush()

    def _print_statistics(self):
        """–í—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏"""
        stats = self.semantic.get_statistics()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏:")
        print(f"   –ö–æ–Ω—Ü–µ–ø—Ç—ã: {stats['total_concepts']} (—Å–∏–ª—å–Ω—ã—Ö: {stats['strong_concepts']})")
        print(f"   –°–≤—è–∑–∏: {stats['total_relations']}")
        print(f"   –≠–ø–∏–∑–æ–¥—ã: {len(self.episodic.episodes)}")
        print(f"   –ü—Ä–∏—á–∏–Ω–Ω—ã–µ —Å–≤—è–∑–∏: {len(self.causal.graph)}")
        print(f"   –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {self.meta['interactions']}")

    def build_context(self, query: str) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø–∞–º—è—Ç–∏"""
        context_parts = []

        # 1. –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å (—Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        recent = self.working.get_recent_context(3)
        if recent:
            context_parts.append("üí≠ –¢–ï–ö–£–©–ò–ô –ö–û–ù–¢–ï–ö–°–¢:")
            for i, item in enumerate(recent[::-1], 1):
                context_parts.append(f"  {i}. {item[:100]}")

        # 2. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã (–ø—Ä–æ—à–ª—ã–π –æ–ø—ã—Ç)
        similar_episodes = self.episodic.recall_similar(query, top_k=2)
        if similar_episodes:
            context_parts.append("\nüìö –†–ï–õ–ï–í–ê–ù–¢–ù–´–ô –û–ü–´–¢:")
            for i, ep in enumerate(similar_episodes, 1):
                context_parts.append(f"  {i}. –í–æ–ø—Ä–æ—Å: {ep.input_text[:80]}")
                context_parts.append(f"     –û—Ç–≤–µ—Ç: {ep.response[:80]}")

        # 3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
        similar_concepts = self.semantic.find_similar(query, top_k=5)
        if similar_concepts:
            context_parts.append("\nüîó –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¢–´:")
            for name, score in similar_concepts:
                concept = self.semantic.concepts[name]
                context_parts.append(
                    f"  ‚Ä¢ {name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {concept.confidence:.2f}, "
                    f"—á–∞—Å—Ç–æ—Ç–∞: {concept.frequency})"
                )

                # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
                if concept.relations:
                    top_relations = sorted(
                        concept.relations.items(),
                        key=lambda x: x[1],
                        reverse=True
                    )[:3]
                    rel_str = ", ".join([f"{r[0]}({r[1]:.2f})" for r in top_relations])
                    context_parts.append(f"    –°–≤—è–∑–∏: {rel_str}")

        # 4. –ü—Ä–∏—á–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏
        query_keywords = extract_keywords(query)
        if query_keywords:
            chains = []
            for keyword in query_keywords[:2]:
                chain = self.causal.predict_chain(keyword, max_steps=3)
                if len(chain) > 1:
                    chains.append(" ‚Üí ".join(chain))

            if chains:
                context_parts.append("\n‚ö° –ü–†–ò–ß–ò–ù–ù–´–ï –°–í–Ø–ó–ò:")
                for chain in chains[:3]:
                    context_parts.append(f"  ‚Ä¢ {chain}")

        if context_parts:
            return "\n".join(context_parts)

        return ""

    def process(self, user_input: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–∞"""
        self.meta['interactions'] += 1

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.log(f"INPUT: {user_input}")

        # 1. –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å
        self.working.add(user_input, importance=0.7)

        # 2. –û–±—É—á–∞–µ–º—Å—è –∏–∑ –≤—Ö–æ–¥–∞
        self.semantic.learn_from_text(user_input, importance=0.6)
        self.causal.learn_from_conditional(user_input)

        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
        if user_input.lower() in ['—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', 'stats', '–ø–∞–º—è—Ç—å']:
            return self._handle_stats_command()

        if user_input.lower().startswith('–≤—Å–ø–æ–º–Ω–∏'):
            return self._handle_recall_command(user_input)

        # 4. –°—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = self.build_context(user_input)

        # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω—é—é –º–æ–¥–µ–ª—å
        response = self._query_llm(user_input, context)

        # 6. –û–±—É—á–∞–µ–º—Å—è –∏–∑ –æ—Ç–≤–µ—Ç–∞
        self.semantic.learn_from_text(response, importance=0.5)

        # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–ø–∏–∑–æ–¥
        concepts = extract_keywords(user_input) + extract_keywords(response)
        self.episodic.add(user_input, response, list(set(concepts)), importance=0.6)

        # 8. –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è
        if self.meta['interactions'] % 10 == 0:
            self._consolidate_memory()

        # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_all()

        self.log(f"OUTPUT: {response[:100]}...")

        return response

    def _query_llm(self, query: str, context: str) -> str:
        """–ó–∞–ø—Ä–æ—Å –∫ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            system_prompt = (
                "–¢—ã ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é. "
                "–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Å–≤–æ–µ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞. "
                "–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ.\n\n"
            )

            if context:
                system_prompt += f"–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ü–ê–ú–Ø–¢–ò:\n{context}\n\n"
                system_prompt += (
                    "–í–ê–ñ–ù–û: –û–ø–∏—Ä–∞–π—Å—è –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ. "
                    "–ï—Å–ª–∏ –≤ –ø–∞–º—è—Ç–∏ –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –µ—ë. "
                    "–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ."
                )

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–º—è—Ç—å
            if context:
                print(f"\nüß† –ò—Å–ø–æ–ª—å–∑—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ ({len(context)} —Å–∏–º–≤–æ–ª–æ–≤)")

            # –ó–∞–ø—Ä–æ—Å –∫ API
            headers = {
                "Authorization": f"Bearer {Config.OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "http://localhost:8000",
                "X-Title": "CognitiveSystemV23"
            }

            payload = {
                "model": Config.MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                "temperature": 0.4,
                "max_tokens": Config.MAX_TOKENS
            }

            print("‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...", flush=True)

            response = requests.post(
                Config.OPENROUTER_URL,
                headers=headers,
                json=payload,
                timeout=Config.TIMEOUT
            )

            response.raise_for_status()
            content = response.json()["choices"][0]["message"]["content"].strip()

            return content

        except requests.exceptions.Timeout:
            return "‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞."
        except requests.exceptions.RequestException as e:
            error_msg = f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {str(e)[:100]}"
            self.log(f"API ERROR: {e}")
            return error_msg
        except Exception as e:
            error_msg = f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)[:100]}"
            self.log(f"ERROR: {e}")
            return error_msg

    def _handle_stats_command(self) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        stats = self.semantic.get_statistics()

        output = ["üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–û–ì–ù–ò–¢–ò–í–ù–û–ô –°–ò–°–¢–ï–ú–´\n"]
        output.append(f"–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {self.meta['interactions']}")
        output.append(f"\n–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨:")
        output.append(f"  ‚Ä¢ –í—Å–µ–≥–æ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤: {stats['total_concepts']}")
        output.append(f"  ‚Ä¢ –°–∏–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤: {stats['strong_concepts']}")
        output.append(f"  ‚Ä¢ –°–≤—è–∑–µ–π: {stats['total_relations']}")
        output.append(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {stats['avg_confidence']:.2f}")

        output.append(f"\n–≠–ü–ò–ó–û–î–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨:")
        output.append(f"  ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —ç–ø–∏–∑–æ–¥–æ–≤: {len(self.episodic.episodes)}")

        output.append(f"\n–ü–†–ò–ß–ò–ù–ù–ê–Ø –ü–ê–ú–Ø–¢–¨:")
        output.append(f"  ‚Ä¢ –ü—Ä–∏—á–∏–Ω–Ω—ã—Ö —É–∑–ª–æ–≤: {len(self.causal.graph)}")
        total_links = sum(len(effects) for effects in self.causal.graph.values())
        output.append(f"  ‚Ä¢ –ü—Ä–∏—á–∏–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π: {total_links}")

        # –¢–æ–ø –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        top_concepts = sorted(
            self.semantic.concepts.values(),
            key=lambda c: c.confidence * c.frequency,
            reverse=True
        )[:5]

        if top_concepts:
            output.append(f"\n–¢–û–ü-5 –ö–û–ù–¶–ï–ü–¢–û–í:")
            for i, concept in enumerate(top_concepts, 1):
                output.append(
                    f"  {i}. {concept.name} "
                    f"(conf: {concept.confidence:.2f}, freq: {concept.frequency})"
                )

        return "\n".join(output)

    def _handle_recall_command(self, command: str) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è"""
        query = command.replace('–≤—Å–ø–æ–º–Ω–∏', '').strip()

        if not query:
            recent = self.episodic.get_recent(5)
            if not recent:
                return "ü§î –ú–æ—è —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –ø—É—Å—Ç–∞."

            output = ["üìö –ü–û–°–õ–ï–î–ù–ò–ï –í–û–°–ü–û–ú–ò–ù–ê–ù–ò–Ø:\n"]
            for i, ep in enumerate(recent, 1):
                time_str = datetime.fromtimestamp(ep.timestamp).strftime('%Y-%m-%d %H:%M')
                output.append(f"{i}. [{time_str}]")
                output.append(f"   –í–æ–ø—Ä–æ—Å: {ep.input_text[:80]}")
                output.append(f"   –û—Ç–≤–µ—Ç: {ep.response[:80]}\n")

            return "\n".join(output)

        else:
            similar = self.episodic.recall_similar(query, top_k=3)
            if not similar:
                return f"ü§î –ù–µ –Ω–∞—à—ë–ª –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –æ: {query}"

            output = [f"üìö –í–û–°–ü–û–ú–ò–ù–ê–ù–ò–Ø –û '{query}':\n"]
            for i, ep in enumerate(similar, 1):
                time_str = datetime.fromtimestamp(ep.timestamp).strftime('%Y-%m-%d %H:%M')
                output.append(f"{i}. [{time_str}]")
                output.append(f"   –í–æ–ø—Ä–æ—Å: {ep.input_text}")
                output.append(f"   –û—Ç–≤–µ—Ç: {ep.response}\n")

            return "\n".join(output)

    def _consolidate_memory(self):
        """–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ (–ø–µ—Ä–µ–Ω–æ—Å –∏–∑ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –≤ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é)"""
        print("üîÑ –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–∞–º—è—Ç–∏...", end=" ", flush=True)

        # –ó–∞—Ç—É—Ö–∞–Ω–∏–µ —Å—Ç–∞—Ä–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        self.semantic.decay_all()
        self.causal.decay_all()

        # –£—Å–∏–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        concept_counter = Counter()
        for item in self.working.items:
            for concept in item.concepts:
                concept_counter[concept] += 1

        for concept_name, count in concept_counter.items():
            if count >= Config.CONSOLIDATION_THRESHOLD:
                concept = self.semantic.get_or_create(concept_name)
                concept.reinforce(0.2)

        print("‚úì")
        self.log("Memory consolidated")

    def save_all(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏"""
        self.semantic.save()
        self.episodic.save()
        self.causal.save()

        with open(Config.META_DB, 'w', encoding='utf-8') as f:
            json.dump(self.meta, f, ensure_ascii=False, indent=2)

    def load_meta(self) -> dict:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        if Config.META_DB.exists():
            try:
                with open(Config.META_DB, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

        return {
            'interactions': 0,
            'created_at': datetime.now(timezone.utc).isoformat()
        }

    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä"""
        if hasattr(self, 'log_file'):
            self.log_file.close()


# ================= –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê =================
def run_diagnosis() -> bool:
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
    print("=" * 70)
    print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´")
    print("=" * 70)

    if not Config.OPENROUTER_API_KEY:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω OPENROUTER_API_KEY")
        print("\nüí° –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env —Å–æ —Å—Ç—Ä–æ–∫–æ–π:")
        print("   OPENROUTER_API_KEY=your_key_here")
        return False

    print(f"‚úÖ API –∫–ª—é—á: {Config.OPENROUTER_API_KEY[:12]}...{Config.OPENROUTER_API_KEY[-4:]}")
    print(f"‚úÖ –ú–æ–¥–µ–ª—å: {Config.MODEL}")
    print(f"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø–∞–º—è—Ç–∏: {Config.ROOT}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    try:
        print("\nüì° –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API...", end=" ", flush=True)

        headers = {
            "Authorization": f"Bearer {Config.OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
            "HTTP-Referer": "http://localhost:8000",
            "X-Title": "DiagnosticTest"
        }

        payload = {
            "model": Config.MODEL,
            "messages": [{"role": "user", "content": "test"}],
            "max_tokens": 10
        }

        response = requests.post(
            Config.OPENROUTER_URL,
            headers=headers,
            json=payload,
            timeout=10
        )

        if response.status_code == 200:
            print("‚úÖ –£–°–ü–ï–®–ù–û")
            return True
        else:
            print(f"‚ùå –û–®–ò–ë–ö–ê {response.status_code}")
            print(f"–û—Ç–≤–µ—Ç: {response.text[:200]}")
            return False

    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê: {e}")
        return False


# ================= –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø =================
def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Å–æ–ª–∏ –¥–ª—è Windows
    if sys.platform == "win32":
        try:
            import ctypes
            kernel32 = ctypes.windll.kernel32
            kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)
        except:
            pass

    print("\n" + "=" * 70)
    print("üß† COGNITIVE SYSTEM v23")
    print("   Advanced Memory Architecture")
    print("=" * 70 + "\n")

    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
    if not run_diagnosis():
        print("\n‚ùå –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.")
        return

    print("\n" + "=" * 70)
    print("üöÄ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´")
    print("=" * 70 + "\n")

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    system = CognitiveSystemV23()

    print("\n" + "=" * 70)
    print("üí¨ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê –ö –î–ò–ê–õ–û–ì–£")
    print("=" * 70)
    print("\nüìã –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
    print("  ‚Ä¢ –î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å")
    print("  ‚Ä¢ –≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è)")
    print("  ‚Ä¢ –ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏")
    print("  ‚Ä¢ –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    print("  ‚Ä¢ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –ø–∞–º—è—Ç–∏")
    print("\nüéØ –ö–æ–º–∞–Ω–¥—ã:")
    print("  ‚Ä¢ '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞' –∏–ª–∏ '–ø–∞–º—è—Ç—å' ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏")
    print("  ‚Ä¢ '–≤—Å–ø–æ–º–Ω–∏' ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è")
    print("  ‚Ä¢ '–≤—Å–ø–æ–º–Ω–∏ <—Ç–µ–º–∞>' ‚Äî –≤—Å–ø–æ–º–Ω–∏—Ç—å –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–µ–º–µ")
    print("  ‚Ä¢ '–≤—ã—Ö–æ–¥' –∏–ª–∏ 'exit' ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–±–æ—Ç—É")
    print("=" * 70 + "\n")

    # –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª
    while True:
        try:
            user_input = input("üí≠ –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['exit', '–≤—ã—Ö–æ–¥', 'quit', 'q']:
                print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
                system.save_all()
                print("üíæ –ü–∞–º—è—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
                break

            print()
            response = system.process(user_input)

            print("\nü§ñ –û—Ç–≤–µ—Ç:")
            print_typing(response, delay=0.01)

            print("\n" + "-" * 70 + "\n")

        except KeyboardInterrupt:
            print("\n\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            system.save_all()
            print("üíæ –ü–∞–º—è—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            break

        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            print()


if __name__ == "__main__":
    main()