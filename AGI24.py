# coding: utf-8
"""
AGI_v29_Enhanced.py ‚Äî –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ê–í–¢–û–ù–û–ú–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –° –†–ê–°–®–ò–†–ï–ù–ù–´–ú–ò –ö–û–ì–ù–ò–¢–ò–í–ù–´–ú–ò –°–ü–û–°–û–ë–ù–û–°–¢–Ø–ú–ò

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
1. –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –º—ã—à–ª–µ–Ω–∏—è (–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ, —Ç–≤–æ—Ä—á–µ—Å–∫–æ–µ, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ)
2. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º–∞—è –ø–∞–º—è—Ç—å —Å –≤–µ—Å–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
3. –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
4. –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
5. Self-improvement —á–µ—Ä–µ–∑ –º–µ—Ç–∞–ø–æ–∑–Ω–∞–Ω–∏–µ
6. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –ø–∞–º—è—Ç–∏
7. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
"""

import re
import json
import asyncio
import aiohttp
import time
import os
import sys
import sqlite3
import hashlib
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple, Set
from datetime import datetime
from collections import defaultdict, deque
import logging
from contextlib import contextmanager
import random
import math


# ================= –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =================

class Config:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    ROOT = Path("./cognitive_system_v29")
    ROOT.mkdir(exist_ok=True)

    DB_PATH = ROOT / "memory.db"
    CACHE_PATH = ROOT / "cache.json"
    LOG_PATH = ROOT / "system.log"
    KNOWLEDGE_GRAPH_PATH = ROOT / "knowledge_graph.json"

    OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
    MODEL = "nvidia/nemotron-3-nano-30b-a3b:free"
    TIMEOUT = 300
    MAX_TOKENS = 8000

    # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    REFLECTION_INTERVAL = 3  # –ß–∞—â–µ —Ä–µ—Ñ–ª–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
    DEEP_THINKING_THRESHOLD = 0.7  # –ü–æ—Ä–æ–≥ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    CREATIVITY_FACTOR = 0.8  # –£—Ä–æ–≤–µ–Ω—å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
    LEARNING_RATE = 0.15  # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è

    # –ü–∞–º—è—Ç—å
    MAX_MEMORY_ITEMS = 10000
    CONTEXT_WINDOW_SIZE = 10  # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞
    MEMORY_DECAY_RATE = 0.05  # –°–∫–æ—Ä–æ—Å—Ç—å "–∑–∞–±—ã–≤–∞–Ω–∏—è"

    # –ú—ã—à–ª–µ–Ω–∏–µ
    THOUGHT_TYPES = [
        '—Ä–µ—Ñ–ª–µ–∫—Å–∏—è', '–∞–Ω–∞–ª–∏–∑', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '–æ–±—É—á–µ–Ω–∏–µ',
        '–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ', '—Å–∏–Ω—Ç–µ–∑', '–∫—Ä–∏—Ç–∏–∫–∞', '—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ',
        '–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ', '–æ—Ü–µ–Ω–∫–∞'
    ]

    @classmethod
    def get_api_key(cls):
        key = os.getenv("OPENROUTER_API_KEY")
        if key:
            return key

        env_path = Path(".env")
        if env_path.exists():
            try:
                with open(env_path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line.startswith("OPENROUTER_API_KEY="):
                            return line.split("=", 1)[1].strip('"\' ')
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è .env: {e}")

        raise ValueError("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env —Å OPENROUTER_API_KEY=–≤–∞—à_–∫–ª—é—á")


# ================= –£–õ–£–ß–®–ï–ù–ù–´–ï –£–¢–ò–õ–ò–¢–´ =================

def calculate_text_similarity(text1: str, text2: str) -> float:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å —É—á—ë—Ç–æ–º n-–≥—Ä–∞–º–º"""

    def get_ngrams(text: str, n: int = 2) -> Set[str]:
        words = text.lower().split()
        return set(' '.join(words[i:i + n]) for i in range(len(words) - n + 1))

    # –Æ–Ω–∏–≥—Ä–∞–º–º—ã
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())

    if not words1 or not words2:
        return 0.0

    # –ë–∏–≥—Ä–∞–º–º—ã
    bigrams1 = get_ngrams(text1, 2)
    bigrams2 = get_ngrams(text2, 2)

    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
    unigram_sim = len(words1.intersection(words2)) / max(len(words1), len(words2))
    bigram_sim = len(bigrams1.intersection(bigrams2)) / max(len(bigrams1), len(bigrams2), 1)

    return 0.6 * unigram_sim + 0.4 * bigram_sim


def extract_semantic_features(text: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ç–µ–∫—Å—Ç–∞"""
    features = {
        'length': len(text.split()),
        'complexity': len(set(text.lower().split())) / max(len(text.split()), 1),
        'question_words': len(re.findall(r'\b(–∫–∞–∫|—á—Ç–æ|–ø–æ—á–µ–º—É|–∑–∞—á–µ–º|–∫–æ–≥–¥–∞|–≥–¥–µ|–∫—Ç–æ)\b', text.lower())),
        'numbers': len(re.findall(r'\b\d+\b', text)),
        'emotions': len(re.findall(r'\b(—Ö–æ—Ä–æ—à–æ|–ø–ª–æ—Ö–æ|–æ—Ç–ª–∏—á–Ω–æ|—É–∂–∞—Å–Ω–æ|–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ|—Å–∫—É—á–Ω–æ)\b', text.lower())),
        'imperatives': len(re.findall(r'\b(—Å–¥–µ–ª–∞–π|—Å–æ–∑–¥–∞–π|–Ω–∞–π–¥–∏|–ø–æ–∫–∞–∂–∏|—Ä–∞—Å—Å–∫–∞–∂–∏)\b', text.lower())),
        'has_question': '?' in text,
        'sentiment': analyze_sentiment(text)
    }
    return features


def analyze_sentiment(text: str) -> float:
    """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (-1 –¥–æ 1)"""
    positive = ['—Ö–æ—Ä–æ—à–æ', '–æ—Ç–ª–∏—á–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–∫–ª–∞—Å—Å–Ω–æ', '—Å—É–ø–µ—Ä']
    negative = ['–ø–ª–æ—Ö–æ', '—É–∂–∞—Å–Ω–æ', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ', '–∫–æ—à–º–∞—Ä', '–ø—Ä–æ–≤–∞–ª']

    text_lower = text.lower()
    pos_count = sum(1 for word in positive if word in text_lower)
    neg_count = sum(1 for word in negative if word in text_lower)

    total = pos_count + neg_count
    if total == 0:
        return 0.0

    return (pos_count - neg_count) / total


# ================= –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –ë–ê–ó–ê –î–ê–ù–ù–´–• =================

class EnhancedMemoryDB:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å–≤—è–∑–µ–π"""

    def __init__(self, db_path: Path):
        self.db_path = db_path
        self._init_tables()

    @contextmanager
    def get_connection(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()

    def _init_tables(self):
        with self.get_connection() as conn:
            cursor = conn.cursor()

            # –¢–∞–±–ª–∏—Ü–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS interactions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    user_input TEXT NOT NULL,
                    system_response TEXT NOT NULL,
                    context TEXT,
                    emotion TEXT DEFAULT 'neutral',
                    category TEXT,
                    importance REAL DEFAULT 0.5,
                    complexity REAL DEFAULT 0.5,
                    satisfaction REAL DEFAULT 0.5,
                    tokens_used INTEGER DEFAULT 0
                )
            ''')

            # –¢–∞–±–ª–∏—Ü–∞ —Ñ–∞–∫—Ç–æ–≤ —Å –≤–µ—Å–∞–º–∏ –∏ —Å–≤—è–∑—è–º–∏
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS facts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    key TEXT NOT NULL,
                    value TEXT NOT NULL,
                    category TEXT,
                    confidence REAL DEFAULT 1.0,
                    importance REAL DEFAULT 0.5,
                    created_at REAL NOT NULL,
                    last_used REAL,
                    usage_count INTEGER DEFAULT 0,
                    decay_factor REAL DEFAULT 1.0,
                    source TEXT,
                    UNIQUE(key, value)
                )
            ''')

            # –¢–∞–±–ª–∏—Ü–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∞–º–∏
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS fact_relations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    fact_id_1 INTEGER NOT NULL,
                    fact_id_2 INTEGER NOT NULL,
                    relation_type TEXT NOT NULL,
                    strength REAL DEFAULT 0.5,
                    created_at REAL NOT NULL,
                    FOREIGN KEY (fact_id_1) REFERENCES facts(id),
                    FOREIGN KEY (fact_id_2) REFERENCES facts(id)
                )
            ''')

            # –¢–∞–±–ª–∏—Ü–∞ –º—ã—Å–ª–µ–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS thoughts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    thought_type TEXT NOT NULL,
                    content TEXT NOT NULL,
                    trigger TEXT,
                    importance REAL DEFAULT 0.5,
                    depth_level INTEGER DEFAULT 1,
                    confidence REAL DEFAULT 0.7,
                    outcome TEXT
                )
            ''')

            # –¢–∞–±–ª–∏—Ü–∞ —Ü–µ–ª–µ–π —Å –ø–æ–¥—Ü–µ–ª—è–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS goals (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    parent_goal_id INTEGER,
                    created_at REAL NOT NULL,
                    description TEXT NOT NULL,
                    priority REAL DEFAULT 0.5,
                    status TEXT DEFAULT 'active',
                    progress REAL DEFAULT 0.0,
                    deadline REAL,
                    next_action TEXT,
                    success_criteria TEXT,
                    learned_lessons TEXT,
                    FOREIGN KEY (parent_goal_id) REFERENCES goals(id)
                )
            ''')

            # –¢–∞–±–ª–∏—Ü–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (–æ–±—É—á–µ–Ω–∏–µ)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    pattern_type TEXT NOT NULL,
                    description TEXT NOT NULL,
                    occurrences INTEGER DEFAULT 1,
                    confidence REAL DEFAULT 0.5,
                    created_at REAL NOT NULL,
                    last_seen REAL NOT NULL,
                    success_rate REAL DEFAULT 0.5
                )
            ''')

            # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_interactions_time ON interactions(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_interactions_category ON interactions(category)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_facts_key ON facts(key)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_facts_importance ON facts(importance)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_thoughts_type ON thoughts(thought_type)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_thoughts_importance ON thoughts(importance)')

            conn.commit()

    # === –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è ===

    def add_interaction(self, user_input: str, system_response: str, **kwargs) -> int:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO interactions 
                (timestamp, user_input, system_response, context, emotion, category,
                 importance, complexity, satisfaction, tokens_used)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                time.time(),
                user_input,
                system_response,
                kwargs.get('context', ''),
                kwargs.get('emotion', 'neutral'),
                kwargs.get('category', ''),
                kwargs.get('importance', 0.5),
                kwargs.get('complexity', 0.5),
                kwargs.get('satisfaction', 0.5),
                kwargs.get('tokens_used', 0)
            ))
            conn.commit()
            return cursor.lastrowid

    def get_contextual_interactions(self, query: str, limit: int = 5) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"""
        with self.get_connection() as conn:
            cursor = conn.cursor()

            # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            cursor.execute('''
                SELECT * FROM interactions
                ORDER BY timestamp DESC
                LIMIT ?
            ''', (limit * 3,))

            all_interactions = [dict(row) for row in cursor.fetchall()]

            # –†–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            scored = []
            for interaction in all_interactions:
                relevance = calculate_text_similarity(
                    query,
                    interaction['user_input'] + ' ' + interaction['system_response']
                )
                recency = 1.0 - (time.time() - interaction['timestamp']) / (30 * 24 * 3600)  # 30 –¥–Ω–µ–π
                recency = max(0, min(1, recency))

                score = 0.6 * relevance + 0.3 * interaction['importance'] + 0.1 * recency
                scored.append((score, interaction))

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø
            scored.sort(reverse=True, key=lambda x: x[0])
            return [item[1] for item in scored[:limit]]

    # === –§–∞–∫—Ç—ã ===

    def add_fact(self, key: str, value: str, **kwargs):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        with self.get_connection() as conn:
            cursor = conn.cursor()

            cursor.execute('SELECT id FROM facts WHERE key = ? AND value = ?', (key, value))
            existing = cursor.fetchone()

            if existing:
                cursor.execute('''
                    UPDATE facts
                    SET confidence = ?, importance = ?, last_used = ?, 
                        usage_count = usage_count + 1, decay_factor = 1.0
                    WHERE id = ?
                ''', (
                    kwargs.get('confidence', 1.0),
                    kwargs.get('importance', 0.5),
                    time.time(),
                    existing[0]
                ))
            else:
                cursor.execute('''
                    INSERT INTO facts 
                    (key, value, category, confidence, importance, created_at, last_used, 
                     usage_count, decay_factor, source)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    key, value,
                    kwargs.get('category', ''),
                    kwargs.get('confidence', 1.0),
                    kwargs.get('importance', 0.5),
                    time.time(), time.time(), 1, 1.0,
                    kwargs.get('source', 'user')
                ))

            conn.commit()

    def get_relevant_facts(self, query: str, limit: int = 5) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º –∑–∞—Ç—É—Ö–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        with self.get_connection() as conn:
            cursor = conn.cursor()

            # –û–±–Ω–æ–≤–ª—è–µ–º decay_factor
            cursor.execute('''
                UPDATE facts
                SET decay_factor = decay_factor * (1 - ?)
                WHERE last_used < ?
            ''', (Config.MEMORY_DECAY_RATE, time.time() - 86400))  # 1 –¥–µ–Ω—å

            cursor.execute('''
                SELECT * FROM facts
                WHERE confidence > 0.3 AND decay_factor > 0.1
                ORDER BY importance DESC, usage_count DESC
                LIMIT ?
            ''', (limit * 2,))

            all_facts = [dict(row) for row in cursor.fetchall()]

            # –†–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            scored = []
            for fact in all_facts:
                relevance = calculate_text_similarity(query, f"{fact['key']} {fact['value']}")
                score = (
                        0.4 * relevance +
                        0.3 * fact['importance'] +
                        0.2 * fact['confidence'] +
                        0.1 * fact['decay_factor']
                )
                scored.append((score, fact))

            scored.sort(reverse=True, key=lambda x: x[0])
            return [item[1] for item in scored[:limit]]

    def add_fact_relation(self, fact_id_1: int, fact_id_2: int, relation_type: str, strength: float = 0.5):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∞–º–∏"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO fact_relations
                (fact_id_1, fact_id_2, relation_type, strength, created_at)
                VALUES (?, ?, ?, ?, ?)
            ''', (fact_id_1, fact_id_2, relation_type, strength, time.time()))
            conn.commit()

    # === –ú—ã—Å–ª–∏ ===

    def add_thought(self, thought_type: str, content: str, **kwargs):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º—ã—Å–ª–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO thoughts
                (timestamp, thought_type, content, trigger, importance, depth_level, confidence, outcome)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                time.time(),
                thought_type,
                content,
                kwargs.get('trigger', ''),
                kwargs.get('importance', 0.5),
                kwargs.get('depth_level', 1),
                kwargs.get('confidence', 0.7),
                kwargs.get('outcome', '')
            ))
            conn.commit()

    def get_thought_insights(self, limit: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ –º—ã—Å–ª–µ–π"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT thought_type, COUNT(*) as count, AVG(importance) as avg_importance
                FROM thoughts
                WHERE timestamp > ?
                GROUP BY thought_type
                ORDER BY count DESC
                LIMIT ?
            ''', (time.time() - 7 * 86400, limit))  # –ó–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é

            return [dict(row) for row in cursor.fetchall()]

    # === –ü–∞—Ç—Ç–µ—Ä–Ω—ã ===

    def add_pattern(self, pattern_type: str, description: str, confidence: float = 0.5):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        with self.get_connection() as conn:
            cursor = conn.cursor()

            cursor.execute('''
                SELECT id FROM patterns WHERE pattern_type = ? AND description = ?
            ''', (pattern_type, description))

            existing = cursor.fetchone()

            if existing:
                cursor.execute('''
                    UPDATE patterns
                    SET occurrences = occurrences + 1, last_seen = ?, confidence = ?
                    WHERE id = ?
                ''', (time.time(), min(1.0, confidence * 1.1), existing[0]))
            else:
                cursor.execute('''
                    INSERT INTO patterns
                    (pattern_type, description, occurrences, confidence, created_at, last_seen)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (pattern_type, description, 1, confidence, time.time(), time.time()))

            conn.commit()

    def get_patterns(self, min_confidence: float = 0.6, limit: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT * FROM patterns
                WHERE confidence >= ?
                ORDER BY occurrences DESC, confidence DESC
                LIMIT ?
            ''', (min_confidence, limit))

            return [dict(row) for row in cursor.fetchall()]

    # === –¶–µ–ª–∏ ===

    def add_goal(self, description: str, **kwargs) -> int:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO goals
                (parent_goal_id, created_at, description, priority, status, progress,
                 deadline, next_action, success_criteria)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                kwargs.get('parent_goal_id'),
                time.time(),
                description,
                kwargs.get('priority', 0.5),
                kwargs.get('status', 'active'),
                kwargs.get('progress', 0.0),
                kwargs.get('deadline'),
                kwargs.get('next_action', ''),
                kwargs.get('success_criteria', '')
            ))
            conn.commit()
            return cursor.lastrowid

    def get_goal_hierarchy(self, parent_id: Optional[int] = None) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ —Ü–µ–ª–µ–π"""
        with self.get_connection() as conn:
            cursor = conn.cursor()

            if parent_id is None:
                cursor.execute('''
                    SELECT * FROM goals
                    WHERE parent_goal_id IS NULL AND status = 'active'
                    ORDER BY priority DESC
                ''')
            else:
                cursor.execute('''
                    SELECT * FROM goals
                    WHERE parent_goal_id = ? AND status = 'active'
                    ORDER BY priority DESC
                ''', (parent_id,))

            return [dict(row) for row in cursor.fetchall()]


# ================= –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ú–´–®–õ–ï–ù–ò–Ø =================

class EnhancedThinkingSystem:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –º—ã—à–ª–µ–Ω–∏—è"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.rate_limit = 1.5
        self.last_request_time = 0

        # –ö—ç—à —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0

        # –ò—Å—Ç–æ—Ä–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
        self.reasoning_history = deque(maxlen=50)

        # –ú–µ—Ç–∞–∫–æ–≥–Ω–∏—Ü–∏—è
        self.thinking_performance = {
            'successful_predictions': 0,
            'failed_predictions': 0,
            'average_depth': 1.0,
            'creativity_score': 0.5
        }

    async def _wait_for_rate_limit(self):
        now = time.time()
        elapsed = now - self.last_request_time
        if elapsed < self.rate_limit:
            await asyncio.sleep(self.rate_limit - elapsed)
        self.last_request_time = time.time()

    async def multi_level_thinking(self, context: str, depth: int = 2) -> Dict[str, str]:
        """–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        thoughts = {}

        thinking_layers = [
            ('surface', '–ß—Ç–æ –æ—á–µ–≤–∏–¥–Ω–æ?', 0.3),
            ('analytical', '–ö–∞–∫–∏–µ —Å–≤—è–∑–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å?', 0.5),
            ('strategic', '–ö–∞–∫–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è?', 0.7),
            ('creative', '–ö–∞–∫–∏–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω—ã?', 0.9)
        ]

        for layer_name, prompt, temperature in thinking_layers[:depth]:
            thought = await self.generate_thought_with_prompt(
                f"{prompt}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}",
                temperature=temperature
            )
            if thought:
                thoughts[layer_name] = thought

        return thoughts

    async def generate_thought_with_prompt(self, prompt: str, temperature: float = 0.7) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º—ã—Å–ª–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º"""
        system_prompt = """–¢—ã ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É.
–¢–≤–æ–∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:
- –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–µ—è–≤–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–π

–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –Ω–æ —ë–º–∫–æ. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∏–Ω—Å–∞–π—Ç–∞—Ö, –∞ –Ω–µ –Ω–∞ –æ—á–µ–≤–∏–¥–Ω—ã—Ö –≤–µ—â–∞—Ö."""

        response = await self.call_llm(system_prompt, prompt, temperature)

        if response and len(response) > 15:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
            self.reasoning_history.append({
                'timestamp': time.time(),
                'prompt': prompt[:100],
                'response': response[:200],
                'temperature': temperature
            })
            return response

        return None

    async def predict_outcome(self, action: str, context: str) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ–π—Å—Ç–≤–∏—è"""
        prompt = f"""–ü—Ä–µ–¥—Å–∫–∞–∂–∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è:
–î–µ–π—Å—Ç–≤–∏–µ: {action}
–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–û—Ü–µ–Ω–∏:
1. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ (0-1)
2. –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∏—Å–∫–∏
3. –û–∂–∏–¥–∞–µ–º—ã–µ –≤—ã–≥–æ–¥—ã
4. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: JSON"""

        system_prompt = "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –∞–Ω–∞–ª–∏–∑—É —Ä–∏—Å–∫–æ–≤. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–º."

        response = await self.call_llm(system_prompt, prompt, temperature=0.5)

        # –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass

        return {'raw_prediction': response}

    async def synthesize_knowledge(self, facts: List[Dict], question: str) -> str:
        """–°–∏–Ω—Ç–µ–∑ –∑–Ω–∞–Ω–∏–π –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤"""
        facts_text = "\n".join([f"- {f['key']}: {f['value']}" for f in facts])

        prompt = f"""–°–∏–Ω—Ç–µ–∑–∏—Ä—É–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–∫—Ç—ã:

–í–æ–ø—Ä–æ—Å: {question}

–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–∫—Ç—ã:
{facts_text}

–°–æ–∑–¥–∞–π —Å–≤—è–∑–Ω—ã–π –æ—Ç–≤–µ—Ç, –∫–æ–º–±–∏–Ω–∏—Ä—É—è —ç—Ç–∏ —Ñ–∞–∫—Ç—ã. –ï—Å–ª–∏ —Ñ–∞–∫—Ç–æ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —É–∫–∞–∂–∏ —ç—Ç–æ."""

        system_prompt = "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–∏–Ω—Ç–µ–∑—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ù–∞—Ö–æ–¥–∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∞–º–∏."

        return await self.call_llm(system_prompt, prompt, temperature=0.6)

    async def call_llm(self, system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:
        """–í—ã–∑–æ–≤ LLM —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = hashlib.md5(
            f"{system_prompt[:100]}{user_prompt[:200]}{temperature}".encode()
        ).hexdigest()

        if cache_key in self.cache:
            self.cache_hits += 1
            return self.cache[cache_key]

        self.cache_misses += 1
        await self._wait_for_rate_limit()

        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }

            payload = {
                "model": Config.MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": temperature,
                "max_tokens": Config.MAX_TOKENS,
                "top_p": 0.95
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                        Config.OPENROUTER_URL,
                        headers=headers,
                        json=payload,
                        timeout=Config.TIMEOUT
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data["choices"][0]["message"]["content"].strip()

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                        self.cache[cache_key] = content

                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
                        if len(self.cache) > 200:
                            # –£–¥–∞–ª—è–µ–º 25% —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
                            keys_to_remove = list(self.cache.keys())[:50]
                            for key in keys_to_remove:
                                del self.cache[key]

                        return content
                    else:
                        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ API: {response.status}"

        except asyncio.TimeoutError:
            return "‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞"
        except Exception as e:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {str(e)[:100]}"

    def get_performance_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º—ã—à–ª–µ–Ω–∏—è"""
        cache_total = self.cache_hits + self.cache_misses
        cache_hit_rate = self.cache_hits / cache_total if cache_total > 0 else 0

        return {
            'cache_hit_rate': cache_hit_rate,
            'cache_size': len(self.cache),
            'reasoning_history_size': len(self.reasoning_history),
            'thinking_performance': self.thinking_performance
        }


# ================= –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–í–¢–û–ù–û–ú–ù–´–ô –ê–ì–ï–ù–¢ =================

class EnhancedAutonomousAgent:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏"""

    def __init__(self):
        print("üß† –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç v2.0\n")

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.api_key = Config.get_api_key()
        self.db = EnhancedMemoryDB(Config.DB_PATH)
        self.thinker = EnhancedThinkingSystem(self.api_key)

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        self.interaction_count = 0
        self.deep_thoughts_count = 0
        self.patterns_found = 0
        self.start_time = time.time()

        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
        self.context_window = deque(maxlen=Config.CONTEXT_WINDOW_SIZE)

        # –¢–µ–∫—É—â–∏–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏
        self.active_tasks = []

        # –ú–µ—Ç–∞–∫–æ–≥–Ω–∏—Ü–∏—è
        self.self_assessment = {
            'knowledge_gaps': [],
            'strong_areas': [],
            'improvement_areas': []
        }

        self._init_system()
        self.print_welcome()

    def _init_system(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        # –°–æ–∑–¥–∞—ë–º –±–∞–∑–æ–≤—ã–µ —Ü–µ–ª–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        existing_goals = self.db.get_goal_hierarchy()

        if not existing_goals:
            # –ì–ª–∞–≤–Ω—ã–µ —Ü–µ–ª–∏
            main_goal = self.db.add_goal(
                "–ë—ã—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–º –ø–æ–º–æ—â–Ω–∏–∫–æ–º",
                priority=1.0,
                success_criteria="–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
            )

            # –ü–æ–¥—Ü–µ–ª–∏
            self.db.add_goal(
                "–ì–ª—É–±–æ–∫–æ –ø–æ–Ω–∏–º–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã",
                parent_goal_id=main_goal,
                priority=0.9
            )

            self.db.add_goal(
                "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è",
                parent_goal_id=main_goal,
                priority=0.85
            )

            self.db.add_goal(
                "–ù–∞—Ö–æ–¥–∏—Ç—å –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è",
                parent_goal_id=main_goal,
                priority=0.8
            )

    def print_welcome(self):
        print("=" * 70)
        print("ü§ñ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ê–í–¢–û–ù–û–ú–ù–´–ô –ö–û–ì–ù–ò–¢–ò–í–ù–´–ô –ê–ì–ï–ù–¢")
        print("=" * 70)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        with self.db.get_connection() as conn:
            cursor = conn.cursor()

            cursor.execute("SELECT COUNT(*) FROM interactions")
            interactions = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM facts")
            facts = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM thoughts")
            thoughts = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM patterns")
            patterns = cursor.fetchone()[0]

        print(f"\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ:")
        print(f"  –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {interactions}")
        print(f"  –§–∞–∫—Ç–æ–≤: {facts}")
        print(f"  –ú—ã—Å–ª–µ–π: {thoughts}")
        print(f"  –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns}")

        print("\nüß† –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏:")
        print("  ‚úì –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∞–Ω–∞–ª–∏–∑")
        print("  ‚úì –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –ø–∞–º—è—Ç—å")
        print("  ‚úì –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
        print("  ‚úì –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ")
        print("  ‚úì –¢–≤–æ—Ä—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á")

        print("\nüí° –ö–æ–º–∞–Ω–¥—ã:")
        print("  ‚Ä¢ –¥—É–º–∞–π –≥–ª—É–±–æ–∫–æ - –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ")
        print("  ‚Ä¢ –∞–Ω–∞–ª–∏–∑ - –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
        print("  ‚Ä¢ –ø–∞—Ç—Ç–µ—Ä–Ω—ã - –ø–æ–∫–∞–∑–∞—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
        print("  ‚Ä¢ –∏–Ω—Å–∞–π—Ç—ã - –ø–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Å–∞–π—Ç—ã –∏–∑ –º—ã—Å–ª–µ–π")
        print("  ‚Ä¢ —Ü–µ–ª–∏ - –ø–æ–∫–∞–∑–∞—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—é —Ü–µ–ª–µ–π")
        print("  ‚Ä¢ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - –ø–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã")
        print("  ‚Ä¢ –≤—ã—Ö–æ–¥ - –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–±–æ—Ç—É")
        print("=" * 70 + "\n")

    async def process_input(self, user_input: str) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–≤–æ–¥–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        start_time = time.time()
        self.interaction_count += 1

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
        self.context_window.append({
            'type': 'user',
            'content': user_input,
            'timestamp': time.time()
        })

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥
        command_response = self._handle_command(user_input)
        if command_response:
            return command_response

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π –∑–∞–ø—Ä–æ—Å
        features = extract_semantic_features(user_input)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –≤–∞–∂–Ω–æ—Å—Ç—å
        complexity = features['complexity']
        importance = self._calculate_importance(user_input, features)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        self._extract_and_store_information(user_input, importance)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        response = await self._generate_contextual_response(
            user_input,
            features,
            complexity,
            importance
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        self.db.add_interaction(
            user_input=user_input,
            system_response=response,
            context=self._get_context_summary(),
            category=self._categorize_input(user_input, features),
            importance=importance,
            complexity=complexity,
            tokens_used=len(response.split())
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
        self.context_window.append({
            'type': 'assistant',
            'content': response,
            'timestamp': time.time()
        })

        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        await self._detect_patterns()

        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –≥–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
        if self.interaction_count % Config.REFLECTION_INTERVAL == 0:
            await self._deep_autonomous_thinking()

        # –ú–µ—Ç—Ä–∏–∫–∏
        duration = time.time() - start_time
        if duration > 2.0:
            print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {duration:.2f}—Å")

        return response

    def _calculate_importance(self, text: str, features: Dict) -> float:
        """–†–∞—Å—á—ë—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        importance = 0.5

        # –ù–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if any(word in text.lower() for word in ['–≤–∞–∂–Ω–æ', '—Å—Ä–æ—á–Ω–æ', '–∫—Ä–∏—Ç–∏—á–Ω–æ', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ']):
            importance += 0.3

        # –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        importance += min(0.2, features['question_words'] * 0.1)

        # –ò–º–ø–µ—Ä–∞—Ç–∏–≤—ã
        importance += min(0.2, features['imperatives'] * 0.1)

        # –°–ª–æ–∂–Ω–æ—Å—Ç—å
        importance += features['complexity'] * 0.2

        return min(1.0, importance)

    def _extract_and_store_information(self, text: str, importance: float):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        from collections import Counter

        # –ß–∏—Å–ª–∞
        numbers = re.findall(r'\b\d+\b', text)
        for num in numbers:
            self.db.add_fact('—á–∏—Å–ª–æ', num, category='–¥–∞–Ω–Ω—ã–µ', importance=importance * 0.5)

        # –ò–º–µ–Ω–∞ (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ)
        names = re.findall(r'\b[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)*\b', text)
        for name in names:
            if len(name) > 3:
                self.db.add_fact('–∏–º—è', name, category='–ø–µ—Ä—Å–æ–Ω–∞', importance=importance * 0.7)

        # –ö–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        patterns = [
            (r'(\w+)\s+(?:—ç—Ç–æ|—Ä–∞–≤–Ω–æ|—Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç)\s+([^.,]+)', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ'),
            (r'–∑–∞–ø–æ–º–Ω–∏[,:]?\s*(.+)', '–≤–∞–∂–Ω–∞—è_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'),
            (r'(\w+)\s*=\s*([^,]+)', '—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ')
        ]

        for pattern, category in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) == 2:
                    key, value = match
                    self.db.add_fact(
                        key.strip(),
                        value.strip(),
                        category=category,
                        importance=importance,
                        source='user_input'
                    )

    async def _generate_contextual_response(
            self,
            user_input: str,
            features: Dict,
            complexity: float,
            importance: float
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –≥–ª—É–±–æ–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        relevant_interactions = self.db.get_contextual_interactions(user_input, limit=3)
        relevant_facts = self.db.get_relevant_facts(user_input, limit=5)
        active_goals = self.db.get_goal_hierarchy()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = []

        # –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
        if relevant_interactions:
            context_parts.append("üìú –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è:")
            for interaction in relevant_interactions[:2]:
                context_parts.append(f"  –ü: {interaction['user_input'][:60]}...")
                context_parts.append(f"  –Ø: {interaction['system_response'][:60]}...")

        # –§–∞–∫—Ç—ã
        if relevant_facts:
            context_parts.append("\nüìö –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã:")
            for fact in relevant_facts[:4]:
                conf_stars = "‚òÖ" * int(fact['confidence'] * 5)
                context_parts.append(f"  ‚Ä¢ {fact['key']}: {fact['value']} [{conf_stars}]")

        # –¶–µ–ª–∏
        if active_goals:
            context_parts.append("\nüéØ –¢–µ–∫—É—â–∏–µ —Ü–µ–ª–∏:")
            for goal in active_goals[:2]:
                context_parts.append(f"  ‚Ä¢ {goal['description'][:50]}")

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns = self.db.get_patterns(min_confidence=0.7, limit=2)
        if patterns:
            context_parts.append("\nüîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:")
            for pattern in patterns:
                context_parts.append(f"  ‚Ä¢ {pattern['description'][:60]}")

        context = "\n".join(context_parts) if context_parts else "–ù–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–µ–Ω –ª–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
        needs_deep_thinking = (
                complexity > Config.DEEP_THINKING_THRESHOLD or
                importance > 0.7 or
                features['question_words'] > 2
        )

        if needs_deep_thinking:
            # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
            deep_thoughts = await self.thinker.multi_level_thinking(
                f"–ó–∞–ø—Ä–æ—Å: {user_input}\n{context}",
                depth=3
            )

            # –°–∏–Ω—Ç–µ–∑ –æ—Ç–≤–µ—Ç–∞
            synthesis_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–≤–µ—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å.

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_input}

–ê–Ω–∞–ª–∏–∑:
{chr(10).join([f'{level}: {thought}' for level, thought in deep_thoughts.items()])}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–î–∞–π —Ü–µ–ª—å–Ω—ã–π, –∏–Ω—Å–∞–π—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç."""

            system_prompt = """–¢—ã ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–°–∏–Ω—Ç–µ–∑–∏—Ä—É–π –æ—Ç–≤–µ—Ç, —É—á–∏—Ç—ã–≤–∞—è –≤—Å–µ —É—Ä–æ–≤–Ω–∏ –∞–Ω–∞–ª–∏–∑–∞. –ë—É–¥—å —Ç–æ—á–Ω—ã–º, –Ω–æ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º."""

            response = await self.thinker.call_llm(
                system_prompt,
                synthesis_prompt,
                temperature=0.7
            )

            self.deep_thoughts_count += 1
        else:
            # –û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
            system_prompt = f"""–¢—ã ‚Äî –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –ø–∞–º—è—Ç–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–ü—Ä–∏–Ω—Ü–∏–ø—ã:
- –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–∞–∫—Ç—ã –∏–∑ –ø–∞–º—è—Ç–∏
- –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º
- –£—á–∏—Ç—ã–≤–∞–π –∞–∫—Ç–∏–≤–Ω—ã–µ —Ü–µ–ª–∏
- –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã"""

            response = await self.thinker.call_llm(
                system_prompt,
                user_input,
                temperature=0.6
            )

        return response.strip()

    async def _detect_patterns(self):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –∏ –¥–∞–Ω–Ω—ã—Ö"""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        with self.db.get_connection() as conn:
            cursor = conn.cursor()

            # –ß–∞—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            cursor.execute('''
                SELECT category, COUNT(*) as count
                FROM interactions
                WHERE timestamp > ?
                GROUP BY category
                HAVING count > 2
            ''', (time.time() - 7 * 86400,))

            categories = cursor.fetchall()

            for category, count in categories:
                if category:
                    self.db.add_pattern(
                        'frequent_category',
                        f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —á–∞—Å—Ç–æ —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ {category}",
                        confidence=min(1.0, count / 10)
                    )

            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            cursor.execute('''
                SELECT 
                    strftime('%H', datetime(timestamp, 'unixepoch')) as hour,
                    COUNT(*) as count
                FROM interactions
                WHERE timestamp > ?
                GROUP BY hour
                HAVING count > 3
            ''', (time.time() - 7 * 86400,))

            time_patterns = cursor.fetchall()

            for hour, count in time_patterns:
                self.db.add_pattern(
                    'time_preference',
                    f"–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ {hour}:00",
                    confidence=0.6
                )

            self.patterns_found = len(categories) + len(time_patterns)

    async def _deep_autonomous_thinking(self):
        """–ì–ª—É–±–æ–∫–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ"""
        print("\nüí≠ [–ì–ª—É–±–æ–∫–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ...]", flush=True)

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏
        recent = self.db.get_contextual_interactions("–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", limit=5)
        patterns = self.db.get_patterns(min_confidence=0.6)
        insights = self.db.get_thought_insights()

        if not recent:
            print("  üí≠ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
            return

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π
        context_lines = [
            "–ü–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è:",
            *[f"- {i['user_input'][:50]}..." for i in recent[:3]],
            "\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:",
            *[f"- {p['description']}" for p in patterns[:3]],
            "\n–ò–Ω—Å–∞–π—Ç—ã –∏–∑ –º—ã—Å–ª–µ–π:",
            *[f"- {t['thought_type']}: {t['count']} —Ä–∞–∑" for t in insights[:3]]
        ]

        context = "\n".join(context_lines)

        # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
        thoughts = await self.thinker.multi_level_thinking(context, depth=3)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º—ã—Å–ª–∏
        for thought_type, content in thoughts.items():
            if content:
                self.db.add_thought(
                    thought_type=thought_type,
                    content=content,
                    trigger='autonomous_deep_thinking',
                    importance=0.8,
                    depth_level=3,
                    confidence=0.7
                )

                print(f"  üí° [{thought_type}] {content[:80]}...")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∞–º–æ–æ—Ü–µ–Ω–∫—É
        await self._update_self_assessment()

    async def _update_self_assessment(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∞–º–æ–æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        with self.db.get_connection() as conn:
            cursor = conn.cursor()

            # –°—Ä–µ–¥–Ω—è—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å
            cursor.execute('''
                SELECT AVG(satisfaction) FROM interactions
                WHERE timestamp > ?
            ''', (time.time() - 86400,))

            avg_satisfaction = cursor.fetchone()[0] or 0.5

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –Ω–∏–∑–∫–æ–π —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å—é
            cursor.execute('''
                SELECT category, AVG(satisfaction) as avg_sat
                FROM interactions
                WHERE timestamp > ? AND category IS NOT NULL
                GROUP BY category
                HAVING avg_sat < 0.5
            ''', (time.time() - 7 * 86400,))

            weak_categories = [row[0] for row in cursor.fetchall()]

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤—ã—Å–æ–∫–æ–π —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å—é
            cursor.execute('''
                SELECT category, AVG(satisfaction) as avg_sat
                FROM interactions
                WHERE timestamp > ? AND category IS NOT NULL
                GROUP BY category
                HAVING avg_sat > 0.7
            ''', (time.time() - 7 * 86400,))

            strong_categories = [row[0] for row in cursor.fetchall()]

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∞–º–æ–æ—Ü–µ–Ω–∫—É
        self.self_assessment = {
            'avg_satisfaction': avg_satisfaction,
            'improvement_areas': weak_categories,
            'strong_areas': strong_categories,
            'patterns_discovered': self.patterns_found,
            'deep_thoughts': self.deep_thoughts_count
        }

    def _handle_command(self, text: str) -> Optional[str]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º"""
        text_lower = text.lower().strip()

        if text_lower in ['–¥—É–º–∞–π –≥–ª—É–±–æ–∫–æ', '–≥–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ']:
            asyncio.create_task(self._deep_autonomous_thinking())
            return "üß† –ó–∞–ø—É—Å–∫–∞—é –≥–ª—É–±–æ–∫–æ–µ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ..."

        elif text_lower == '–∞–Ω–∞–ª–∏–∑':
            return self._get_comprehensive_analysis()

        elif text_lower == '–ø–∞—Ç—Ç–µ—Ä–Ω—ã':
            return self._format_patterns()

        elif text_lower == '–∏–Ω—Å–∞–π—Ç—ã':
            return self._format_insights()

        elif text_lower == '—Ü–µ–ª–∏':
            return self._format_goal_hierarchy()

        elif text_lower == '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞':
            return self._get_comprehensive_stats()

        elif text_lower in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
            return "SYSTEM_EXIT"

        return None

    def _get_comprehensive_analysis(self) -> str:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        lines = ["üîç –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –°–ò–°–¢–ï–ú–´", "=" * 60]

        # –°–∞–º–æ–æ—Ü–µ–Ω–∫–∞
        lines.append("\nüìä –°–∞–º–æ–æ—Ü–µ–Ω–∫–∞:")
        lines.append(f"  –°—Ä–µ–¥–Ω—è—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å: {self.self_assessment.get('avg_satisfaction', 0.5):.2f}")
        lines.append(f"  –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {self.patterns_found}")
        lines.append(f"  –ì–ª—É–±–æ–∫–∏—Ö –º—ã—Å–ª–µ–π: {self.deep_thoughts_count}")

        if self.self_assessment.get('strong_areas'):
            lines.append(f"\n‚úÖ –°–∏–ª—å–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏:")
            for area in self.self_assessment['strong_areas'][:3]:
                lines.append(f"  ‚Ä¢ {area}")

        if self.self_assessment.get('improvement_areas'):
            lines.append(f"\n‚ö†Ô∏è –û–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è:")
            for area in self.self_assessment['improvement_areas'][:3]:
                lines.append(f"  ‚Ä¢ {area}")

        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º—ã—à–ª–µ–Ω–∏—è
        perf = self.thinker.get_performance_metrics()
        lines.append(f"\nüß† –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º—ã—à–ª–µ–Ω–∏—è:")
        lines.append(f"  Cache hit rate: {perf['cache_hit_rate']:.1%}")
        lines.append(f"  –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {perf['cache_size']}")
        lines.append(f"  –ò—Å—Ç–æ—Ä–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: {perf['reasoning_history_size']}")

        return "\n".join(lines)

    def _format_patterns(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        patterns = self.db.get_patterns(min_confidence=0.5, limit=15)

        if not patterns:
            return "üîç –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."

        lines = ["üîç –û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´", "=" * 60]

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É
        by_type = defaultdict(list)
        for p in patterns:
            by_type[p['pattern_type']].append(p)

        for ptype, plist in by_type.items():
            lines.append(f"\nüìå {ptype.upper().replace('_', ' ')}:")
            for p in plist:
                conf_bar = "‚ñà" * int(p['confidence'] * 10)
                lines.append(f"  ‚Ä¢ {p['description']}")
                lines.append(
                    f"    –í—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å: {p['occurrences']} —Ä–∞–∑ | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: [{conf_bar}] {p['confidence']:.2f}")

        return "\n".join(lines)

    def _format_insights(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ –º—ã—Å–ª–µ–π"""
        insights = self.db.get_thought_insights(limit=10)

        if not insights:
            return "üí° –ò–Ω—Å–∞–π—Ç–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç."

        lines = ["üí° –ò–ù–°–ê–ô–¢–´ –ò–ó –ú–´–°–õ–ï–ô", "=" * 60]

        for insight in insights:
            lines.append(f"\nüß† {insight['thought_type'].upper()}:")
            lines.append(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {insight['count']}")
            lines.append(f"  –°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å: {insight['avg_importance']:.2f}")

        return "\n".join(lines)

    def _format_goal_hierarchy(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ —Ü–µ–ª–µ–π"""
        main_goals = self.db.get_goal_hierarchy(parent_id=None)

        if not main_goals:
            return "üéØ –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ü–µ–ª–µ–π."

        lines = ["üéØ –ò–ï–†–ê–†–•–ò–Ø –¶–ï–õ–ï–ô", "=" * 60]

        for goal in main_goals:
            progress_bar = "‚ñà" * int(goal['progress'] * 10) + "‚ñë" * (10 - int(goal['progress'] * 10))
            lines.append(f"\nüìç {goal['description']}")
            lines.append(f"  –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {goal['priority']:.2f} | –ü—Ä–æ–≥—Ä–µ—Å—Å: [{progress_bar}] {goal['progress']:.0%}")

            if goal['next_action']:
                lines.append(f"  –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: {goal['next_action']}")

            # –ü–æ–¥—Ü–µ–ª–∏
            subgoals = self.db.get_goal_hierarchy(parent_id=goal['id'])
            if subgoals:
                lines.append("  –ü–æ–¥—Ü–µ–ª–∏:")
                for sub in subgoals[:3]:
                    sub_bar = "‚ñà" * int(sub['progress'] * 5)
                    lines.append(f"    ‚Ä¢ {sub['description'][:50]} [{sub_bar}]")

        return "\n".join(lines)

    def _get_comprehensive_stats(self) -> str:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        lines = ["üìä –ü–û–õ–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´", "=" * 70]

        # –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã
        uptime = time.time() - self.start_time
        hours, remainder = divmod(uptime, 3600)
        minutes, seconds = divmod(remainder, 60)

        lines.append(f"\n‚è±Ô∏è –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {int(hours)}—á {int(minutes)}–º {int(seconds)}—Å")
        lines.append(f"–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {self.interaction_count}")
        lines.append(f"–ì–ª—É–±–æ–∫–∏—Ö –º—ã—Å–ª–µ–π: {self.deep_thoughts_count}")
        lines.append(f"–ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {self.patterns_found}")

        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
        with self.db.get_connection() as conn:
            cursor = conn.cursor()

            cursor.execute("SELECT COUNT(*) FROM interactions")
            interactions = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM facts WHERE decay_factor > 0.3")
            active_facts = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM thoughts")
            thoughts = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM patterns WHERE confidence > 0.5")
            patterns = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM goals WHERE status = 'active'")
            goals = cursor.fetchone()[0]

            cursor.execute("SELECT AVG(satisfaction) FROM interactions WHERE timestamp > ?",
                           (time.time() - 86400,))
            avg_satisfaction = cursor.fetchone()[0] or 0.5

        lines.append(f"\nüóÑÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö:")
        lines.append(f"  –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {interactions}")
        lines.append(f"  –ê–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤: {active_facts}")
        lines.append(f"  –ú—ã—Å–ª–µ–π: {thoughts}")
        lines.append(f"  –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns}")
        lines.append(f"  –ê–∫—Ç–∏–≤–Ω—ã—Ö —Ü–µ–ª–µ–π: {goals}")

        lines.append(f"\nüìà –ö–∞—á–µ—Å—Ç–≤–æ:")
        lines.append(f"  –°—Ä–µ–¥–Ω—è—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å (24—á): {avg_satisfaction:.2f}")

        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º—ã—à–ª–µ–Ω–∏—è
        perf = self.thinker.get_performance_metrics()
        lines.append(f"\nüß† –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
        lines.append(f"  Cache hit rate: {perf['cache_hit_rate']:.1%}")
        lines.append(f"  –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {perf['cache_size']}")
        lines.append(f"  –ò—Å—Ç–æ—Ä–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: {perf['reasoning_history_size']}")

        lines.append(f"\n‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
        lines.append(f"  –ú–æ–¥–µ–ª—å: {Config.MODEL}")
        lines.append(f"  –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞: {Config.CONTEXT_WINDOW_SIZE}")
        lines.append(f"  –ò–Ω—Ç–µ—Ä–≤–∞–ª —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏: {Config.REFLECTION_INTERVAL}")
        lines.append(f"  –ü–æ—Ä–æ–≥ –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è: {Config.DEEP_THINKING_THRESHOLD}")

        return "\n".join(lines)

    def _get_context_summary(self) -> str:
        """–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not self.context_window:
            return ""

        summary = []
        for item in list(self.context_window)[-4:]:
            prefix = "–ü:" if item['type'] == 'user' else "–Ø:"
            summary.append(f"{prefix} {item['content'][:50]}...")

        return "\n".join(summary)

    def _categorize_input(self, text: str, features: Dict) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Å ML-–ø–æ–¥—Ö–æ–¥–æ–º"""
        text_lower = text.lower()

        # –ü—Ä–∞–≤–∏–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        categories = {
            '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞': ['—Å–∫–æ–ª—å–∫–æ', '–ø–æ—Å—á–∏—Ç–∞–π', '–≤—ã—á–∏—Å–ª–∏', '+', '-', '*', '/', '='],
            '–ø–∞–º—è—Ç—å': ['–∑–∞–ø–æ–º–Ω–∏', '—Å–æ—Ö—Ä–∞–Ω–∏', '–Ω–∞–ø–æ–º–Ω–∏', '–∑–∞–ø–∏—Å—ã–≤–∞–π'],
            '–∞–Ω–∞–ª–∏–∑': ['–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π', '—Ä–∞–∑–±–µ—Ä–∏', '–æ—Ü–µ–Ω–∏', '—Å—Ä–∞–≤–Ω–∏'],
            '—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ': ['–ø—Ä–∏–¥—É–º–∞–π', '—Å–æ–∑–¥–∞–π', '—Å–æ—á–∏–Ω–∏', '–Ω–∞–ø–∏—à–∏'],
            '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ': ['–ø–ª–∞–Ω', '—Ä–∞—Å–ø–∏—à–∏', '–∫–∞–∫ –¥–æ—Å—Ç–∏—á—å', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è'],
            '–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ': ['–ø–æ—á–µ–º—É', '–∫–∞–∫', '–∑–∞—á–µ–º', '–æ–±—ä—è—Å–Ω–∏', '—Ä–∞—Å—Å–∫–∞–∂–∏'],
            '–ø–æ–∏—Å–∫': ['–Ω–∞–π–¥–∏', '–ø–æ–∫–∞–∂–∏', '–≥–¥–µ', '–∏—â–∏'],
            '–≤–æ–ø—Ä–æ—Å': ['?']
        }

        scores = {}
        for category, keywords in categories.items():
            score = sum(1 for kw in keywords if kw in text_lower)
            if score > 0:
                scores[category] = score

        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]

        return '–¥–∏–∞–ª–æ–≥'


# ================= –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø =================

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("\n" + "=" * 70)
    print("üöÄ –ó–ê–ü–£–°–ö –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –ö–û–ì–ù–ò–¢–ò–í–ù–û–ì–û –ê–ì–ï–ù–¢–ê")
    print("=" * 70)

    try:
        agent = EnhancedAutonomousAgent()

        while True:
            try:
                user_input = input("\nüí¨ –í—ã: ").strip()

                if not user_input:
                    continue

                if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
                    print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
                    print("\n" + agent._get_comprehensive_stats())
                    break

                print("\nü§ñ –°–∏—Å—Ç–µ–º–∞:")
                response = await agent.process_input(user_input)

                if response == "SYSTEM_EXIT":
                    print("üëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ...")
                    break

                # –í—ã–≤–æ–¥ —Å —ç—Ñ—Ñ–µ–∫—Ç–æ–º –ø–µ—á–∞—Ç–∏
                for char in response:
                    print(char, end='', flush=True)
                    await asyncio.sleep(0.002)

                print("\n" + "-" * 70)

            except KeyboardInterrupt:
                print("\n\nüõë –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break
            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
                import traceback
                traceback.print_exc()

    except Exception as e:
        print(f"\nüö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


def run():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    asyncio.run(main())


if __name__ == "__main__":
    run()