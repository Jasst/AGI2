# coding: utf-8
"""
AGI_Hybrid_MultiMind_v14.py
–ì–ò–ë–†–ò–î: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ + –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å + –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
–õ—É—á—à–µ–µ –∏–∑ –æ–±–æ–∏—Ö –º–∏—Ä–æ–≤!
"""

import os
import re
import json
import pickle
import traceback
import math
from collections import Counter, defaultdict
from datetime import datetime
from typing import List, Tuple, Dict
from pathlib import Path
from dataclasses import dataclass

import numpy as np
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from sentence_transformers import SentenceTransformer

    _HAS_ST_MODEL = True
except:
    _HAS_ST_MODEL = False


# ====================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ======================
class Config:
    SAVE_DIR = Path("./cognitive_hybrid_v14")
    KNOWLEDGE_PATH = SAVE_DIR / "knowledge.json"
    VOCAB_PATH = SAVE_DIR / "vocab.pkl"
    MODELS_DIR = SAVE_DIR / "models"

    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    NUM_MINDS = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
    VOCAB_SIZE = 15000
    EMB_DIM = 256
    HIDDEN_SIZE = 512
    NUM_LAYERS = 2
    DROPOUT = 0.1
    MAX_SEQ_LEN = 48
    LEARNING_RATE = 5e-4

    # –ú—ã—à–ª–µ–Ω–∏–µ
    THINKING_ITERATIONS = 2
    CONFIDENCE_THRESHOLD = 0.70
    SEMANTIC_SIMILARITY_THRESHOLD = 0.75

    QWEN_API = "http://localhost:1234/v1/chat/completions"


Config.SAVE_DIR.mkdir(exist_ok=True)
Config.MODELS_DIR.mkdir(exist_ok=True)


# ====================== –£–¢–ò–õ–ò–¢–´ ======================
def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    text = re.sub(r'#{1,3}\s*', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def clean_for_tokenize(text: str) -> str:
    text = re.sub(r'[^\w\s]', ' ', text, flags=re.UNICODE)
    return re.sub(r'\s+', ' ', text.lower()).strip()


# ====================== –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ ======================
@dataclass
class Memory:
    question: str
    answer: str
    embedding: np.ndarray
    source: str  # 'self' or 'qwen'
    confidence: float
    timestamp: str
    usage_count: int = 0


class SemanticMemory:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å —Å embeddings"""

    def __init__(self):
        self.encoder = None
        self.memories: List[Memory] = []

        if _HAS_ST_MODEL:
            try:
                print("üì¶ –ó–∞–≥—Ä—É–∂–∞—é encoder...")
                self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
                print("‚úÖ Encoder –≥–æ—Ç–æ–≤")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

        self.load()

    def add(self, question: str, answer: str, source: str = "qwen", confidence: float = 1.0):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –ø–∞–º—è—Ç—å"""

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        for mem in self.memories:
            if mem.question.lower().strip() == question.lower().strip():
                mem.answer = answer
                mem.confidence = max(mem.confidence, confidence)
                mem.timestamp = datetime.now().isoformat()
                self.save()
                return

        # –°–æ–∑–¥–∞—ë–º embedding
        embedding = self._encode(question)
        if embedding is None:
            return

        memory = Memory(
            question=question,
            answer=answer,
            embedding=embedding,
            source=source,
            confidence=confidence,
            timestamp=datetime.now().isoformat()
        )

        self.memories.append(memory)
        self.save()
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: '{question}' ‚Üí '{answer[:40]}...'")

    def search(self, query: str, top_k: int = 5) -> List[Tuple[Memory, float]]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è"""

        if not self.memories or self.encoder is None:
            return []

        query_emb = self._encode(query)
        if query_emb is None:
            return []

        # –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        results = []
        for mem in self.memories:
            similarity = float(np.dot(query_emb, mem.embedding))
            if similarity > 0.3:
                results.append((mem, similarity))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º
        results.sort(key=lambda x: x[1], reverse=True)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á—ë—Ç—á–∏–∫
        for mem, _ in results[:top_k]:
            mem.usage_count += 1

        return results[:top_k]

    def _encode(self, text: str):
        """–ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç"""
        if self.encoder is None:
            return None
        try:
            return self.encoder.encode(text, normalize_embeddings=True)
        except:
            return None

    def save(self):
        data = {
            'memories': [
                {
                    'question': m.question,
                    'answer': m.answer,
                    'source': m.source,
                    'confidence': m.confidence,
                    'timestamp': m.timestamp,
                    'usage_count': m.usage_count
                }
                for m in self.memories
            ]
        }
        with open(Config.KNOWLEDGE_PATH, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        if Config.KNOWLEDGE_PATH.exists():
            try:
                with open(Config.KNOWLEDGE_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for m in data.get('memories', []):
                        emb = self._encode(m['question'])
                        if emb is not None:
                            self.memories.append(Memory(
                                question=m['question'],
                                answer=m['answer'],
                                embedding=emb,
                                source=m['source'],
                                confidence=m['confidence'],
                                timestamp=m['timestamp'],
                                usage_count=m.get('usage_count', 0)
                            ))
                print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.memories)} –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")


# ====================== –°–õ–û–í–ê–†–¨ ======================
class Vocabulary:
    def __init__(self):
        self.word2idx = {'<pad>': 0, '<unk>': 1}
        self.idx2word = {0: '<pad>', 1: '<unk>'}
        self.next_id = 2
        self.load()

    def add_words(self, text: str):
        words = clean_for_tokenize(text).split()
        for word in words:
            if word and word not in self.word2idx:
                if self.next_id < Config.VOCAB_SIZE:
                    self.word2idx[word] = self.next_id
                    self.idx2word[self.next_id] = word
                    self.next_id += 1

    def encode(self, text: str) -> List[int]:
        words = clean_for_tokenize(text).split()[:Config.MAX_SEQ_LEN]
        ids = [self.word2idx.get(w, 1) for w in words]
        # –ü–∞–¥–¥–∏–Ω–≥
        while len(ids) < Config.MAX_SEQ_LEN:
            ids.append(0)
        return ids[:Config.MAX_SEQ_LEN]

    def decode(self, ids: List[int]) -> str:
        words = []
        for idx in ids:
            if idx == 0:  # pad
                break
            if idx in self.idx2word:
                word = self.idx2word[idx]
                if word != '<pad>' and word != '<unk>':
                    words.append(word)

        if not words:
            return ""

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ–¥—Ä—è–¥
        result = []
        prev = None
        for w in words[:30]:
            if w != prev:
                result.append(w)
                prev = w

        text = ' '.join(result)
        return text.capitalize() if text else ""

    @property
    def size(self):
        return len(self.word2idx)

    def save(self):
        with open(Config.VOCAB_PATH, 'wb') as f:
            pickle.dump({
                'word2idx': self.word2idx,
                'idx2word': self.idx2word,
                'next_id': self.next_id
            }, f)

    def load(self):
        if Config.VOCAB_PATH.exists():
            try:
                with open(Config.VOCAB_PATH, 'rb') as f:
                    data = pickle.load(f)
                    self.word2idx = data['word2idx']
                    self.idx2word = data['idx2word']
                    self.next_id = data['next_id']
            except:
                pass


# ====================== –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ ======================
class MindNetwork(nn.Module):
    """–û–¥–Ω–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å-—Ä–∞–∑—É–º"""

    def __init__(self, vocab_size: int):
        super().__init__()
        self.vocab_size = vocab_size

        self.embedding = nn.Embedding(vocab_size, Config.EMB_DIM, padding_idx=0)

        self.encoder = nn.GRU(
            Config.EMB_DIM,
            Config.HIDDEN_SIZE,
            num_layers=Config.NUM_LAYERS,
            dropout=Config.DROPOUT if Config.NUM_LAYERS > 1 else 0,
            batch_first=True,
            bidirectional=True
        )

        self.decoder = nn.GRU(
            Config.EMB_DIM,
            Config.HIDDEN_SIZE * 2,
            num_layers=Config.NUM_LAYERS,
            dropout=Config.DROPOUT if Config.NUM_LAYERS > 1 else 0,
            batch_first=True
        )

        self.output_layer = nn.Linear(Config.HIDDEN_SIZE * 2, vocab_size)

    def forward(self, input_ids, target_ids):
        # Encode
        input_emb = self.embedding(input_ids)
        _, hidden = self.encoder(input_emb)

        # Decode
        target_emb = self.embedding(target_ids)
        hidden = self._prepare_decoder_hidden(hidden)

        decoder_out, _ = self.decoder(target_emb, hidden)
        logits = self.output_layer(decoder_out)

        return logits

    def generate(self, input_ids, max_len: int = 30, temperature: float = 0.8):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        self.eval()
        with torch.no_grad():
            # Encode
            input_emb = self.embedding(input_ids)
            _, hidden = self.encoder(input_emb)
            hidden = self._prepare_decoder_hidden(hidden)

            # Decode
            generated = []
            current_input = torch.zeros((1, 1), dtype=torch.long, device=input_ids.device)

            for _ in range(max_len):
                emb = self.embedding(current_input)
                out, hidden = self.decoder(emb, hidden)
                logits = self.output_layer(out[:, -1, :])

                # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
                logits = logits / temperature
                logits[0, 0] = -float('inf')  # mask padding

                probs = F.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs[0], 1).item()

                if next_token == 0:  # stop at padding
                    break

                generated.append(next_token)
                current_input = torch.tensor([[next_token]], device=input_ids.device)

        return generated

    def _prepare_decoder_hidden(self, encoder_hidden):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç hidden state –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞"""
        # encoder_hidden: [num_layers*2, batch, hidden_size]
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º forward –∏ backward
        batch_size = encoder_hidden.size(1)
        hidden = encoder_hidden.view(Config.NUM_LAYERS, 2, batch_size, Config.HIDDEN_SIZE)
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º forward –∏ backward
        hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)
        return hidden.contiguous()


# ====================== –ö–û–õ–õ–ï–ö–¢–ò–í–ù–´–ô –†–ê–ó–£–ú ======================
class CollectiveIntelligence:
    """–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥—É–º–∞—é—Ç –≤–º–µ—Å—Ç–µ"""

    def __init__(self, vocab: Vocabulary, device):
        self.vocab = vocab
        self.device = device
        self.minds: List[MindNetwork] = []

        for i in range(Config.NUM_MINDS):
            mind = MindNetwork(vocab.size).to(device)
            self.minds.append(mind)

        print(f"üß† –°–æ–∑–¥–∞–Ω–æ {Config.NUM_MINDS} –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö —Ä–∞–∑—É–º–æ–≤")
        self.load_all()

    def think(self, question: str, context: str = "") -> Tuple[List[str], float]:
        """–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ"""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–º–ø—Ç
        if context:
            full_input = f"{context}\n{question}"
        else:
            full_input = question

        input_ids = torch.tensor([self.vocab.encode(full_input)], device=self.device)

        # –ö–∞–∂–¥—ã–π —Ä–∞–∑—É–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç
        answers = []
        print(f"\nüí≠ –î—É–º–∞—é—Ç {Config.NUM_MINDS} —Ä–∞–∑—É–º–∞:")

        for i, mind in enumerate(self.minds):
            try:
                temp = 0.75 + i * 0.1
                generated_ids = mind.generate(input_ids, max_len=30, temperature=temp)
                answer = self.vocab.decode(generated_ids)

                if answer and len(answer.split()) >= 2:
                    answers.append(answer)
                    print(f"  –†–∞–∑—É–º #{i + 1}: {answer}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –†–∞–∑—É–º #{i + 1}: –æ—à–∏–±–∫–∞")

        if not answers:
            return [], 0.0

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–Ω—Å–µ–Ω—Å—É—Å
        best_answer, confidence = self._find_consensus(answers)

        return answers, confidence

    def _find_consensus(self, answers: List[str]) -> Tuple[str, float]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω—Å–µ–Ω—Å—É—Å"""
        if not answers:
            return "", 0.0

        if len(answers) == 1:
            return answers[0], 0.5

        # –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–∏
        similarities = []
        for i in range(len(answers)):
            for j in range(i + 1, len(answers)):
                sim = self._jaccard_similarity(answers[i], answers[j])
                similarities.append(sim)

        avg_similarity = np.mean(similarities) if similarities else 0.0

        # –ë–µ—Ä—ë–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        best_answer = max(answers, key=lambda x: len(x.split()))

        return best_answer, avg_similarity

    def _jaccard_similarity(self, text1: str, text2: str) -> float:
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        if not words1 or not words2:
            return 0.0
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        return intersection / union if union > 0 else 0.0

    def train_all(self, question: str, answer: str, epochs: int = 10):
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ —Ä–∞–∑—É–º—ã"""

        print("\nüîÑ –û–±—É—á–∞—é –≤—Å–µ —Ä–∞–∑—É–º—ã...")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞
        self.vocab.add_words(question)
        self.vocab.add_words(answer)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä embeddings –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        self._update_embeddings()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        input_ids = torch.tensor([self.vocab.encode(question)], device=self.device)
        target_ids = torch.tensor([self.vocab.encode(answer)], device=self.device)

        # –û–±—É—á–µ–Ω–∏–µ
        for mind in self.minds:
            mind.train()
            optimizer = torch.optim.Adam(mind.parameters(), lr=Config.LEARNING_RATE)

            for epoch in range(epochs):
                optimizer.zero_grad()
                logits = mind(input_ids, target_ids)
                loss = F.cross_entropy(
                    logits.view(-1, self.vocab.size),
                    target_ids.view(-1),
                    ignore_index=0
                )
                loss.backward()
                torch.nn.utils.clip_grad_norm_(mind.parameters(), 1.0)
                optimizer.step()

                if epoch % 3 == 0:
                    print(f"  –≠–ø–æ—Ö–∞ {epoch}: loss={loss.item():.3f}")

        self.save_all()
        self.vocab.save()
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    def _update_embeddings(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä embeddings"""
        for mind in self.minds:
            if mind.vocab_size < self.vocab.size:
                old_emb = mind.embedding.weight.data
                mind.embedding = nn.Embedding(self.vocab.size, Config.EMB_DIM, padding_idx=0).to(self.device)
                mind.embedding.weight.data[:old_emb.size(0)] = old_emb

                mind.output_layer = nn.Linear(Config.HIDDEN_SIZE * 2, self.vocab.size).to(self.device)
                mind.vocab_size = self.vocab.size

    def save_all(self):
        for i, mind in enumerate(self.minds):
            path = Config.MODELS_DIR / f"mind_{i}.pt"
            torch.save(mind.state_dict(), path)

    def load_all(self):
        loaded = 0
        for i, mind in enumerate(self.minds):
            path = Config.MODELS_DIR / f"mind_{i}.pt"
            if path.exists():
                try:
                    mind.load_state_dict(torch.load(path, map_location=self.device))
                    loaded += 1
                except:
                    pass
        if loaded > 0:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded} —Ä–∞–∑—É–º–æ–≤")


# ====================== –ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê ======================
class HybridCognitiveSystem:
    """–ì–∏–±—Ä–∏–¥: –°–µ–º–∞–Ω—Ç–∏–∫–∞ + –ù–µ–π—Ä–æ—Å–µ—Ç–∏"""

    def __init__(self):
        print(f"\n{'=' * 70}")
        print(f"üß† –ì–ò–ë–†–ò–î–ù–ê–Ø –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê v14.0")
        print(f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å ‚Ä¢ –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ‚Ä¢ –°–∏–Ω—Ç–µ–∑")
        print(f"{'=' * 70}\n")

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.memory = SemanticMemory()
        self.vocab = Vocabulary()

        # –ë–∞–∑–æ–≤—ã–µ —Å–ª–æ–≤–∞
        base_text = "–ø—Ä–∏–≤–µ—Ç —Å–ø–∞—Å–∏–±–æ –¥–∞ –Ω–µ—Ç —á—Ç–æ –∫–∞–∫ –ø–æ—á–µ–º—É –≥–¥–µ –∫–æ–≥–¥–∞ –∫—Ç–æ —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—é –∑–Ω–∞—é –¥—É–º–∞—é"
        self.vocab.add_words(base_text)

        self.collective = CollectiveIntelligence(self.vocab, self.device)

        if not _HAS_ST_MODEL:
            print("\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ sentence-transformers –¥–ª—è –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏")
            print("   pip install sentence-transformers\n")

    def process(self, question: str):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å"""

        print(f"\n{'=' * 70}")
        print(f"üë§ –í–û–ü–†–û–°: {question}")
        print(f"{'=' * 70}")

        # –®–ê–ì 1: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        print(f"\nüîç –®–ê–ì 1: –ü–æ–∏—Å–∫ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏")
        similar_memories = self.memory.search(question, top_k=3)

        if similar_memories:
            best_mem, best_sim = similar_memories[0]
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(similar_memories)} –ø–æ—Ö–æ–∂–∏—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π")
            print(f"   –õ—É—á—à–µ–µ: '{best_mem.question}' (—Å—Ö–æ–∂–µ—Å—Ç—å: {best_sim:.1%})")

            # –ï—Å–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ - —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
            if best_sim >= Config.SEMANTIC_SIMILARITY_THRESHOLD:
                print(f"\n‚úÖ –¢–û–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï (sim={best_sim:.1%})")
                print(f"üí° –û–¢–í–ï–¢: {best_mem.answer}")
                best_mem.usage_count += 1
                return best_mem.answer

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = "\n".join([f"{m.question}: {m.answer}" for m, _ in similar_memories])
        else:
            print("‚ùå –ü–æ—Ö–æ–∂–∏—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            context = ""

        # –®–ê–ì 2: –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
        print(f"\nüß† –®–ê–ì 2: –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π")

        best_answer = None
        best_confidence = 0.0

        for iteration in range(Config.THINKING_ITERATIONS):
            print(f"\n   –ò—Ç–µ—Ä–∞—Ü–∏—è #{iteration + 1}:")
            answers, confidence = self.collective.think(question, context)

            if answers:
                print(f"   üìä –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {confidence:.1%}")
                if confidence > best_confidence:
                    best_answer = max(answers, key=len)
                    best_confidence = confidence

                if confidence >= 0.6:
                    break

        # –®–ê–ì 3: –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è
        print(f"\nüéØ –®–ê–ì 3: –°–∏–Ω—Ç–µ–∑ –∏ —Ä–µ—à–µ–Ω–∏–µ")

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        semantic_confidence = best_sim if similar_memories else 0.0
        neural_confidence = best_confidence
        combined_confidence = (semantic_confidence * 0.6 + neural_confidence * 0.4)

        print(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {semantic_confidence:.1%}")
        print(f"   –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {neural_confidence:.1%}")
        print(f"   –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {combined_confidence:.1%}")

        if combined_confidence >= Config.CONFIDENCE_THRESHOLD:
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç
            if semantic_confidence > neural_confidence and similar_memories:
                final_answer = similar_memories[0][0].answer
            else:
                final_answer = best_answer if best_answer else "–ù–µ –∑–Ω–∞—é"

            print(f"\n‚úÖ –û–¢–í–ï–ß–ê–Æ –°–ê–ú–û–°–¢–û–Ø–¢–ï–õ–¨–ù–û")
            print(f"üí° –ú–û–ô –û–¢–í–ï–¢: {final_answer}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ
            self.memory.add(question, final_answer, source='self', confidence=combined_confidence)

            return final_answer
        else:
            print(f"\n‚ùì –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û –£–í–ï–†–ï–ù")
            print(f"   –ú–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: {best_answer if best_answer else '–ù–µ –∑–Ω–∞—é'}")
            print(f"\nüë®‚Äçüè´ –£—á—É—Å—å —É Qwen...")

            return self._learn_from_qwen(question)

    def _learn_from_qwen(self, question: str) -> str:
        """–£—á–∏—Ç—Å—è —É Qwen"""

        try:
            resp = requests.post(Config.QWEN_API, json={
                "messages": [{"role": "user", "content": f"{question}\n\n–î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)."}],
                "max_tokens": 100,
                "temperature": 0.7
            }, timeout=20)

            if resp.status_code == 200:
                answer = clean_text(resp.json()['choices'][0]['message']['content'])
                print(f"üë®‚Äçüè´ QWEN: {answer}")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
                self.memory.add(question, answer, source='qwen', confidence=1.0)

                # –û–±—É—á–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
                self.collective.train_all(question, answer, epochs=8)

                print(f"\nüí° –ò–¢–û–ì–û–í–´–ô –û–¢–í–ï–¢: {answer}")
                return answer
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

        return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç"

    def show_statistics(self):
        total = len(self.memory.memories)
        self_count = sum(1 for m in self.memory.memories if m.source == 'self')
        qwen_count = sum(1 for m in self.memory.memories if m.source == 'qwen')

        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  –í–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π: {total}")
        print(f"  ü§ñ –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã—Ö: {self_count} ({self_count / total * 100 if total else 0:.1f}%)")
        print(f"  üë®‚Äçüè´ –û—Ç Qwen: {qwen_count} ({qwen_count / total * 100 if total else 0:.1f}%)")
        print(f"  üìö –°–ª–æ–≤–∞—Ä—å: {self.vocab.size} —Å–ª–æ–≤")
        print(f"  üß† –ù–µ–π—Ä–æ—Å–µ—Ç–µ–π: {Config.NUM_MINDS}")

        if self.memory.memories:
            most_used = max(self.memory.memories, key=lambda m: m.usage_count)
            print(f"  üî• –ü–æ–ø—É–ª—è—Ä–Ω–æ–µ: '{most_used.question[:30]}...' ({most_used.usage_count}x)")

    def show_memory(self, count: int = 10):
        recent = self.memory.memories[-count:]
        if not recent:
            print("\nüìö –ü–∞–º—è—Ç—å –ø—É—Å—Ç–∞")
            return

        print(f"\nüìö –ü–û–°–õ–ï–î–ù–ò–ï {len(recent)} –í–û–°–ü–û–ú–ò–ù–ê–ù–ò–ô:")
        for i, mem in enumerate(recent, 1):
            icon = "ü§ñ" if mem.source == 'self' else "üë®‚Äçüè´"
            print(f"\n{i}. {icon} {mem.question}")
            print(f"   ‚Üí {mem.answer}")
            print(f"   üìä {mem.confidence:.0%} | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {mem.usage_count}x")


# ====================== –ò–ù–¢–ï–†–§–ï–ô–° ======================
def main():
    try:
        system = HybridCognitiveSystem()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        traceback.print_exc()
        return

    print(f"\nüí° –ö–û–ú–ê–ù–î–´: '–≤—ã—Ö–æ–¥', '–ø–∞–º—è—Ç—å', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞'")

    while True:
        try:
            user_input = input("\nüë§ –í–´: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
                print("\n‚ú® –î–æ –≤—Å—Ç—Ä–µ—á–∏!")
                break

            if user_input.lower() in ['–ø–∞–º—è—Ç—å', '–∑–Ω–∞–Ω–∏—è']:
                system.show_memory(10)
                continue

            if user_input.lower() == '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞':
                system.show_statistics()
                continue

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞
            system.process(user_input)

        except KeyboardInterrupt:
            print("\n‚ú® –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            traceback.print_exc()


if __name__ == "__main__":
    main()