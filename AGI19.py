# coding: utf-8
"""
AGI_Enhanced_v24_FactMemory.py ‚Äî –° –§–ê–ö–¢–û–õ–û–ì–ò–ß–ï–°–ö–û–ô –ü–ê–ú–Ø–¢–¨–Æ
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ (—á–∏—Å–µ–ª, –∏–º—ë–Ω, –¥–∞—Ç)
"""

import re
import json
import requests
import time
import os
import sys
import hashlib
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Set, Any
from datetime import datetime, timezone
from collections import defaultdict, Counter
import math


# ================= –ó–ê–ì–†–£–ó–ö–ê –ö–õ–Æ–ß–ê =================
def load_api_key():
    key = os.getenv("OPENROUTER_API_KEY", "")
    if not key:
        env_path = Path(".env")
        if env_path.exists():
            with open(env_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        if "=" in line:
                            k, v = line.split("=", 1)
                            if k.strip() == "OPENROUTER_API_KEY":
                                key = v.strip().strip('"').strip("'")
    return key


# ================= –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =================
class Config:
    ROOT = Path("./cognitive_v24")
    ROOT.mkdir(exist_ok=True)

    # –§–∞–π–ª—ã –ø–∞–º—è—Ç–∏
    SEMANTIC_DB = ROOT / "semantic_memory.json"
    EPISODIC_DB = ROOT / "episodic_memory.json"
    CAUSAL_DB = ROOT / "causal_graph.json"
    WORKING_DB = ROOT / "working_memory.json"
    META_DB = ROOT / "meta_state.json"
    FACTUAL_DB = ROOT / "factual_memory.json"  # –ù–û–í–û–ï!
    LOG = ROOT / "system.log"

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–º—è—Ç–∏
    WORKING_MEMORY_SIZE = 15
    EPISODIC_MEMORY_SIZE = 200
    SEMANTIC_MEMORY_SIZE = 1000
    FACTUAL_MEMORY_SIZE = 500  # –ù–û–í–û–ï!

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    LEARNING_RATE = 0.15
    DECAY_RATE = 0.003  # –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –∑–∞–±—ã–≤–∞–µ–º —Ñ–∞–∫—Ç—ã
    MIN_CONFIDENCE = 0.1
    CONSOLIDATION_THRESHOLD = 3

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–Ω–∏–º–∞–Ω–∏—è
    ATTENTION_TOP_K = 7
    CONTEXT_WINDOW = 5

    # API –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
    OPENROUTER_API_KEY = load_api_key()
    MODEL = "qwen/qwen-2.5-7b-instruct"
    TIMEOUT = 30
    MAX_TOKENS = 400


# ================= –£–¢–ò–õ–ò–¢–´ =================
def clean_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    text = text.lower()
    text = re.sub(r'[^\w\s\-–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def extract_keywords(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    stop_words = {
        '—á—Ç–æ', '–∫–∞–∫', '–ø–æ—á–µ–º—É', '–µ—Å–ª–∏', '—Ç–æ', '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏',
        '—è', '—Ç—ã', '–º—ã', '–≤—ã', '–æ–Ω–∏', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–º–æ–π', '—Ç–≤–æ–π',
        '–≤', '–Ω–∞', '–∏', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–∫', '–æ', '–∏–∑', '—É',
        '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–Ω–∏', '–∂–µ', '–±—ã', '–ª–∏', '—É–∂–µ', '–µ—â–µ',
        '–±—ã—Ç—å', '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏'
    }

    words = clean_text(text).split()
    keywords = [w for w in words if len(w) > 2 and w not in stop_words]
    return keywords


def extract_numbers(text: str) -> List[int]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–µ–ª –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–∏—Å–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ
    numbers = re.findall(r'\b\d+\b', text)
    return [int(n) for n in numbers]


def extract_facts(text: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    facts = {}

    # –ß–∏—Å–ª–∞
    numbers = extract_numbers(text)
    if numbers:
        facts['numbers'] = numbers

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤
    patterns = {
        'name': r'(?:–º–µ–Ω—è –∑–æ–≤—É—Ç|—è|–º–æ–µ –∏–º—è|–º–æ—ë –∏–º—è)\s+([–ê-–Ø–ÅA-Z][–∞-—è—ëa-z]+)',
        'age': r'(?:–º–Ω–µ|–≤–æ–∑—Ä–∞—Å—Ç)\s+(\d+)\s*(?:–ª–µ—Ç|–≥–æ–¥|–≥–æ–¥–∞)',
        'color': r'(?:–ª—é–±–∏–º—ã–π —Ü–≤–µ—Ç|—Ü–≤–µ—Ç)\s+([–∞-—è—ë]+)',
        'city': r'(?:–∂–∏–≤—É –≤|–≥–æ—Ä–æ–¥|–∏–∑ –≥–æ—Ä–æ–¥–∞)\s+([–ê-–Ø–Å][–∞-—è—ë]+)',
    }

    for fact_type, pattern in patterns.items():
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            facts[fact_type] = match.group(1)

    return facts


def text_hash(text: str) -> str:
    """–•–µ—à —Ç–µ–∫—Å—Ç–∞"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()[:12]


def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
    if not vec1 or not vec2:
        return 0.0

    keys = set(vec1.keys()) & set(vec2.keys())
    if not keys:
        return 0.0

    dot = sum(vec1[k] * vec2[k] for k in keys)
    mag1 = math.sqrt(sum(v ** 2 for v in vec1.values()))
    mag2 = math.sqrt(sum(v ** 2 for v in vec2.values()))

    if mag1 == 0 or mag2 == 0:
        return 0.0

    return dot / (mag1 * mag2)


def print_typing(text: str, delay=0.01):
    """–≠—Ñ—Ñ–µ–∫—Ç –ø–µ—á–∞—Ç–∞–Ω–∏—è"""
    for c in text:
        print(c, end="", flush=True)
        time.sleep(delay)
    print(flush=True)


# ================= –§–ê–ö–¢–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ (–ù–û–í–û–ï!) =================
@dataclass
class Fact:
    """–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–∫—Ç"""
    fact_type: str  # 'number', 'name', 'date', 'color', etc.
    value: Any
    context: str  # –í –∫–∞–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —É–ø–æ–º—è–Ω—É—Ç
    timestamp: float
    confidence: float = 1.0
    source: str = "user"  # –æ—Ç–∫—É–¥–∞ –ø–æ–ª—É—á–µ–Ω —Ñ–∞–∫—Ç

    def to_dict(self) -> dict:
        return {
            'fact_type': self.fact_type,
            'value': self.value,
            'context': self.context,
            'timestamp': self.timestamp,
            'confidence': self.confidence,
            'source': self.source
        }

    @staticmethod
    def from_dict(data: dict) -> 'Fact':
        return Fact(
            fact_type=data['fact_type'],
            value=data['value'],
            context=data['context'],
            timestamp=data['timestamp'],
            confidence=data.get('confidence', 1.0),
            source=data.get('source', 'user')
        )


class FactualMemory:
    """–î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å - —Ö—Ä–∞–Ω–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã"""

    def __init__(self):
        self.facts: Dict[str, List[Fact]] = defaultdict(list)
        self.load()

    def add_fact(self, fact_type: str, value: Any, context: str, confidence: float = 1.0):
        """–î–æ–±–∞–≤–∏—Ç—å —Ñ–∞–∫—Ç"""
        fact = Fact(
            fact_type=fact_type,
            value=value,
            context=context[:200],  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            timestamp=time.time(),
            confidence=confidence
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ —Ñ–∞–∫—Ç–∞
        existing = self.facts[fact_type]
        for i, old_fact in enumerate(existing):
            if old_fact.value == value:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–∫—Ç
                existing[i] = fact
                return

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ñ–∞–∫—Ç
        self.facts[fact_type].append(fact)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ–≤ –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞
        if len(self.facts[fact_type]) > 50:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            self.facts[fact_type].sort(key=lambda f: f.confidence * f.timestamp)
            self.facts[fact_type] = self.facts[fact_type][-50:]

    def learn_from_text(self, text: str):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã
        facts = extract_facts(text)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å–ª–∞
        if 'numbers' in facts:
            for num in facts['numbers']:
                # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —á–∏—Å–ª–∞
                context_match = re.search(rf'(.{{0,50}}){num}(.{{0,50}})', text)
                context = text if not context_match else context_match.group(0)
                self.add_fact('number', num, context)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥—Ä—É–≥–∏–µ —Ñ–∞–∫—Ç—ã
        for fact_type, value in facts.items():
            if fact_type != 'numbers':
                self.add_fact(fact_type, value, text)

    def get_facts_by_type(self, fact_type: str) -> List[Fact]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Ñ–∞–∫—Ç—ã –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞"""
        return sorted(
            self.facts.get(fact_type, []),
            key=lambda f: f.timestamp,
            reverse=True
        )

    def search_facts(self, query: str) -> List[Fact]:
        """–ü–æ–∏—Å–∫ —Ñ–∞–∫—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        results = []
        query_lower = query.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ —Ñ–∞–∫—Ç–æ–≤
        if any(word in query_lower for word in ['—á–∏—Å–ª–æ', '—Ü–∏—Ñ—Ä', 'number']):
            results.extend(self.get_facts_by_type('number'))

        if any(word in query_lower for word in ['–∏–º—è', '–∑–æ–≤—É—Ç', 'name']):
            results.extend(self.get_facts_by_type('name'))

        if any(word in query_lower for word in ['–≤–æ–∑—Ä–∞—Å—Ç', '–ª–µ—Ç', 'age']):
            results.extend(self.get_facts_by_type('age'))

        if any(word in query_lower for word in ['—Ü–≤–µ—Ç', 'color']):
            results.extend(self.get_facts_by_type('color'))

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Ç–∏–ø—É, –∏—â–µ–º –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        if not results:
            for fact_list in self.facts.values():
                for fact in fact_list:
                    if query_lower in fact.context.lower():
                        results.append(fact)

        return results[:10]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

    def get_all_facts(self) -> List[Fact]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Ñ–∞–∫—Ç—ã"""
        all_facts = []
        for fact_list in self.facts.values():
            all_facts.extend(fact_list)
        return sorted(all_facts, key=lambda f: f.timestamp, reverse=True)

    def format_facts_for_context(self, facts: List[Fact]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–∫—Ç—ã –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not facts:
            return ""

        lines = []

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        by_type = defaultdict(list)
        for fact in facts:
            by_type[fact.fact_type].append(fact)

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        if 'number' in by_type:
            numbers = [str(f.value) for f in by_type['number']]
            lines.append(f"–ó–∞–ø–æ–º–Ω–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞: {', '.join(numbers)}")

        if 'name' in by_type:
            names = [str(f.value) for f in by_type['name']]
            lines.append(f"–ò–º–µ–Ω–∞: {', '.join(names)}")

        if 'age' in by_type:
            ages = [str(f.value) for f in by_type['age']]
            lines.append(f"–í–æ–∑—Ä–∞—Å—Ç: {', '.join(ages)}")

        if 'color' in by_type:
            colors = [str(f.value) for f in by_type['color']]
            lines.append(f"–¶–≤–µ—Ç–∞: {', '.join(colors)}")

        # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã
        for fact_type, fact_list in by_type.items():
            if fact_type not in ['number', 'name', 'age', 'color']:
                values = [str(f.value) for f in fact_list]
                lines.append(f"{fact_type}: {', '.join(values)}")

        return "\n".join(lines)

    def get_statistics(self) -> dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏"""
        return {
            'total_facts': sum(len(facts) for facts in self.facts.values()),
            'fact_types': len(self.facts),
            'by_type': {k: len(v) for k, v in self.facts.items()}
        }

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–º—è—Ç—å"""
        data = {
            fact_type: [f.to_dict() for f in facts]
            for fact_type, facts in self.facts.items()
        }
        with open(Config.FACTUAL_DB, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å"""
        if Config.FACTUAL_DB.exists():
            try:
                with open(Config.FACTUAL_DB, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for fact_type, facts_data in data.items():
                        self.facts[fact_type] = [
                            Fact.from_dict(f) for f in facts_data
                        ]
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏: {e}")


# ================= –ö–û–ù–¶–ï–ü–¢ =================
@dataclass
class Concept:
    """–ö–æ–Ω—Ü–µ–ø—Ç"""
    name: str
    confidence: float = 0.2
    frequency: int = 0
    last_accessed: float = field(default_factory=time.time)
    relations: Dict[str, float] = field(default_factory=dict)
    causes: Dict[str, float] = field(default_factory=dict)
    effects: Dict[str, float] = field(default_factory=dict)
    contexts: List[str] = field(default_factory=list)
    emotional_valence: float = 0.0
    vector: Dict[str, float] = field(default_factory=dict)

    def reinforce(self, amount: float = None):
        if amount is None:
            amount = Config.LEARNING_RATE
        self.confidence = min(1.0, self.confidence + amount)
        self.frequency += 1
        self.last_accessed = time.time()

    def decay(self):
        self.confidence *= (1 - Config.DECAY_RATE)
        if self.frequency > 0:
            self.frequency -= 1

    def add_relation(self, other: str, strength: float = 0.3):
        current = self.relations.get(other, 0.0)
        self.relations[other] = min(1.0, current + strength)

    def add_context(self, context: str):
        if context not in self.contexts:
            self.contexts.append(context)
            if len(self.contexts) > 10:
                self.contexts.pop(0)

    def update_vector(self, keywords: List[str]):
        for word in keywords:
            self.vector[word] = self.vector.get(word, 0.0) + 1.0
        total = sum(self.vector.values())
        if total > 0:
            self.vector = {k: v / total for k, v in self.vector.items()}

    def to_dict(self) -> dict:
        return {
            'name': self.name,
            'confidence': self.confidence,
            'frequency': self.frequency,
            'last_accessed': self.last_accessed,
            'relations': self.relations,
            'causes': self.causes,
            'effects': self.effects,
            'contexts': self.contexts,
            'emotional_valence': self.emotional_valence,
            'vector': self.vector
        }

    @staticmethod
    def from_dict(data: dict) -> 'Concept':
        return Concept(
            name=data['name'],
            confidence=data.get('confidence', 0.2),
            frequency=data.get('frequency', 0),
            last_accessed=data.get('last_accessed', time.time()),
            relations=data.get('relations', {}),
            causes=data.get('causes', {}),
            effects=data.get('effects', {}),
            contexts=data.get('contexts', []),
            emotional_valence=data.get('emotional_valence', 0.0),
            vector=data.get('vector', {})
        )


# ================= –≠–ü–ò–ó–û–î =================
@dataclass
class Episode:
    """–≠–ø–∏–∑–æ–¥"""
    id: str
    timestamp: float
    input_text: str
    response: str
    concepts: List[str]
    importance: float = 0.5
    emotional_tone: float = 0.0

    def to_dict(self) -> dict:
        return {
            'id': self.id,
            'timestamp': self.timestamp,
            'input_text': self.input_text,
            'response': self.response,
            'concepts': self.concepts,
            'importance': self.importance,
            'emotional_tone': self.emotional_tone
        }

    @staticmethod
    def from_dict(data: dict) -> 'Episode':
        return Episode(
            id=data['id'],
            timestamp=data['timestamp'],
            input_text=data['input_text'],
            response=data['response'],
            concepts=data['concepts'],
            importance=data.get('importance', 0.5),
            emotional_tone=data.get('emotional_tone', 0.0)
        )


# ================= –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ =================
class SemanticMemory:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å"""

    def __init__(self):
        self.concepts: Dict[str, Concept] = {}
        self.load()

    def get_or_create(self, name: str) -> Concept:
        if name not in self.concepts:
            self.concepts[name] = Concept(name=name)
        return self.concepts[name]

    def learn_from_text(self, text: str, importance: float = 0.5):
        keywords = extract_keywords(text)
        for word in keywords:
            concept = self.get_or_create(word)
            concept.reinforce(Config.LEARNING_RATE * importance)
            concept.update_vector(keywords)
            concept.add_context(text[:100])

        for i in range(len(keywords) - 1):
            c1 = self.get_or_create(keywords[i])
            c2 = self.get_or_create(keywords[i + 1])
            c1.add_relation(c2.name, 0.2)
            c2.add_relation(c1.name, 0.15)

    def find_similar(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        query_keywords = extract_keywords(query)
        query_vector = {}
        for word in query_keywords:
            query_vector[word] = query_vector.get(word, 0.0) + 1.0

        total = sum(query_vector.values())
        if total > 0:
            query_vector = {k: v / total for k, v in query_vector.items()}

        similarities = []
        for name, concept in self.concepts.items():
            if concept.confidence < Config.MIN_CONFIDENCE:
                continue

            sim = cosine_similarity(query_vector, concept.vector)
            if sim > 0:
                score = sim * concept.confidence * (1 + math.log1p(concept.frequency))
                similarities.append((name, score))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

    def decay_all(self):
        for concept in self.concepts.values():
            concept.decay()

        to_remove = [
            name for name, c in self.concepts.items()
            if c.confidence < Config.MIN_CONFIDENCE and c.frequency == 0
        ]
        for name in to_remove:
            del self.concepts[name]

    def get_statistics(self) -> dict:
        return {
            'total_concepts': len(self.concepts),
            'strong_concepts': sum(1 for c in self.concepts.values() if c.confidence > 0.5),
            'total_relations': sum(len(c.relations) for c in self.concepts.values()),
            'avg_confidence': sum(c.confidence for c in self.concepts.values()) / max(len(self.concepts), 1)
        }

    def save(self):
        data = {name: concept.to_dict() for name, concept in self.concepts.items()}
        with open(Config.SEMANTIC_DB, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        if Config.SEMANTIC_DB.exists():
            try:
                with open(Config.SEMANTIC_DB, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.concepts = {
                        name: Concept.from_dict(cdata)
                        for name, cdata in data.items()
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏: {e}")


# ================= –≠–ü–ò–ó–û–î–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ =================
class EpisodicMemory:
    """–≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å"""

    def __init__(self):
        self.episodes: List[Episode] = []
        self.load()

    def add(self, input_text: str, response: str, concepts: List[str], importance: float = 0.5):
        episode = Episode(
            id=text_hash(f"{input_text}{time.time()}"),
            timestamp=time.time(),
            input_text=input_text,
            response=response,
            concepts=concepts,
            importance=importance
        )

        self.episodes.append(episode)

        if len(self.episodes) > Config.EPISODIC_MEMORY_SIZE:
            self.episodes.sort(key=lambda e: e.importance * (1 / (time.time() - e.timestamp + 1)))
            self.episodes = self.episodes[-Config.EPISODIC_MEMORY_SIZE:]

    def recall_similar(self, query: str, top_k: int = 3) -> List[Episode]:
        query_keywords = set(extract_keywords(query))

        scored_episodes = []
        for episode in self.episodes:
            episode_keywords = set(extract_keywords(episode.input_text))

            intersection = len(query_keywords & episode_keywords)
            union = len(query_keywords | episode_keywords)

            if union > 0:
                similarity = intersection / union
                recency = 1 / (1 + (time.time() - episode.timestamp) / 86400)
                score = similarity * episode.importance * (0.5 + 0.5 * recency)
                scored_episodes.append((episode, score))

        scored_episodes.sort(key=lambda x: x[1], reverse=True)
        return [ep for ep, _ in scored_episodes[:top_k]]

    def get_recent(self, n: int = 5) -> List[Episode]:
        return sorted(self.episodes, key=lambda e: e.timestamp, reverse=True)[:n]

    def save(self):
        data = [ep.to_dict() for ep in self.episodes]
        with open(Config.EPISODIC_DB, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        if Config.EPISODIC_DB.exists():
            try:
                with open(Config.EPISODIC_DB, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.episodes = [Episode.from_dict(ep) for ep in data]
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏: {e}")


# ================= –ü–†–ò–ß–ò–ù–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ =================
class CausalMemory:
    """–ü—Ä–∏—á–∏–Ω–Ω–∞—è –ø–∞–º—è—Ç—å"""

    def __init__(self):
        self.graph: Dict[str, Dict[str, float]] = {}
        self.load()

    def add_link(self, cause: str, effect: str, strength: float = 0.3):
        if cause not in self.graph:
            self.graph[cause] = {}
        current = self.graph[cause].get(effect, 0.0)
        self.graph[cause][effect] = min(1.0, current + strength)

    def learn_from_conditional(self, text: str):
        text = clean_text(text)

        if '–µ—Å–ª–∏' in text and '—Ç–æ' in text:
            parts = text.split('—Ç–æ', 1)
            condition = parts[0].replace('–µ—Å–ª–∏', '').strip()
            consequence = parts[1].strip()

            cond_keywords = extract_keywords(condition)
            cons_keywords = extract_keywords(consequence)

            if cond_keywords and cons_keywords:
                for c in cond_keywords[-2:]:
                    for e in cons_keywords[:2]:
                        self.add_link(c, e, 0.4)

        elif '–ø–æ—Ç–æ–º—É —á—Ç–æ' in text or '—Ç–∞–∫ –∫–∞–∫' in text:
            if '–ø–æ—Ç–æ–º—É —á—Ç–æ' in text:
                parts = text.split('–ø–æ—Ç–æ–º—É —á—Ç–æ', 1)
            else:
                parts = text.split('—Ç–∞–∫ –∫–∞–∫', 1)

            effect_part = parts[0].strip()
            cause_part = parts[1].strip()

            cause_keywords = extract_keywords(cause_part)
            effect_keywords = extract_keywords(effect_part)

            if cause_keywords and effect_keywords:
                for c in cause_keywords[-2:]:
                    for e in effect_keywords[:2]:
                        self.add_link(c, e, 0.4)

    def predict_chain(self, start: str, max_steps: int = 5) -> List[str]:
        chain = [start]
        current = start

        for _ in range(max_steps):
            if current not in self.graph or not self.graph[current]:
                break

            next_concept = max(self.graph[current].items(), key=lambda x: x[1])
            if next_concept[1] < 0.2:
                break

            if next_concept[0] in chain:
                break

            chain.append(next_concept[0])
            current = next_concept[0]

        return chain

    def decay_all(self):
        for cause in list(self.graph.keys()):
            for effect in list(self.graph[cause].keys()):
                self.graph[cause][effect] *= (1 - Config.DECAY_RATE)

                if self.graph[cause][effect] < Config.MIN_CONFIDENCE:
                    del self.graph[cause][effect]

            if not self.graph[cause]:
                del self.graph[cause]

    def save(self):
        with open(Config.CAUSAL_DB, 'w', encoding='utf-8') as f:
            json.dump(self.graph, f, ensure_ascii=False, indent=2)

    def load(self):
        if Config.CAUSAL_DB.exists():
            try:
                with open(Config.CAUSAL_DB, 'r', encoding='utf-8') as f:
                    self.graph = json.load(f)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏—á–∏–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏: {e}")


# ================= –†–ê–ë–û–ß–ê–Ø –ü–ê–ú–Ø–¢–¨ =================
@dataclass
class WorkingMemoryItem:
    content: str
    timestamp: float
    importance: float
    concepts: List[str]


class WorkingMemory:
    """–†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å"""

    def __init__(self):
        self.items: List[WorkingMemoryItem] = []
        self.attention_focus: Optional[str] = None

    def add(self, content: str, importance: float = 0.5):
        concepts = extract_keywords(content)

        item = WorkingMemoryItem(
            content=content,
            timestamp=time.time(),
            importance=importance,
            concepts=concepts
        )

        self.items.append(item)

        if concepts:
            self.attention_focus = concepts[0]

        if len(self.items) > Config.WORKING_MEMORY_SIZE:
            self.items.sort(key=lambda x: x.importance * (1 / (time.time() - x.timestamp + 1)))
            self.items = self.items[-Config.WORKING_MEMORY_SIZE:]

    def get_recent_context(self, n: int = 5) -> List[str]:
        recent = sorted(self.items, key=lambda x: x.timestamp, reverse=True)[:n]
        return [item.content for item in recent]


# ================= –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê =================
class CognitiveSystemV24:
    """–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç—å—é"""

    def __init__(self):
        print("üß† Cognitive System v24 ‚Äî With Factual Memory\n")

        if not Config.OPENROUTER_API_KEY:
            print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ –Ω–∞–π–¥–µ–Ω OPENROUTER_API_KEY!")
            sys.exit(1)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º –ø–∞–º—è—Ç–∏
        self.semantic = SemanticMemory()
        self.episodic = EpisodicMemory()
        self.causal = CausalMemory()
        self.working = WorkingMemory()
        self.factual = FactualMemory()  # –ù–û–í–û–ï!

        self.meta = self.load_meta()
        self.log_file = open(Config.LOG, 'a', encoding='utf-8')
        self.log("System initialized with factual memory")

        print("‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        self._print_statistics()

    def log(self, message: str):
        timestamp = datetime.now(timezone.utc).isoformat()
        self.log_file.write(f"[{timestamp}] {message}\n")
        self.log_file.flush()

    def _print_statistics(self):
        stats = self.semantic.get_statistics()
        fact_stats = self.factual.get_statistics()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏:")
        print(f"   –ö–æ–Ω—Ü–µ–ø—Ç—ã: {stats['total_concepts']} (—Å–∏–ª—å–Ω—ã—Ö: {stats['strong_concepts']})")
        print(f"   –°–≤—è–∑–∏: {stats['total_relations']}")
        print(f"   –≠–ø–∏–∑–æ–¥—ã: {len(self.episodic.episodes)}")
        print(f"   –ü—Ä–∏—á–∏–Ω–Ω—ã–µ —Å–≤—è–∑–∏: {len(self.causal.graph)}")
        print(f"   –§–∞–∫—Ç—ã: {fact_stats['total_facts']} ({fact_stats['fact_types']} —Ç–∏–ø–æ–≤)")  # –ù–û–í–û–ï!
        print(f"   –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {self.meta['interactions']}")

    def build_context(self, query: str) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ñ–∞–∫—Ç–∞–º–∏"""
        context_parts = []

        # 1. –§–ê–ö–¢–´ (–°–ê–ú–û–ï –í–ê–ñ–ù–û–ï!)
        relevant_facts = self.factual.search_facts(query)
        if relevant_facts:
            context_parts.append("üéØ –ó–ê–ü–û–ú–ù–ï–ù–ù–´–ï –§–ê–ö–¢–´:")
            fact_text = self.factual.format_facts_for_context(relevant_facts)
            context_parts.append(fact_text)

        # 2. –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å
        recent = self.working.get_recent_context(3)
        if recent:
            context_parts.append("\nüí≠ –¢–ï–ö–£–©–ò–ô –ö–û–ù–¢–ï–ö–°–¢:")
            for i, item in enumerate(recent[::-1], 1):
                context_parts.append(f"  {i}. {item[:100]}")

        # 3. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã
        similar_episodes = self.episodic.recall_similar(query, top_k=2)
        if similar_episodes:
            context_parts.append("\nüìö –†–ï–õ–ï–í–ê–ù–¢–ù–´–ô –û–ü–´–¢:")
            for i, ep in enumerate(similar_episodes, 1):
                context_parts.append(f"  {i}. –í–æ–ø—Ä–æ—Å: {ep.input_text[:80]}")
                context_parts.append(f"     –û—Ç–≤–µ—Ç: {ep.response[:80]}")

        # 4. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
        similar_concepts = self.semantic.find_similar(query, top_k=4)
        if similar_concepts:
            context_parts.append("\nüîó –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¢–´:")
            for name, score in similar_concepts[:3]:
                concept = self.semantic.concepts[name]
                context_parts.append(
                    f"  ‚Ä¢ {name} (conf: {concept.confidence:.2f}, freq: {concept.frequency})"
                )

        if context_parts:
            return "\n".join(context_parts)

        return ""

    def process(self, user_input: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        self.meta['interactions'] += 1
        self.log(f"INPUT: {user_input}")

        # 1. –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å
        self.working.add(user_input, importance=0.7)

        # 2. –ò–ó–í–õ–ï–ö–ê–ï–ú –ò –°–û–•–†–ê–ù–Ø–ï–ú –§–ê–ö–¢–´!
        self.factual.learn_from_text(user_input)

        # 3. –û–±—É—á–∞–µ–º—Å—è
        self.semantic.learn_from_text(user_input, importance=0.6)
        self.causal.learn_from_conditional(user_input)

        # 4. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
        if user_input.lower() in ['—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', 'stats', '–ø–∞–º—è—Ç—å']:
            return self._handle_stats_command()

        if user_input.lower().startswith('–≤—Å–ø–æ–º–Ω–∏'):
            return self._handle_recall_command(user_input)

        if user_input.lower() in ['—Ñ–∞–∫—Ç—ã', 'facts']:
            return self._handle_facts_command()

        # 5. –°—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = self.build_context(user_input)

        # 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self._query_llm(user_input, context)

        # 7. –ò–ó–í–õ–ï–ö–ê–ï–ú –§–ê–ö–¢–´ –ò–ó –û–¢–í–ï–¢–ê
        self.factual.learn_from_text(response)

        # 8. –û–±—É—á–∞–µ–º—Å—è –∏–∑ –æ—Ç–≤–µ—Ç–∞
        self.semantic.learn_from_text(response, importance=0.5)

        # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–ø–∏–∑–æ–¥
        concepts = extract_keywords(user_input) + extract_keywords(response)
        self.episodic.add(user_input, response, list(set(concepts)), importance=0.6)

        # 10. –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è
        if self.meta['interactions'] % 10 == 0:
            self._consolidate_memory()

        # 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_all()

        self.log(f"OUTPUT: {response[:100]}...")

        return response

    def _query_llm(self, query: str, context: str) -> str:
        """–ó–∞–ø—Ä–æ—Å –∫ LLM —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        try:
            system_prompt = (
                "–¢—ã ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é. "
                "–í–ê–ñ–ù–û: –í —Ä–∞–∑–¥–µ–ª–µ '–ó–ê–ü–û–ú–ù–ï–ù–ù–´–ï –§–ê–ö–¢–´' –Ω–∞—Ö–æ–¥—è—Ç—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –î–û–õ–ñ–ï–ù –ø–æ–º–Ω–∏—Ç—å. "
                "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ —Ñ–∞–∫—Ç—ã –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.\n\n"
            )

            if context:
                system_prompt += f"–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ü–ê–ú–Ø–¢–ò:\n{context}\n\n"
                system_prompt += (
                    "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ '–ó–ê–ü–û–ú–ù–ï–ù–ù–´–ï –§–ê–ö–¢–´' –µ—Å—Ç—å —á–∏—Å–ª–∞, –∏–º–µ–Ω–∞ –∏–ª–∏ –¥—Ä—É–≥–∏–µ —Ñ–∞–∫—Ç—ã ‚Äî "
                    "–∏—Å–ø–æ–ª—å–∑—É–π –ò–ú–ï–ù–ù–û –∏—Ö –≤ –æ—Ç–≤–µ—Ç–µ. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ —Ñ–∞–∫—Ç—ã."
                )

            if context:
                print(f"\nüß† –ò—Å–ø–æ–ª—å–∑—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ ({len(context)} —Å–∏–º–≤–æ–ª–æ–≤)")
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∞–∫—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
                if "–ó–ê–ü–û–ú–ù–ï–ù–ù–´–ï –§–ê–ö–¢–´" in context:
                    print("üìå –ù–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –≤ –ø–∞–º—è—Ç–∏!")

            headers = {
                "Authorization": f"Bearer {Config.OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "http://localhost:8000",
                "X-Title": "CognitiveSystemV24"
            }

            payload = {
                "model": Config.MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                "temperature": 0.3,  # –ù–∏–∂–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ñ–∞–∫—Ç–æ–≤
                "max_tokens": Config.MAX_TOKENS
            }

            print("‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...", flush=True)

            response = requests.post(
                Config.OPENROUTER_URL,
                headers=headers,
                json=payload,
                timeout=Config.TIMEOUT
            )

            response.raise_for_status()
            content = response.json()["choices"][0]["message"]["content"].strip()

            return content

        except Exception as e:
            error_msg = f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {str(e)[:100]}"
            self.log(f"API ERROR: {e}")
            return error_msg

    def _handle_stats_command(self) -> str:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        stats = self.semantic.get_statistics()
        fact_stats = self.factual.get_statistics()

        output = ["üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–û–ì–ù–ò–¢–ò–í–ù–û–ô –°–ò–°–¢–ï–ú–´\n"]
        output.append(f"–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {self.meta['interactions']}")

        output.append(f"\n–§–ê–ö–¢–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨:")
        output.append(f"  ‚Ä¢ –í—Å–µ–≥–æ —Ñ–∞–∫—Ç–æ–≤: {fact_stats['total_facts']}")
        output.append(f"  ‚Ä¢ –¢–∏–ø–æ–≤ —Ñ–∞–∫—Ç–æ–≤: {fact_stats['fact_types']}")
        for fact_type, count in fact_stats['by_type'].items():
            output.append(f"    - {fact_type}: {count}")

        output.append(f"\n–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨:")
        output.append(f"  ‚Ä¢ –ö–æ–Ω—Ü–µ–ø—Ç—ã: {stats['total_concepts']}")
        output.append(f"  ‚Ä¢ –°–≤—è–∑–∏: {stats['total_relations']}")

        output.append(f"\n–≠–ü–ò–ó–û–î–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨:")
        output.append(f"  ‚Ä¢ –≠–ø–∏–∑–æ–¥—ã: {len(self.episodic.episodes)}")

        output.append(f"\n–ü–†–ò–ß–ò–ù–ù–ê–Ø –ü–ê–ú–Ø–¢–¨:")
        output.append(f"  ‚Ä¢ –ü—Ä–∏—á–∏–Ω–Ω—ã–µ —Å–≤—è–∑–∏: {len(self.causal.graph)}")

        return "\n".join(output)

    def _handle_facts_command(self) -> str:
        """–ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ —Ñ–∞–∫—Ç—ã"""
        all_facts = self.factual.get_all_facts()

        if not all_facts:
            return "ü§î –§–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –ø—É—Å—Ç–∞."

        output = ["üìö –í–°–ï –ó–ê–ü–û–ú–ù–ï–ù–ù–´–ï –§–ê–ö–¢–´:\n"]

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        by_type = defaultdict(list)
        for fact in all_facts:
            by_type[fact.fact_type].append(fact)

        for fact_type, facts in by_type.items():
            output.append(f"\n{fact_type.upper()}:")
            for fact in facts[:10]:  # –º–∞–∫—Å–∏–º—É–º 10 –Ω–∞ —Ç–∏–ø
                time_str = datetime.fromtimestamp(fact.timestamp).strftime('%Y-%m-%d %H:%M')
                output.append(f"  ‚Ä¢ {fact.value} [{time_str}]")
                if fact.context:
                    output.append(f"    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {fact.context[:60]}...")

        return "\n".join(output)

    def _handle_recall_command(self, command: str) -> str:
        """–í–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è"""
        query = command.replace('–≤—Å–ø–æ–º–Ω–∏', '').strip()

        if not query:
            recent = self.episodic.get_recent(5)
            if not recent:
                return "ü§î –≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –ø—É—Å—Ç–∞."

            output = ["üìö –ü–û–°–õ–ï–î–ù–ò–ï –í–û–°–ü–û–ú–ò–ù–ê–ù–ò–Ø:\n"]
            for i, ep in enumerate(recent, 1):
                time_str = datetime.fromtimestamp(ep.timestamp).strftime('%Y-%m-%d %H:%M')
                output.append(f"{i}. [{time_str}]")
                output.append(f"   Q: {ep.input_text[:80]}")
                output.append(f"   A: {ep.response[:80]}\n")

            return "\n".join(output)
        else:
            similar = self.episodic.recall_similar(query, top_k=3)
            if not similar:
                return f"ü§î –ù–µ—Ç –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –æ: {query}"

            output = [f"üìö –í–û–°–ü–û–ú–ò–ù–ê–ù–ò–Ø –û '{query}':\n"]
            for i, ep in enumerate(similar, 1):
                time_str = datetime.fromtimestamp(ep.timestamp).strftime('%Y-%m-%d %H:%M')
                output.append(f"{i}. [{time_str}]")
                output.append(f"   Q: {ep.input_text}")
                output.append(f"   A: {ep.response}\n")

            return "\n".join(output)

    def _consolidate_memory(self):
        """–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è"""
        print("üîÑ –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–∞–º—è—Ç–∏...", end=" ", flush=True)

        self.semantic.decay_all()
        self.causal.decay_all()

        concept_counter = Counter()
        for item in self.working.items:
            for concept in item.concepts:
                concept_counter[concept] += 1

        for concept_name, count in concept_counter.items():
            if count >= Config.CONSOLIDATION_THRESHOLD:
                concept = self.semantic.get_or_create(concept_name)
                concept.reinforce(0.2)

        print("‚úì")
        self.log("Memory consolidated")

    def save_all(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å—ë"""
        self.semantic.save()
        self.episodic.save()
        self.causal.save()
        self.factual.save()  # –ù–û–í–û–ï!

        with open(Config.META_DB, 'w', encoding='utf-8') as f:
            json.dump(self.meta, f, ensure_ascii=False, indent=2)

    def load_meta(self) -> dict:
        if Config.META_DB.exists():
            try:
                with open(Config.META_DB, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

        return {
            'interactions': 0,
            'created_at': datetime.now(timezone.utc).isoformat()
        }

    def __del__(self):
        if hasattr(self, 'log_file'):
            self.log_file.close()


# ================= –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê =================
def run_diagnosis() -> bool:
    print("=" * 70)
    print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´")
    print("=" * 70)

    if not Config.OPENROUTER_API_KEY:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω OPENROUTER_API_KEY")
        return False

    print(f"‚úÖ API –∫–ª—é—á: {Config.OPENROUTER_API_KEY[:12]}...{Config.OPENROUTER_API_KEY[-4:]}")
    print(f"‚úÖ –ú–æ–¥–µ–ª—å: {Config.MODEL}")
    print(f"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {Config.ROOT}")

    try:
        print("\nüì° –ü—Ä–æ–≤–µ—Ä–∫–∞ API...", end=" ", flush=True)

        headers = {
            "Authorization": f"Bearer {Config.OPENROUTER_API_KEY}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": Config.MODEL,
            "messages": [{"role": "user", "content": "test"}],
            "max_tokens": 5
        }

        response = requests.post(
            Config.OPENROUTER_URL,
            headers=headers,
            json=payload,
            timeout=10
        )

        if response.status_code == 200:
            print("‚úÖ –£–°–ü–ï–®–ù–û")
            return True
        else:
            print(f"‚ùå –û–®–ò–ë–ö–ê {response.status_code}")
            return False

    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê: {e}")
        return False


# ================= MAIN =================
def main():
    if sys.platform == "win32":
        try:
            import ctypes
            kernel32 = ctypes.windll.kernel32
            kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)
        except:
            pass

    print("\n" + "=" * 70)
    print("üß† COGNITIVE SYSTEM v24")
    print("   With Factual Memory ‚Äî REMEMBERS NUMBERS!")
    print("=" * 70 + "\n")

    if not run_diagnosis():
        print("\n‚ùå –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞.")
        return

    print("\n" + "=" * 70)
    print("üöÄ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø")
    print("=" * 70 + "\n")

    system = CognitiveSystemV24()

    print("\n" + "=" * 70)
    print("üí¨ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê")
    print("=" * 70)
    print("\nüéØ –ù–æ–≤–æ–µ –≤ v24:")
    print("  ‚Ä¢ –§–ê–ö–¢–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –ü–ê–ú–Ø–¢–¨ ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç —á–∏—Å–ª–∞, –∏–º–µ–Ω–∞, –¥–∞—Ç—ã")
    print("  ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞")
    print("  ‚Ä¢ –§–∞–∫—Ç—ã –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è LLM")
    print("\nüìã –ö–æ–º–∞–Ω–¥—ã:")
    print("  ‚Ä¢ '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞' ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏")
    print("  ‚Ä¢ '—Ñ–∞–∫—Ç—ã' ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã")
    print("  ‚Ä¢ '–≤—Å–ø–æ–º–Ω–∏' ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è")
    print("  ‚Ä¢ '–≤—ã—Ö–æ–¥' ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å")
    print("=" * 70 + "\n")

    while True:
        try:
            user_input = input("üí≠ –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['exit', '–≤—ã—Ö–æ–¥', 'quit', 'q']:
                print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ...")
                system.save_all()
                print("üíæ –ü–∞–º—è—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
                break

            print()
            response = system.process(user_input)

            print("\nü§ñ –û—Ç–≤–µ—Ç:")
            print_typing(response, delay=0.01)

            print("\n" + "-" * 70 + "\n")

        except KeyboardInterrupt:
            print("\n\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ")
            system.save_all()
            print("üíæ –ü–∞–º—è—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            break

        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()