import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Set
import requests
import os
import traceback
from collections import Counter
import re


# ======================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –û–ß–ò–°–¢–ö–ò –ò –†–ê–ó–ú–ï–¢–ö–ò
# ======================

def clean_qwen_response(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç Qwen –æ—Ç markdown, –ª–∏—à–Ω–µ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –æ–±—Ä–µ–∑–∞–µ—Ç –¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."""
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    text = re.sub(r'#{1,3}\s*', '', text)
    text = re.sub(r'>\s*', '', text)
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()

    text = re.sub(r'^[\*\.\!\?\:\-\‚Äì‚Äî\s]+', '', text)
    text = re.sub(r'[\*\.\!\?\:\-\‚Äì‚Äî\s]+$', '', text)

    words = text.split()
    if len(words) > 60:
        text = ' '.join(words[:60])
        if not text.endswith(('.', '!', '?')):
            text += '.'

    return text if text else "–•–æ—Ä–æ—à–æ."


def safe_cell_name(base: str) -> str:
    name = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø0-9_]', '_', base)
    name = re.sub(r'_+', '_', name)
    name = name.strip('_')
    return name if name else "unknown"


def clean_for_similarity(text: str) -> str:
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.lower().strip()


# ======================
# –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –†–ê–ó–ú–ï–¢–ö–ê –ó–ù–ê–ù–ò–ô
# ======================

def classify_and_tag_response(text: str) -> str:
    if not text.strip():
        return "[SOC] –•–æ—Ä–æ—à–æ."

    text = clean_qwen_response(text)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    tagged_parts = []

    for sent in sentences:
        if not sent.strip():
            continue
        tag = _detect_sentence_type(sent)
        tagged_parts.append(f"[{tag}] {sent}")

    return " ".join(tagged_parts)


def _detect_sentence_type(sentence: str) -> str:
    s = sentence.lower().strip()

    if re.search(r'\b(—á—Ç–æ —Ç–∞–∫–æ–µ|–ø–æ—á–µ–º—É|–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç|–æ–±—ä—è—Å–Ω–∏|–∑–Ω–∞—á–∏—Ç –ª–∏|–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ|—Ä–∞–∑–º—ã—à–ª—è|–¥—É–º–∞–µ—à—å|—á—Ç–æ –∑–Ω–∞—á–∏—Ç)\b', s):
        return "MET"
    if re.search(r'\b(—á—Ç–æ–±—ã|–Ω—É–∂–Ω–æ|—Å–ª–µ–¥—É–µ—Ç|—à–∞–≥|—Å–Ω–∞—á–∞–ª–∞|–ø–æ—Ç–æ–º|–∑–∞—Ç–µ–º|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è|–∞–ª–≥–æ—Ä–∏—Ç–º|–∫–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å|–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å)\b', s):
        return "PRC"
    if re.search(r'\b(–ø–æ—Ç–æ–º—É —á—Ç–æ|—Ç–∞–∫ –∫–∞–∫|–∏–∑-–∑–∞|—Å–ª–µ–¥—Å—Ç–≤–∏–µ|–ø—Ä–∏—á–∏–Ω–∞|–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–∑-–∑–∞|–≤–µ–¥—ë—Ç –∫|–æ–±—É—Å–ª–æ–≤–ª–µ–Ω–æ)\b', s):
        return "CAU"
    if re.search(r'.+ ‚Äî —ç—Ç–æ .+|.+ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è .+|.+ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ .+|—Å—Ç–æ–ª–∏—Ü–∞ .+ ‚Äî .+|—Ñ–æ—Ä–º—É–ª–∞ .+ ‚Äî .+', s):
        return "FCT"
    if re.search(r'\b(—è –¥—É–º–∞—é|–º–Ω–µ –∫–∞–∂–µ—Ç—Å—è|–ø–æ –º–æ–µ–º—É –º–Ω–µ–Ω–∏—é|—è —Å—á–∏—Ç–∞—é|–º–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è|—Å–∫—É—á–Ω—ã–π|–æ—Ç–ª–∏—á–Ω—ã–π|–ª—É—á—à–µ|—Ö—É–∂–µ)\b', s):
        return "OPN"
    if re.search(r'\b(–ø—Ä–µ–¥—Å—Ç–∞–≤—å|–≤–æ–æ–±—Ä–∞–∑–∏|–∫–∞–∫ –±—É–¥—Ç–æ|—Å–ª–æ–≤–Ω–æ|–ø–æ–¥–æ–±–Ω–æ|–∂–∏–∑–Ω—å ‚Äî|–º–∏—Ä –∫–∞–∫|–µ—Å–ª–∏ –±—ã|—Ñ–∞–Ω—Ç–∞–∑–∏—è)\b', s):
        return "CRT"

    social_keywords = ["–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "–¥–æ–±—Ä—ã–π", "—Å–ø–∞—Å–∏–±–æ", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "–∏–∑–≤–∏–Ω–∏", "—Ö–æ—Ä–æ—à–æ", "–ª–∞–¥–Ω–æ", "–æ–∫", "–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é"]
    if any(kw in s for kw in social_keywords) or len(s.split()) <= 3:
        return "SOC"

    return "FCT"


def detect_input_type(user_input: str) -> str:
    s = user_input.lower().strip()
    if re.search(r'\b(–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π|–¥–æ–±—Ä—ã–π –¥–µ–Ω—å|–∫–∞–∫ –¥–µ–ª–∞|–ø–æ–∫–∞)\b', s):
        return "SOC"
    if re.search(r'\b(—á—Ç–æ —Ç–∞–∫–æ–µ|–∫—Ç–æ —Ç–∞–∫–æ–π|–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è|–∫–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞|—Ñ–æ—Ä–º—É–ª–∞|–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)\b', s):
        return "FCT"
    if re.search(r'\b(–ø–æ—á–µ–º—É|–∑–∞—á–µ–º|–æ—Ç—á–µ–≥–æ|–ø—Ä–∏—á–∏–Ω–∞)\b', s):
        return "CAU"
    if re.search(r'\b(–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å|–∫–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è|—à–∞–≥|–∞–ª–≥–æ—Ä–∏—Ç–º)\b', s):
        return "PRC"
    if re.search(r'\b(–∫–∞–∫ —Ç—ã –¥—É–º–∞–µ—à—å|—Ç–≤–æ—ë –º–Ω–µ–Ω–∏–µ|–ª—É—á—à–µ –ª–∏|–Ω—Ä–∞–≤–∏—Ç—Å—è –ª–∏)\b', s):
        return "OPN"
    if re.search(r'\b(–ø—Ä–µ–¥—Å—Ç–∞–≤—å|–≤–æ–æ–±—Ä–∞–∑–∏|—Å–æ—á–∏–Ω–∏|–æ–ø–∏—à–∏ –∫–∞–∫|–º–µ—Ç–∞—Ñ–æ—Ä–∞)\b', s):
        return "CRT"
    if re.search(r'\b(–ø–æ—á–µ–º—É —Ç—ã|–∫–∞–∫ —Ç—ã –ø–æ–Ω—è–ª|—á—Ç–æ —Ç—ã –∏–º–µ–ª –≤ –≤–∏–¥—É|–æ–±—ä—è—Å–Ω–∏ —Å–≤–æ–π –æ—Ç–≤–µ—Ç)\b', s):
        return "MET"
    return "FCT"


# ======================
# –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –ö–õ–ï–¢–ö–ê
# ======================

class BrainCell(nn.Module):
    def __init__(self, cell_id: int, input_size: int, hidden_size: int, output_size: int, cell_type: str = "generic"):
        super().__init__()
        self.cell_id = cell_id
        self.cell_type = cell_type
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.input_adapter = nn.Linear(input_size, hidden_size) if input_size != hidden_size else nn.Identity()
        self.perception = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.LayerNorm(hidden_size)
        )
        self.memory = nn.LSTMCell(hidden_size, hidden_size)
        self.association = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh()
        )
        # –£–ë–†–ê–ù–û: output_proj –∏ emotion_bias ‚Äî –æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã –≤–Ω—É—Ç—Ä–∏ –∫–ª–µ—Ç–∫–∏
        self.activation_level = 0.0
        self.confidence = 0.0

    def forward(self, x: torch.Tensor, hx: Optional[torch.Tensor] = None, cx: Optional[torch.Tensor] = None):
        batch_size = x.size(0)
        device = x.device

        x = self.input_adapter(x)
        perceived = self.perception(x)
        self.activation_level = torch.mean(torch.abs(perceived)).item()

        if hx is None:
            hx = torch.zeros(batch_size, self.hidden_size, device=device)
        if cx is None:
            cx = torch.zeros(batch_size, self.hidden_size, device=device)

        hx, cx = self.memory(perceived, (hx, cx))
        associated = self.association(hx)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¢–û–õ–¨–ö–û —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        return associated, hx, cx, self.activation_level


# ======================
# –ú–ï–¢–ê–ö–û–ì–ù–ò–¢–ò–í–ù–´–ô –ú–û–î–£–õ–¨
# ======================

class MetaCognition:
    def __init__(self, vocab: Dict[str, int]):
        self.vocab = vocab
        self.unknown_concepts: Set[str] = set()
        self.reflection_log: List[Dict] = []

    def detect_unknown_words(self, text: str) -> Set[str]:
        words = set(clean_for_similarity(text).split())
        known = set(k for k in self.vocab.keys() if clean_for_similarity(k))
        return words - known

    def log_reflection(self, user_input: str, qwen_resp: str, brain_resp: str, question: Optional[str], reason: str):
        entry = {
            "timestamp": datetime.now().isoformat(),
            "user_input": user_input,
            "qwen_response": qwen_resp,
            "brain_response": brain_resp,
            "generated_question": question,
            "reason": reason
        }
        self.reflection_log.append(entry)

    def _estimate_similarity_semantic(self, resp1: str, resp2: str) -> float:
        words1 = resp1.split()
        words2 = resp2.split()
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0

        all_words = words1 + words2
        freq = Counter(all_words)
        tf1 = Counter(words1)
        tf2 = Counter(words2)

        vec1 = np.array([tf1.get(w, 0) / freq[w] for w in freq])
        vec2 = np.array([tf2.get(w, 0) / freq[w] for w in freq])

        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        cos_sim = np.dot(vec1, vec2) / (norm1 * norm2)
        return float(cos_sim)

    def should_reflect(self, user_input: str, qwen_resp: str, brain_resp: str, vocab: Dict[str, int]) -> Tuple[bool, str]:
        expected_type = detect_input_type(user_input)
        actual_type = "FCT"
        marker_match = re.search(r'\[(SOC|FCT|CAU|PRC|OPN|MET|CRT)\]', qwen_resp)
        if marker_match:
            actual_type = marker_match.group(1)

        type_mismatch = False
        reason = ""

        if expected_type == "FCT" and actual_type in ["SOC", "OPN"]:
            type_mismatch = True
            reason = f"–æ–∂–∏–¥–∞–ª—Å—è —Ñ–∞–∫—Ç [FCT], –Ω–æ Qwen –¥–∞–ª {actual_type}"
        elif expected_type == "PRC" and actual_type not in ["PRC", "FCT"]:
            type_mismatch = True
            reason = f"–æ–∂–∏–¥–∞–ª–∞—Å—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è [PRC], –Ω–æ Qwen –¥–∞–ª {actual_type}"
        elif expected_type == "CRT" and actual_type in ["SOC", "FCT"]:
            type_mismatch = True
            reason = f"–æ–∂–∏–¥–∞–ª–æ—Å—å —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ [CRT], –Ω–æ Qwen –¥–∞–ª {actual_type}"

        clean_brain = clean_for_similarity(brain_resp)
        clean_qwen = clean_for_similarity(qwen_resp)
        similarity = self._estimate_similarity_semantic(clean_brain, clean_qwen)

        if type_mismatch or similarity < 0.35:
            return True, reason if type_mismatch else "–Ω–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"
        return False, ""


# ======================
# –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –°–ï–¢–¨
# ======================

class CognitiveNetwork(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int = 256, hidden_size: int = 512, eos_token_id: int = 1):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.eos_token_id = eos_token_id

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.embed_proj = nn.Linear(embedding_dim, hidden_size)
        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=8, batch_first=True)
        self.norm = nn.LayerNorm(hidden_size)
        self.final_proj = nn.Linear(hidden_size, vocab_size)  # ‚Üê –ï–î–ò–ù–°–¢–í–ï–ù–ù–ê–Ø –ü–†–û–ï–ö–¶–ò–Ø –í –°–õ–û–í–ê–†–¨

        self.cells = nn.ModuleDict()
        self.cell_states = {}
        self.cell_activations = {}

        self.cell_counter = 0
        self.activation_threshold = 0.25
        self.thought_cycles = 3

        self.meta_cog = None

    def _initialize_base_cells(self):
        base_cells = [
            ("social_greeting", "social"),
            ("social_thanks", "social"),
            ("fact_definition", "fact"),
            ("cause_explanation", "cause"),
            ("procedure_step", "procedure"),
            ("opinion_expression", "opinion"),
            ("meta_question", "meta"),
            ("creative_metaphor", "creative")
        ]
        for name, ctype in base_cells:
            # –í—Å–µ –∫–ª–µ—Ç–∫–∏: input=hidden, output=hidden
            self.add_cell(name, self.hidden_size, self.hidden_size, self.hidden_size, ctype)

    def add_cell(self, cell_type: str, input_size: int, hidden_size: int, output_size: int, cell_subtype: str):
        # output_size –ò–ì–ù–û–†–ò–†–£–ï–¢–°–Ø ‚Äî –≤—Å–µ –∫–ª–µ—Ç–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ hidden_size
        safe_type = safe_cell_name(cell_type)
        cell_id = f"{safe_type}_{self.cell_counter}"
        self.cells[cell_id] = BrainCell(self.cell_counter, input_size, hidden_size, hidden_size, cell_subtype)
        self.cell_states[cell_id] = (None, None)
        self.cell_activations[cell_id] = 0.0
        self.cell_counter += 1
        return cell_id

    def reset_cell_states(self, batch_size: int, device: torch.device):
        for cell_id in self.cells:
            self.cell_states[cell_id] = (
                torch.zeros(batch_size, self.hidden_size, device=device),
                torch.zeros(batch_size, self.hidden_size, device=device)
            )

    def _process_step(self, token_emb: torch.Tensor):
        batch_size = token_emb.size(0)
        device = token_emb.device
        x = token_emb  # [B, hidden_size]

        stages = ["social", "fact", "cause", "procedure", "opinion", "meta", "creative"]
        for stage in stages:
            stage_outputs = []
            for cid in [c for c in self.cells if stage in c]:
                cell = self.cells[cid]
                hx, cx = self.cell_states[cid]
                if hx is None:
                    hx = torch.zeros(batch_size, self.hidden_size, device=device)
                    cx = torch.zeros(batch_size, self.hidden_size, device=device)
                # –ü–æ–ª—É—á–∞–µ–º –°–ö–†–´–¢–û–ï —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                cell_output, new_hx, new_cx, act = cell(x, hx, cx)
                self.cell_states[cid] = (new_hx, new_cx)
                self.cell_activations[cid] = act
                stage_outputs.append(cell_output)
            if stage_outputs:
                x = torch.mean(torch.stack(stage_outputs), dim=0)

        # –ï–î–ò–ù–°–¢–í–ï–ù–ù–ê–Ø –ü–†–û–ï–ö–¶–ò–Ø –í –°–õ–û–í–ê–†–¨
        return self.final_proj(x)

    def process_sequence(self, input_tokens: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len = input_tokens.shape
        device = input_tokens.device
        self.reset_cell_states(batch_size, device)

        embedded = self.embedding(input_tokens)
        embedded = self.embed_proj(embedded)
        attn_out, _ = self.attention(embedded, embedded, embedded)
        combined = self.norm(embedded + attn_out)

        outputs = []
        for t in range(seq_len):
            token_emb = combined[:, t, :]
            logits = self._process_step(token_emb)
            outputs.append(logits)

        return torch.stack(outputs, dim=1)

    def generate_sequence(self, input_tokens: torch.Tensor, max_length: int = 20, base_temperature: float = 0.9) -> List[int]:
        device = input_tokens.device
        batch_size = input_tokens.size(0)

        with torch.no_grad():
            embedded = self.embedding(input_tokens)
            embedded = self.embed_proj(embedded)
            attn_out, _ = self.attention(embedded, embedded, embedded)
            combined = self.norm(embedded + attn_out)

            for t in range(combined.size(1)):
                token_emb = combined[:, t, :]
                _ = self._process_step(token_emb)

        generated = []
        current_token = input_tokens[:, -1:]

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–æ –º–∞—Ä–∫–µ—Ä—É
        temperature = base_temperature
        if current_token.numel() > 0:
            last_token_id = current_token.item()
            if 3 <= last_token_id <= 9:  # ID –º–∞—Ä–∫–µ—Ä–æ–≤ [SOC]...[CRT]
                marker_map = {3: "SOC", 4: "FCT", 5: "CAU", 6: "PRC", 7: "OPN", 8: "MET", 9: "CRT"}
                marker = marker_map.get(last_token_id, "FCT")
                if marker in ["FCT", "CAU"]:
                    temperature = 0.3
                elif marker == "CRT":
                    temperature = 1.2
                elif marker == "OPN":
                    temperature = 0.7
                else:
                    temperature = 0.6

        for _ in range(max_length):
            with torch.no_grad():
                emb = self.embedding(current_token)
                emb = self.embed_proj(emb)
                token_emb = emb.squeeze(1)

                for _ in range(self.thought_cycles):
                    logits = self._process_step(token_emb)

                probs = F.softmax(logits / temperature, dim=-1)
                next_token = torch.multinomial(probs, 1)
                token_id = next_token.item()

                if token_id == self.eos_token_id or len(generated) >= max_length:
                    break

                generated.append(token_id)
                current_token = next_token

        if not generated:
            fallback = [10, 13, 33, 39]
            generated = random.choices(fallback, k=min(2, max_length))
        return generated

    def _check_cell_creation(self, user_input: str, qwen_resp: str, brain_resp: str):
        unknown = self.meta_cog.detect_unknown_words(qwen_resp)
        if unknown:
            for word in list(unknown)[:2]:
                clean_word = safe_cell_name(word)
                self.meta_cog.unknown_concepts.add(word)
                self.add_cell(f"concept_{clean_word}", self.hidden_size, self.hidden_size, self.hidden_size, "association")
                print(f"üß¨ –°–æ–∑–¥–∞–Ω–∞ –∫–ª–µ—Ç–∫–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–Ω—è—Ç–∏—è: {word} ‚Üí concept_{clean_word}")

        high_activation = [cid for cid, act in self.cell_activations.items() if act > self.activation_threshold]
        if len(high_activation) >= 3 and random.random() < 0.3:
            cell_types = ["social", "fact", "cause", "procedure", "opinion", "meta", "creative"]
            new_type = random.choice(cell_types)
            new_id = self.add_cell(f"{new_type}_adaptive", self.hidden_size, self.hidden_size, self.hidden_size, new_type)
            print(f"üß¨ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–ª–µ—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {new_id}")
            return True
        return False

    def reflect_and_learn(self, user_input: str, qwen_response: str, vocab: Dict[str, int], ivocab: Dict[int, str]) -> Optional[str]:
        input_tokens = self.text_to_tokens(user_input, vocab)
        with torch.no_grad():
            brain_tokens = self.generate_sequence(torch.tensor([input_tokens], dtype=torch.long), max_length=15)
            brain_response = " ".join(ivocab.get(tid, "<UNK>") for tid in brain_tokens)

        tokens = brain_response.split()
        if not tokens:
            return None
        unk_ratio = brain_response.count("<UNK>") / len(tokens)
        if unk_ratio > 0.7:
            return None

        should_reflect, reason = self.meta_cog.should_reflect(user_input, qwen_response, brain_response, vocab)
        if should_reflect:
            question = self._formulate_deep_question(user_input, qwen_response, brain_response, vocab)
            self.meta_cog.log_reflection(user_input, qwen_response, brain_response, question, reason)
            return question
        return None

    def _formulate_deep_question(self, user_input: str, qwen_resp: str, brain_resp: str, vocab: Dict[str, int]) -> str:
        unknown = self.meta_cog.detect_unknown_words(qwen_resp)
        if unknown:
            word = next(iter(unknown))
            if len(word) > 2:
                return f"—á—Ç–æ –∑–Ω–∞—á–∏—Ç '{word}'?"

        if "–Ω–µ—Ç" in brain_resp.lower() and "–¥–∞" in qwen_resp.lower():
            return f"–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: —Ç—ã —Å–∫–∞–∑–∞–ª '{brain_resp}', –∞ Qwen ‚Äî '{qwen_resp[:30]}...'. –ö—Ç–æ –ø—Ä–∞–≤?"

        return f"–æ–±—ä—è—Å–Ω–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ: {user_input}"

    def text_to_tokens(self, text: str, vocab: Dict[str, int]) -> List[int]:
        words = text.lower().replace('.', ' .').replace(',', ' ,').replace('?', ' ?').replace('!', ' !').split()
        tokens = []
        for word in words:
            if word not in vocab:
                vocab[word] = len(vocab)
            tokens.append(vocab[word])
        return tokens

    def find_activated_cells_by_type(self, threshold: float = 0.15) -> Dict[str, List[str]]:
        grouped = {
            "social": [], "fact": [], "cause": [], "procedure": [],
            "opinion": [], "meta": [], "creative": [], "other": []
        }

        for cid, act in self.cell_activations.items():
            if act <= threshold:
                continue

            if any(kw in cid for kw in ["social", "greeting", "thanks"]):
                grouped["social"].append(cid)
            elif any(kw in cid for kw in ["fact", "definition"]):
                grouped["fact"].append(cid)
            elif any(kw in cid for kw in ["cause", "explanation"]):
                grouped["cause"].append(cid)
            elif any(kw in cid for kw in ["procedure", "step"]):
                grouped["procedure"].append(cid)
            elif "opinion" in cid:
                grouped["opinion"].append(cid)
            elif any(kw in cid for kw in ["meta", "question"]):
                grouped["meta"].append(cid)
            elif any(kw in cid for kw in ["creative", "metaphor"]):
                grouped["creative"].append(cid)
            else:
                grouped["other"].append(cid)

        return {k: v for k, v in grouped.items() if v}

    def save_knowledge(self, filepath: str, vocab: Dict[str, int]):
        cell_configs = []
        for cell_id, cell in self.cells.items():
            config = {
                'cell_id': cell_id,
                'input_size': cell.input_size,
                'hidden_size': cell.hidden_size,
                'output_size': cell.output_size,
                'cell_type': cell.cell_type
            }
            cell_configs.append(config)

        knowledge = {
            'model_state': self.state_dict(),
            'vocab': vocab,
            'cell_configs': cell_configs,
            'cell_counter': self.cell_counter,
            'meta_cog_log': self.meta_cog.reflection_log if self.meta_cog else [],
            'unknown_concepts': list(self.meta_cog.unknown_concepts) if self.meta_cog else []
        }
        torch.save(knowledge, filepath)
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.cells)} –∫–ª–µ—Ç–æ–∫, {len(vocab)} —Å–ª–æ–≤, {len(self.meta_cog.reflection_log)} —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π")

    def load_knowledge(self, filepath: str):
        if not os.path.exists(filepath):
            return None
        try:
            checkpoint = torch.load(filepath, map_location='cpu')
            saved_vocab = checkpoint['vocab']
            cell_configs = checkpoint['cell_configs']
            saved_state_dict = checkpoint['model_state']

            self.cells = nn.ModuleDict()
            self.cell_states = {}
            self.cell_activations = {}

            for config in cell_configs:
                input_size = config['input_size']
                hidden_size = config['hidden_size']
                cell_type = config['cell_type']

                safe_type = safe_cell_name(config['cell_id'].rsplit('_', 1)[0])
                cell_id = f"{safe_type}_{config['cell_id'].rsplit('_', 1)[1]}"

                self.cells[cell_id] = BrainCell(
                    cell_id=int(config['cell_id'].split('_')[-1]),
                    input_size=input_size,
                    hidden_size=hidden_size,
                    output_size=hidden_size,  # –≤—Å–µ–≥–¥–∞ hidden_size
                    cell_type=cell_type
                )
                self.cell_states[cell_id] = (None, None)
                self.cell_activations[cell_id] = 0.0

            self.cell_counter = checkpoint.get('cell_counter', len(self.cells))

            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å —É—á—ë—Ç–æ–º final_proj
            self.load_state_dict(saved_state_dict, strict=False)

            self.meta_cog = MetaCognition(saved_vocab)
            self.meta_cog.reflection_log = checkpoint.get('meta_cog_log', [])
            self.meta_cog.unknown_concepts = set(checkpoint.get('unknown_concepts', []))

            print(f"üß† –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.cells)} –∫–ª–µ—Ç–æ–∫, —Å–ª–æ–≤–∞—Ä—å —Ä–∞—Å—à–∏—Ä–µ–Ω –¥–æ {len(saved_vocab)}")
            return saved_vocab

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            traceback.print_exc()
            return None


# ======================
# –£–ß–ò–¢–ï–õ–¨ –î–õ–Ø LM STUDIO
# ======================

class BrainTeacher:
    def __init__(self, api_url: str = "http://localhost:1234/v1/chat/completions"):
        self.api_url = api_url
        self.conversation_history = []

    def query_qwen(self, prompt: str) -> str:
        try:
            response = requests.post(
                self.api_url,
                headers={"Content-Type": "application/json"},
                json={
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                    "max_tokens": 80,
                    "stream": False
                },
                timeout=30
            )
            if response.status_code == 200:
                raw = response.json()['choices'][0]['message']['content'].strip()
                return clean_qwen_response(raw)
            else:
                return "–•–æ—Ä–æ—à–æ."
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ LM Studio: {e}")
            return "–•–æ—Ä–æ—à–æ."

    def teach_brain(self, brain: CognitiveNetwork, user_input: str, vocab: Dict[str, int], ivocab: Dict[int, str]):
        raw_qwen = self.query_qwen(user_input)
        tagged_qwen = classify_and_tag_response(raw_qwen)

        print(f"üë§: {user_input}")
        print(f"ü§ñ: {tagged_qwen}")

        input_tokens = brain.text_to_tokens(user_input, vocab)
        response_tokens = brain.text_to_tokens(tagged_qwen, vocab)

        if not response_tokens:
            response_tokens = [vocab.get("—Ö–æ—Ä–æ—à–æ", 15), vocab.get(".", 34)]

        eos_id = vocab.get('<EOS>', 1)
        full_seq = input_tokens + response_tokens + [eos_id]
        input_seq = full_seq[:-1]
        target_seq = full_seq[1:]

        input_tensor = torch.tensor([input_seq], dtype=torch.long)
        target_tensor = torch.tensor([target_seq], dtype=torch.long)

        optimizer = torch.optim.AdamW(brain.parameters(), lr=0.0005)
        criterion = nn.CrossEntropyLoss(ignore_index=0)

        brain.train()
        total_loss = 0.0
        for _ in range(3):
            brain.reset_cell_states(input_tensor.size(0), input_tensor.device)
            optimizer.zero_grad()
            logits = brain.process_sequence(input_tensor)
            loss = criterion(logits.view(-1, brain.vocab_size), target_tensor.view(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(brain.parameters(), 0.5)
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / 3
        print(f"üìö –ü–æ—Ç–µ—Ä—è: {avg_loss:.4f}")

        brain.eval()
        with torch.no_grad():
            brain_tokens = brain.generate_sequence(
                torch.tensor([input_tokens], dtype=torch.long),
                max_length=25,
                base_temperature=0.85
            )
            brain_response_raw = " ".join(ivocab.get(tid, "<UNK>") for tid in brain_tokens)

        brain_response_clean = re.sub(r'\[(SOC|FCT|CAU|PRC|OPN|MET|CRT)\]\s*', '', brain_response_raw)

        self.conversation_history.append({
            'input': user_input,
            'qwen_tagged': tagged_qwen,
            'brain_raw': brain_response_raw,
            'brain_clean': brain_response_clean,
            'loss': avg_loss,
            'timestamp': datetime.now().isoformat()
        })

        print(f"üß† –°—ã—Ä–æ–π: {brain_response_raw}")
        print(f"üí¨ –ß–∏—Å—Ç—ã–π: {brain_response_clean}")

        return tagged_qwen, brain_response_clean


# ======================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ======================

def create_initial_vocabulary() -> Dict[str, int]:
    return {
        '<PAD>': 0, '<EOS>': 1, '<UNK>': 2,
        '[SOC]': 3, '[FCT]': 4, '[CAU]': 5,
        '[PRC]': 6, '[OPN]': 7, '[MET]': 8, '[CRT]': 9,
        '–ø—Ä–∏–≤–µ—Ç': 10, '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π': 11, '–¥–æ–±—Ä—ã–π': 12, '–¥–µ–Ω—å': 13,
        '–∫–∞–∫': 14, '—Ç—ã': 15, '–¥–µ–ª–∞': 16, '—á—Ç–æ': 17, '—Ç–∞–∫–æ–µ': 18,
        '—ç—Ç–æ': 19, '–ø–æ—Ç–æ–º—É': 20, '—á—Ç–æ': 21, '—á—Ç–æ–±—ã': 22,
        '—è': 23, '–¥—É–º–∞—é': 24, '–º–Ω–µ': 25, '–∫–∞–∂–µ—Ç—Å—è': 26,
        '–ø—Ä–µ–¥—Å—Ç–∞–≤—å': 27, '–≤–æ–æ–±—Ä–∞–∑–∏': 28, '–∂–∏–∑–Ω—å': 29,
        '—Å–ø–∞—Å–∏–±–æ': 30, '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞': 31, '–∏–∑–≤–∏–Ω–∏': 32,
        '—Ö–æ—Ä–æ—à–æ': 33, '–ª–∞–¥–Ω–æ': 34, '–æ–∫': 35,
        '–Ω–µ–π—Ä–æ–Ω': 36, '–≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è': 37, '–≤–æ–¥–∞': 38,
        '.': 39, ',': 40, '?': 41, '!': 42
    }


# ======================
# MAIN
# ======================

def main():
    print("üß† –ó–∞–ø—É—Å–∫ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ —Å —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏...")
    vocab = create_initial_vocabulary()
    vocab_size = len(vocab) + 10000  # –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –Ω–æ–≤—ã—Ö —Å–ª–æ–≤
    brain = CognitiveNetwork(vocab_size=vocab_size, eos_token_id=vocab['<EOS>'])
    teacher = BrainTeacher()

    loaded_vocab = brain.load_knowledge("brain_knowledge.pth")
    if loaded_vocab is not None:
        vocab = loaded_vocab
        print("‚úÖ –ó–Ω–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ —Å–ª–æ–≤–∞—Ä—å –æ–±–Ω–æ–≤–ª—ë–Ω")
    else:
        brain._initialize_base_cells()
        brain.cell_counter = len(brain.cells)
        brain.meta_cog = MetaCognition(vocab)
        print("üìù –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —Å–µ—Ç—å —Å –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –º–æ–¥—É–ª–µ–º")

    ivocab = {v: k for k, v in vocab.items()}
    print(f"üî¢ –ö–ª–µ—Ç–æ–∫: {len(brain.cells)} | üìö –°–ª–æ–≤–∞—Ä—å: {len(vocab)} —Å–ª–æ–≤")
    print("üí¨ –ì–æ—Ç–æ–≤ –∫ –¥–∏–∞–ª–æ–≥—É! (–≤–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")

    conversation_count = 0
    reflection_buffer = []
    MAX_REFLECTION_CHAIN = 4
    reflection_chain = 0

    while True:
        try:
            if reflection_buffer:
                user_input = reflection_buffer.pop(0)
                print(f"\nüí≠ –ú–æ–∑–≥ –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å: {user_input}")
            else:
                user_input = input("\nüë§ –í—ã: ").strip()
                if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
                    break
                if not user_input:
                    continue
                reflection_chain = 0

            qwen_resp, brain_resp = teacher.teach_brain(brain, user_input, vocab, ivocab)
            ivocab = {v: k for k, v in vocab.items()}

            new_question = brain.reflect_and_learn(user_input, qwen_resp, vocab, ivocab)
            if new_question:
                recent_inputs = {entry['input'] for entry in teacher.conversation_history[-5:]}
                if new_question not in recent_inputs and new_question not in reflection_buffer:
                    if reflection_chain < MAX_REFLECTION_CHAIN:
                        reflection_buffer.append(new_question)
                        reflection_chain += 1
                    else:
                        print("üß† –¶–µ–ø–æ—á–∫–∞ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –ø—Ä–µ—Ä–≤–∞–Ω–∞ (—Å–ª–∏—à–∫–æ–º –≥–ª—É–±–æ–∫–æ)")
                        reflection_chain = 0

            brain._check_cell_creation(user_input, qwen_resp, brain_resp)

            # üî¨ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–ª–µ—Ç–æ–∫ –ø–æ —Ç–∏–ø—É
            activated_by_type = brain.find_activated_cells_by_type(threshold=0.15)
            if activated_by_type:
                print("üî¨ –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–ª–µ—Ç–∫–∏ –ø–æ —Ç–∏–ø—É:")
                for cell_type, cells in activated_by_type.items():
                    display_cells = ', '.join(cells[:2]) + ('...' if len(cells) > 2 else '')
                    print(f"  ‚Ä¢ {cell_type.upper()}: {len(cells)} –∫–ª–µ—Ç–æ–∫ ({display_cells})")

            conversation_count += 1
            if conversation_count % 2 == 0:
                brain.save_knowledge("brain_knowledge.pth", vocab)
                print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ!")

        except KeyboardInterrupt:
            print("\nüõë –ü—Ä–µ—Ä–≤–∞–Ω–æ")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            traceback.print_exc()

    brain.save_knowledge("brain_knowledge.pth", vocab)
    print(f"\nüß† –ò—Ç–æ–≥: –∫–ª–µ—Ç–æ–∫ = {len(brain.cells)}, —Å–ª–æ–≤–∞—Ä—å = {len(vocab)} —Å–ª–æ–≤")
    if brain.meta_cog:
        print(f"ü§î –†–∞–∑–º—ã—à–ª–µ–Ω–∏–π: {len(brain.meta_cog.reflection_log)}")
        print(f"‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ–Ω—è—Ç–∏–π: {len(brain.meta_cog.unknown_concepts)}")


if __name__ == "__main__":
    main()